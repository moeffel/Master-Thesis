{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296acc01",
   "metadata": {},
   "source": [
    "# ARIMA-GARCH Modeling for Crypto Risk Assessment (Portfolio Notebook)\n",
    "\n",
    "**Source material:** `Masterthesis___ARIMA_GARCH_Oeffel_final.pdf`, `ARIMA GARCH FINAL.py`  \n",
    "**Author:** Markus Oeffel (portfolio version)  \n",
    "**Goal:** Convert the masterâ€™s thesis into a detailed, reproducible data-science notebook that demonstrates quantitative depth and implementation skills.\n",
    "\n",
    "---\n",
    "### How to use this notebook\n",
    "- Run the *Setup* cell to install/import dependencies.\n",
    "- Use *Quick Results* to load existing outputs and figures.\n",
    "- Use *Full Pipeline* to re-run the ARIMA-GARCH workflow.\n",
    "\n",
    "---\n",
    "### To-do checklist (implemented)\n",
    "- [x] Extract thesis structure and map to notebook sections.\n",
    "- [x] Expand theory: crypto market characteristics, ARIMA, GARCH family, evaluation logic.\n",
    "- [x] Reproduce data pipeline (download, cleaning, log returns, splits).\n",
    "- [x] Bring in substantial code from the thesis implementation.\n",
    "- [x] Add evaluation metrics, VaR/ES, and backtesting procedures.\n",
    "- [x] Include plots and result loaders for portfolio presentation.\n",
    "- [x] Document limitations and extensions.\n",
    "- [x] Final publication readiness checklist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d12aa",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Introduction and Research Question  \n",
    "2. Literature Review and Market Characteristics  \n",
    "3. Methodology and Data Pipeline  \n",
    "4. ARIMA Model (Conditional Mean)  \n",
    "5. GARCH Family (Conditional Variance)  \n",
    "6. Model Selection and Forecasting Workflow  \n",
    "7. Evaluation Metrics and Statistical Tests  \n",
    "8. Risk Metrics (VaR / ES) and Backtesting  \n",
    "9. Empirical Results (Quick Load)  \n",
    "10. Discussion, Limitations, Alternatives  \n",
    "11. Conclusion  \n",
    "12. Publication Readiness Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6fd47",
   "metadata": {},
   "source": [
    "## 1. Introduction and Research Question\n",
    "\n",
    "Cryptocurrencies such as Bitcoin (BTC), Ethereum (ETH), Dogecoin (DOGE), and Solana (SOL) \n",
    "have evolved from niche innovations into assets that attract both retail and institutional participation. \n",
    "They exhibit extreme volatility, heavy-tailed returns, and 24/7 trading, making them difficult to model \n",
    "with standard techniques developed for traditional markets.\n",
    "\n",
    "**Research question:** How accurately can ARIMA-GARCH models predict price movements and volatility \n",
    "patterns of BTC/ETH/DOGE/SOL, and how does accuracy vary across these cryptocurrencies?  \n",
    "\n",
    "This notebook reproduces the thesis methodology and code to demonstrate time-series modeling, \n",
    "volatility forecasting, and quantitative evaluation at a professional standard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab35f42b",
   "metadata": {},
   "source": [
    "## 2. Literature Review and Market Characteristics\n",
    "\n",
    "The thesis highlights several stylized facts of crypto markets:\n",
    "\n",
    "- **Extreme volatility:** Large price swings and persistent volatility regimes.  \n",
    "- **Heavy tails / excess kurtosis:** Return distributions deviate strongly from Gaussian assumptions.  \n",
    "- **Volatility persistence (long memory):** The decay of shocks is slow, motivating FIGARCH models.\n",
    "\n",
    "These stylized facts justify a two-stage modeling approach: ARIMA for the conditional mean and \n",
    "GARCH-family models for conditional variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3925d65",
   "metadata": {},
   "source": [
    "### 2.1 Overview of Selected Cryptocurrencies\n",
    "\n",
    "- **Bitcoin (BTC):** First decentralized cryptocurrency (digital gold narrative).  \n",
    "- **Ethereum (ETH):** Smart-contract platform and backbone of DeFi.  \n",
    "- **Dogecoin (DOGE):** Meme-driven, sentiment-sensitive asset with extreme spikes.  \n",
    "- **Solana (SOL):** High-throughput blockchain for scalable dApps.\n",
    "\n",
    "These assets cover a broad spectrum of market behaviors and provide a robust testbed for the ARIMA-GARCH framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92490b2c",
   "metadata": {},
   "source": [
    "### 2.2 Heavy Tails and Volatility Clustering\n",
    "\n",
    "Crypto returns typically show leptokurtosis and strong volatility clustering. This motivates:\n",
    "\n",
    "- Stationarity tests (ADF / KPSS)  \n",
    "- Volatility models beyond constant-variance assumptions  \n",
    "- Risk measures (VaR / ES) that account for fat tails\n",
    "\n",
    "The thesis uses these properties to motivate the ARIMA-GARCH structure as a transparent baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d033c083",
   "metadata": {},
   "source": [
    "## 3. Methodology and Data Pipeline\n",
    "\n",
    "**Data source:** Daily prices from Yahoo Finance (yfinance).  \n",
    "**Period:** 2020-05-11 to 2024-04-20 (consistent with thesis code).  \n",
    "**Preprocessing:** remove duplicates, interpolate missing values, compute log returns.  \n",
    "**Splits:** Train / Validation / Test (70% / 15% / 15%).\n",
    "\n",
    "Evaluation is performed via rolling 1-step backtests and multi-horizon forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d6a2c",
   "metadata": {},
   "source": [
    "### 3.1 ARIMA Model (Conditional Mean)\n",
    "\n",
    "For a differenced series $w_t = \\nabla^d z_t$, the ARIMA($p,d,q$) model is:\n",
    "\n",
    "$$\n",
    " w_t = c + \\sum_{i=1}^{p} \\phi_i w_{t-i} + \\sum_{j=1}^{q} \\theta_j \\varepsilon_{t-j} + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "ARIMA captures linear dependence in the mean but assumes constant variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd0206",
   "metadata": {},
   "source": [
    "### 3.2 GARCH Model (Conditional Variance)\n",
    "\n",
    "The GARCH($p,q$) specification is:\n",
    "\n",
    "$$\n",
    " \\sigma_t^2 = \\omega + \\sum_{i=1}^{p} \\alpha_i \\varepsilon_{t-i}^2 + \\sum_{j=1}^{q} \\beta_j \\sigma_{t-j}^2\n",
    "$$\n",
    "\n",
    "Extensions (EGARCH, GJR, FIGARCH) capture asymmetry or long-memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887ef57",
   "metadata": {},
   "source": [
    "### Setup (dependencies)\n",
    "\n",
    "This notebook mirrors the thesis codebase. You may need to install:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy matplotlib scipy statsmodels arch yfinance dieboldmariano dataframe_image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional (used in full pipeline)\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    from statsmodels.tsa.stattools import adfuller, kpss\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from arch import arch_model\n",
    "except Exception as e:\n",
    "    print(\"Optional deps not available:\", e)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33268cdf",
   "metadata": {},
   "source": [
    "## 4. Configuration (from original implementation)\n",
    "\n",
    "Below is the configuration block from `ARIMA GARCH FINAL.py` (as used in the thesis experiments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77342abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"coins_to_analyze\": [\"bitcoin\", \"ethereum\", \"dogecoin\", \"solana\"],\n",
    "    \"start_date\": \"2020-05-11\",  # YYYY-MM-DD\n",
    "    \"end_date\": \"2024-04-20\",    # None = today #\"2025-04-01\"\n",
    "    \"split_ratios\": (0.70, 0.15, 0.15),  # Train, Validation, Test\n",
    "    \"forecast_mode\": 'backtest',  # 'horizon_evaluation', 'backtest', 'future' \n",
    "    \"evaluation_horizons\": [1, 3, 7, 14,30],  # Days for horizon evaluation\n",
    "    \"dm_test_loss_type\": \"Squared Error\",  # For price/return DM tests\n",
    "    \"dm_test_alpha\": 0.05,  # Significance level for Diebold-Mariano tests\n",
    "    \"ewma_lambda\": 0.94,  # Smoothing factor for EWMA benchmark volatility\n",
    "    \"dm_test_variance_loss_type\": \"QLIKE\",  # qlike or squared error\n",
    "    \"fitting_window_size\": 60,  # Days for rolling window in 'backtest' mode ##90\n",
    "    \"refit_interval\": 1,  # Refit model every N steps in 'backtest' mode\n",
    "    \"future_forecast_horizon\": 70,  # Days for 'future' mode forecast\n",
    "    \"use_auto_tune\": True,  # Use automated model selection\n",
    "    \"compare_garch_dists\": True,  # If not auto-tuning, compare dists\n",
    "    \"manual_arima_order\": (1, 0, 1),  # Manual ARIMA order (p, d, q)\n",
    "    \"manual_garch_order\": (1, 1),\n",
    "    \"garch_distribution_manual_choice\": 't',\n",
    "    \"garch_vol_model_manual\": 'FIGARCH', # GARCH, EGARCH, GJR, FIGARCH\n",
    "    \"tune_min_p\": 0, \"tune_max_p\": 3,\n",
    "    \"tune_min_q\": 0, \"tune_max_q\": 3,\n",
    "    \"tune_min_d\": 0, \"tune_max_d\": 0,\n",
    "    \"tune_min_gp\": 1, \"tune_max_gp\": 2,\n",
    "    \"tune_min_gq\": 1, \"tune_max_gq\": 2,\n",
    "    \"garch_types_to_tune\": ['GARCH', 'FIGARCH','GJR','EGARCH'], # EGARCH unstable in tuning\n",
    "    \"tune_criterion\": 'AIC', # 'AIC' or 'BIC'\n",
    "    \"param_significance_level\": 0.10,  # Significance level for parameter significance\n",
    "    \"plot_dir_base\": \"thesis_results\", # Base directory for plots\n",
    "    \"generate_parameter_tables\": True,\n",
    "    \"generate_console_parameter_output\": True,\n",
    "    \"generate_stability_plots\": True, # \n",
    "    \"verbose_tuning\": False,\n",
    "    \"default_scale_factor\": 100.0, \n",
    "    \"max_differencing_order\": 0,\n",
    "    \"min_data_length\": 400,\n",
    "    \"min_fitting_window_size\": 30,  # Minimum fitting window size for backtest\n",
    "    \"min_test_set_size\": 80,\n",
    "    \"adf_significance\": 0.05,\n",
    "    \"kpss_significance\": 0.05,\n",
    "    \"qlike_epsilon\": 1e-8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9065066",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRYPTO_SYMBOLS = {\n",
    "    \"bitcoin\":  \"BTC-USD\",\n",
    "    \"ethereum\": \"ETH-USD\",\n",
    "    \"dogecoin\": \"DOGE-USD\",\n",
    "    \"solana\":   \"SOL-USD\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c2109",
   "metadata": {},
   "source": [
    "## 5. Core Functions (extracted from the original thesis code)\n",
    "\n",
    "The following cells contain substantial code extracted from `ARIMA GARCH FINAL.py`, representing the\n",
    "core implementation of the pipeline used in the thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512ab01",
   "metadata": {},
   "source": [
    "### 5.1 Descriptive Statistics and Stationarity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptive_stats(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Computes descriptive statistics for price and log return columns.\"\"\"\n",
    "    stats_dict: Dict[str, float] = {}\n",
    "    log.debug(\"Computing descriptive statistics...\")\n",
    "    def _add_stats(series: pd.Series, prefix: str, target_dict: Dict[str, float]):\n",
    "        series_numeric = pd.to_numeric(series, errors='coerce').dropna()\n",
    "        if series_numeric.empty:\n",
    "            log.warning(f\"'{prefix}' series empty or not numeric\")\n",
    "            target_dict[f'{prefix}_mean'] = np.nan\n",
    "            target_dict[f'{prefix}_std'] = np.nan\n",
    "            target_dict[f'{prefix}_min'] = np.nan\n",
    "            target_dict[f'{prefix}_max'] = np.nan\n",
    "            target_dict[f'{prefix}_skew'] = np.nan\n",
    "            target_dict[f'{prefix}_kurtosis'] = np.nan\n",
    "            return\n",
    "        try:\n",
    "            n = len(series_numeric)\n",
    "            target_dict[f'{prefix}_mean'] = float(series_numeric.mean())\n",
    "            target_dict[f'{prefix}_std'] = float(series_numeric.std()) if n > 1 else 0.0\n",
    "            target_dict[f'{prefix}_min'] = float(series_numeric.min())\n",
    "            target_dict[f'{prefix}_max'] = float(series_numeric.max())\n",
    "            min_obs_sk = 4 # Min observations for robust skew/kurtosis\n",
    "            if n >= min_obs_sk:\n",
    "                vals_1d = series_numeric.values\n",
    "                if hasattr(vals_1d, 'ndim') and vals_1d.ndim != 1: # Ensure 1D for scipy\n",
    "                    vals_1d = vals_1d.flatten()\n",
    "                if scipy_skew_kurt_available and skew is not None and scipy_kurtosis is not None:\n",
    "                    try:\n",
    "                        target_dict[f'{prefix}_skew'] = float(skew(vals_1d))\n",
    "                        target_dict[f'{prefix}_kurtosis'] = float(scipy_kurtosis(vals_1d, fisher=True)) # Fisher's definition (normal=0)\n",
    "\n",
    "                        if jarque_bera is not None:\n",
    "                            jb_result = jarque_bera(vals_1d)\n",
    "                            target_dict[f'{prefix}_jb_stat'] = float(jb_result.statistic)\n",
    "                            target_dict[f'{prefix}_jb_pvalue'] = float(jb_result.pvalue)\n",
    "                        else:\n",
    "                            target_dict[f'{prefix}_jb_stat'] = np.nan\n",
    "                            target_dict[f'{prefix}_jb_pvalue'] = np.nan\n",
    "\n",
    "                    except Exception as sk_e:\n",
    "                        log.error(f\"Error calculating Scipy Skew/Kurtosis for {prefix}: {sk_e}\")\n",
    "                        target_dict[f'{prefix}_skew'] = np.nan\n",
    "                        target_dict[f'{prefix}_kurtosis'] = np.nan\n",
    "                else:\n",
    "                    log.warning(f\"Using Pandas Skew/Kurtosis for {prefix} (SciPy unavailable or failed).\")\n",
    "                    temp_series = pd.Series(vals_1d)\n",
    "                    target_dict[f'{prefix}_skew'] = float(temp_series.skew())\n",
    "                    target_dict[f'{prefix}_kurtosis'] = float(temp_series.kurtosis()) # Pandas uses Fisher=True by default\n",
    "            else:\n",
    "                log.debug(f\"Need {min_obs_sk} numeric values for Skew/Kurtosis '{prefix}' (had {n})\")\n",
    "                target_dict[f'{prefix}_skew'] = np.nan\n",
    "                target_dict[f'{prefix}_kurtosis'] = np.nan\n",
    "        except Exception as e:\n",
    "            log.error(f\"Statistics calculation error for '{prefix}': {e}\")\n",
    "            # Ensure keys exist even on error\n",
    "            target_dict.setdefault(f'{prefix}_mean', np.nan)\n",
    "            target_dict.setdefault(f'{prefix}_std', np.nan)\n",
    "            target_dict.setdefault(f'{prefix}_min', np.nan)\n",
    "            target_dict.setdefault(f'{prefix}_max', np.nan)\n",
    "            target_dict.setdefault(f'{prefix}_skew', np.nan)\n",
    "            target_dict.setdefault(f'{prefix}_kurtosis', np.nan)\n",
    "\n",
    "    if 'price' in df:\n",
    "        _add_stats(df['price'], 'price', stats_dict)\n",
    "    if 'log_return' in df:\n",
    "        _add_stats(df['log_return'], 'logret', stats_dict)\n",
    "    log.debug(\"Descriptive statistics calculated.\")\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969720cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series: pd.Series, significance: float = 0.05) -> Dict[str, Any]:\n",
    "    \"\"\"Performs the Augmented Dickey-Fuller test for stationarity.\"\"\"\n",
    "    log.debug(\"Performing ADF test...\")\n",
    "    series_clean = series.dropna()\n",
    "    result_dict = {'p_value':np.nan,'is_stationary':False,'test_statistic':np.nan,'critical_values':{}, 'error': None}\n",
    "    if series_clean.empty:\n",
    "        log.warning(\"ADF skipped: Empty series\")\n",
    "        result_dict['error'] = \"Empty series\"\n",
    "        return result_dict\n",
    "    try:\n",
    "        # Ensure finite values\n",
    "        series_clean = series_clean[np.isfinite(series_clean)]\n",
    "        if series_clean.empty:\n",
    "            log.warning(\"ADF skipped: Empty after non-finite filtering\")\n",
    "            result_dict['error'] = \"Empty after non-finite filtering\"\n",
    "            return result_dict\n",
    "        # Ensure 1D\n",
    "        if hasattr(series_clean, 'ndim') and series_clean.ndim != 1:\n",
    "             log.warning(f\"ADF Input NOT 1D: Shape={series_clean.shape}, attempting flatten.\")\n",
    "             if isinstance(series_clean, pd.DataFrame) and series_clean.shape[1] == 1:\n",
    "                 series_clean = series_clean.iloc[:, 0]\n",
    "             elif isinstance(series_clean, np.ndarray):\n",
    "                 series_clean = series_clean.flatten()\n",
    "             else: # Attempt generic conversion\n",
    "                 series_clean = pd.Series(np.asarray(series_clean).flatten(), index=getattr(series_clean, 'index', None))\n",
    "        # Ensure Series or ndarray\n",
    "        if not isinstance(series_clean, (pd.Series, np.ndarray)):\n",
    "             series_clean = pd.Series(series_clean)\n",
    "\n",
    "        result = adfuller(series_clean, autolag='BIC')\n",
    "        p_val = result[1]\n",
    "        is_stat = bool(p_val < significance) # ADF null hypothesis is non-stationarity\n",
    "        log.info(f\"ADF Test: p-value={p_val:.4f}, Stationary={is_stat} (alpha={significance})\")\n",
    "        result_dict.update({'p_value':p_val,'is_stationary':is_stat, 'test_statistic': result[0],'critical_values': result[4]})\n",
    "    except ValueError as ve:\n",
    "        if \"1-dimensional\" in str(ve):\n",
    "            log.critical(f\"ADF failed: Dimension error: {ve}\") # Critical as it indicates upstream data issues\n",
    "        elif \"maximum lag\" in str(ve).lower():\n",
    "            log.warning(f\"ADF failed: likely due to small sample size: {ve}\")\n",
    "        else:\n",
    "            log.error(f\"ADF ValueError: {ve}\")\n",
    "        result_dict['error'] = str(ve)\n",
    "    except Exception as e:\n",
    "        log.error(f\"ADF failed: unexpected error: {e}\")\n",
    "        result_dict['error'] = str(e)\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpss_test(series: pd.Series, significance: float = 0.05, regression: str = 'c') -> Dict[str, Any]:\n",
    "    \"\"\"Performs the KPSS test for stationarity.\"\"\"\n",
    "    log.debug(f\"Performing KPSS test (reg='{regression}')...\")\n",
    "    series_clean = series.dropna()\n",
    "    result_dict = {'p_value':np.nan,'is_stationary':False,'test_statistic':np.nan,'critical_values':{}, 'lags':None, 'error': None}\n",
    "    if series_clean.empty:\n",
    "        log.warning(\"KPSS skipped: Empty series\")\n",
    "        result_dict['error'] = \"Empty series\"\n",
    "        return result_dict\n",
    "    try:\n",
    "        # Ensure finite values\n",
    "        series_clean = series_clean[np.isfinite(series_clean)]\n",
    "        if series_clean.empty:\n",
    "            log.warning(\"KPSS skipped: Empty after non-finite filtering\")\n",
    "            result_dict['error'] = \"Empty after non-finite filtering\"\n",
    "            return result_dict\n",
    "        # Ensure 1D\n",
    "        if hasattr(series_clean, 'ndim') and series_clean.ndim != 1:\n",
    "             log.warning(f\"KPSS Input NOT 1D: Shape={series_clean.shape}, attempting flatten.\")\n",
    "             if isinstance(series_clean, pd.DataFrame) and series_clean.shape[1] == 1:\n",
    "                 series_clean = series_clean.iloc[:, 0]\n",
    "             elif isinstance(series_clean, np.ndarray):\n",
    "                 series_clean = series_clean.flatten()\n",
    "             else: # Attempt generic conversion\n",
    "                 series_clean = pd.Series(np.asarray(series_clean).flatten(), index=getattr(series_clean, 'index', None))\n",
    "        # Ensure Series or ndarray\n",
    "        if not isinstance(series_clean, (pd.Series, np.ndarray)):\n",
    "             series_clean = pd.Series(series_clean)\n",
    "        # Check length\n",
    "        min_len_kpss = 5 # Heuristic minimum length for KPSS\n",
    "        if len(series_clean) < min_len_kpss:\n",
    "            log.warning(f\"KPSS skipped: Series length {len(series_clean)} too short.\")\n",
    "            result_dict['error'] = f\"Series length {len(series_clean)} too short\"\n",
    "            return result_dict\n",
    "\n",
    "        # Run KPSS test, catch specific warning about p-value range\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', message='The test statistic is outside of the range of p-values')\n",
    "            try:\n",
    "                # Use recommended lag calculation based on Schwert (1989) default in statsmodels\n",
    "                n_lags = math.floor(12 * (len(series_clean) / 100)**(1/4))\n",
    "                kpss_stat, p_val, lags, crit_vals = kpss(series_clean, regression=regression, nlags=n_lags)\n",
    "            except ValueError as kpss_ve:\n",
    "                 # Fallback if calculated nlags is too large for the sample\n",
    "                 if \"nlags is larger than\" in str(kpss_ve).lower():\n",
    "                      log.warning(f\"KPSS nlags={n_lags} failed ({kpss_ve}), trying 'legacy'.\")\n",
    "                      kpss_stat, p_val, lags, crit_vals = kpss(series_clean, regression=regression, nlags='legacy')\n",
    "                 else:\n",
    "                     raise kpss_ve # Re-raise other ValueErrors\n",
    "\n",
    "        is_stat = bool(p_val >= significance) # KPSS null hypothesis is stationarity\n",
    "        log.info(f\"KPSS Test (reg='{regression}', lags={lags}): p-value={p_val:.4f}, Stationary={is_stat} (alpha={significance})\")\n",
    "        result_dict.update({'p_value':p_val,'is_stationary':is_stat, 'test_statistic': kpss_stat,'critical_values': crit_vals, 'lags': lags})\n",
    "    except ValueError as ve:\n",
    "        if \"1-dimensional\" in str(ve):\n",
    "            log.critical(f\"KPSS failed: Dimension error: {ve}\") # Critical\n",
    "        else:\n",
    "            log.error(f\"KPSS ValueError: {ve}\")\n",
    "        result_dict['error'] = str(ve)\n",
    "    except Exception as e:\n",
    "        log.error(f\"KPSS failed: unexpected error: {e}\")\n",
    "        result_dict['error'] = str(e)\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5242ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_series(series: pd.Series, order: int = 1) -> pd.Series:\n",
    "    \"\"\"Applies differencing to a pandas Series.\"\"\"\n",
    "    if not isinstance(series, pd.Series):\n",
    "        series = pd.Series(series) # Attempt conversion\n",
    "    if order <= 0:\n",
    "        return series.copy()\n",
    "    if order >= len(series):\n",
    "        log.warning(f\"Differencing order {order} >= series length {len(series)}. Returning empty series.\")\n",
    "        empty_index = series.index[:0] if hasattr(series, 'index') else pd.Index([])\n",
    "        return pd.Series(dtype=series.dtype, index=empty_index)\n",
    "    log.debug(f\"Applying differencing d={order}\")\n",
    "    return series.diff(periods=order).dropna()\n",
    "\n",
    "# --- Error Metrics ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b599d9",
   "metadata": {},
   "source": [
    "### 5.2 Forecast Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b015e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(yt:Any,yp:Any)->float:\n",
    "    \"\"\"Calculates Mean Absolute Error, ignoring NaNs.\"\"\"\n",
    "    yta=np.asarray(yt)\n",
    "    ypa=np.asarray(yp)\n",
    "    mask=pd.notna(yta)&pd.notna(ypa)\n",
    "    return np.nan if not np.any(mask) else float(np.mean(np.abs(yta[mask]-ypa[mask])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(yt:Any,yp:Any)->float:\n",
    "    \"\"\"Calculates Mean Squared Error, ignoring NaNs.\"\"\"\n",
    "    yta=np.asarray(yt)\n",
    "    ypa=np.asarray(yp)\n",
    "    mask=pd.notna(yta)&pd.notna(ypa)\n",
    "    return np.nan if not np.any(mask) else float(np.mean((yta[mask]-ypa[mask])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12667e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(yt:Any,yp:Any)->float:\n",
    "    \"\"\"Calculates Root Mean Squared Error, ignoring NaNs.\"\"\"\n",
    "    mse=mean_squared_error(yt,yp)\n",
    "    return np.nan if np.isnan(mse) else float(np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_variance(yt_var:Any,yp_var:Any)->float:\n",
    "    \"\"\"Calculates Mean Squared Error for variance forecasts.\"\"\"\n",
    "    yta=np.asarray(yt_var)\n",
    "    ypa=np.asarray(yp_var)\n",
    "    # Ensure valid (non-NaN, finite, non-negative) pairs for variance MSE\n",
    "    mask=pd.notna(yta)&pd.notna(ypa)&np.isfinite(yta)&np.isfinite(ypa)&(yta>=0)&(ypa>=0)\n",
    "    return np.nan if not np.any(mask) else float(np.mean((yta[mask]-ypa[mask])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error_variance(yt_var:Any,yp_var:Any)->float:\n",
    "    \"\"\"Calculates Root Mean Squared Error for variance forecasts.\"\"\"\n",
    "    mse=mean_squared_error_variance(yt_var,yp_var)\n",
    "    # Ensure non-negative MSE before sqrt\n",
    "    return np.nan if (np.isnan(mse) or mse < 0) else float(np.sqrt(max(0,mse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed07cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlike_loss_calc(actual_var: float, forecast_var: float, epsilon: float = 1e-8) -> float:\n",
    "    \"\"\"Calculates QLIKE loss for a single point, ensuring positivity.\"\"\"\n",
    "    safe_actual = max(epsilon, actual_var)\n",
    "    safe_forecast = max(epsilon, forecast_var)\n",
    "    try:\n",
    "        # QLIKE = actual/forecast - log(actual/forecast) - 1\n",
    "        loss = safe_actual / safe_forecast - np.log(safe_actual / safe_forecast) - 1\n",
    "        # QLIKE should theoretically be >= 0. Clamp small negative values due to precision.\n",
    "        if loss < -epsilon:\n",
    "            log.debug(f\"QLIKE loss < 0 ({loss:.2e}) for actual={actual_var:.2e}, forecast={forecast_var:.2e}. Clamping to 0.\")\n",
    "            return 0.0\n",
    "        return loss\n",
    "    except (ValueError, OverflowError, ZeroDivisionError) as e:\n",
    "        log.warning(f\"QLIKE calculation error: {e} for actual={actual_var}, forecast={forecast_var}. Returning NaN.\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlike_loss(yt_var: Any, yp_var: Any, epsilon: float = 1e-8) -> float:\n",
    "    \"\"\"Calculates the average QLIKE loss for variance forecasts.\"\"\"\n",
    "    yta = np.asarray(yt_var)\n",
    "    ypa = np.asarray(yp_var)\n",
    "    mask = pd.notna(yta) & pd.notna(ypa) & np.isfinite(yta) & np.isfinite(ypa) & (yta >= 0) & (ypa >= 0)\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    losses = [qlike_loss_calc(yta[i], ypa[i], epsilon) for i in np.where(mask)[0]]\n",
    "    valid_losses = [l for l in losses if pd.notna(l) and np.isfinite(l)]\n",
    "    return np.nan if not valid_losses else float(np.mean(valid_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b140d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(yt:Any,yp:Any)->float:\n",
    "    \"\"\"Calculates Mean Absolute Percentage Error, ignoring NaNs and zero actuals.\"\"\"\n",
    "    yta=np.asarray(yt)\n",
    "    ypa=np.asarray(yp)\n",
    "    eps=1e-8 # Threshold to avoid division by zero or near-zero\n",
    "    mask_nan=pd.notna(yta)&pd.notna(ypa)\n",
    "    mask_zero=np.abs(yta)>eps # Filter out actual values close to zero\n",
    "    mask=mask_nan&mask_zero\n",
    "    if not np.any(mask):\n",
    "        log.warning(\"MAPE: No valid pairs found after filtering zeros/NaNs.\")\n",
    "        return np.nan\n",
    "    ignored=np.sum(~mask_zero&mask_nan)\n",
    "    if ignored > 0:\n",
    "        log.debug(f\"MAPE: Ignoring {ignored} zero/near-zero values in actuals.\")\n",
    "# Insert Risk Management Metrics code block immediately after MAPE function\n",
    "    return float(np.mean(np.abs((yta[mask]-ypa[mask])/yta[mask]))*100.0)\n",
    "\n",
    "# --- Risk Management Metrics ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9aa73",
   "metadata": {},
   "source": [
    "### 5.3 Risk Metrics (VaR / ES) and Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_at_risk(returns: Any, alpha: float = 0.05) -> float:\n",
    "    \"\"\"Calculates the empirical Value-at-Risk (VaR) at level alpha.\"\"\"\n",
    "    arr = np.asarray(returns)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if arr.size == 0:\n",
    "        log.warning(\"VaR: empty returns series.\")\n",
    "        return np.nan\n",
    "    return np.percentile(arr, 100 * alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388343f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parametric_var_es(mean_forecast: float,\n",
    "                                vol_forecast: float,\n",
    "                                distribution: str,\n",
    "                                alpha: float = 0.05,\n",
    "                                dist_params: Optional[Dict] = None) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculates parametric Value-at-Risk (VaR) and Expected Shortfall (ES).\n",
    "\n",
    "    Args:\n",
    "        mean_forecast: Forecasted mean (log return).\n",
    "        vol_forecast: Forecasted volatility (standard deviation).\n",
    "        distribution: Assumed distribution ('normal', 't', 'skewt').\n",
    "        alpha: VaR/ES significance level (e.g., 0.05 for 95%).\n",
    "        dist_params: Dictionary containing distribution parameters if needed\n",
    "                     (e.g., {'nu': df} for t, {'nu': df, 'lambda': skew} for skewt).\n",
    "\n",
    "    Returns:\n",
    "        Tuple (VaR_threshold, ES_value). Returns (NaN, NaN) on error.\n",
    "    \"\"\"\n",
    "    if pd.isna(mean_forecast) or pd.isna(vol_forecast) or vol_forecast < 0:\n",
    "        # log.debug(\"NaN/Invalid input for parametric VaR/ES.\")\n",
    "        return np.nan, np.nan\n",
    "    if vol_forecast < 1e-8: # Avoid issues with zero volatility\n",
    "        return mean_forecast, mean_forecast # VaR/ES is just the mean if no volatility\n",
    "\n",
    "    dist_lower = distribution.lower()\n",
    "    var_thresh = np.nan\n",
    "    es_val = np.nan\n",
    "\n",
    "    try:\n",
    "        if dist_lower == 'normal':\n",
    "            if norm is None: raise ImportError(\"Scipy norm not available.\")\n",
    "            q = norm.ppf(alpha) # Quantile (negative for left tail)\n",
    "            var_thresh = mean_forecast + q * vol_forecast\n",
    "            # ES for Normal: mu - sigma * pdf(ppf(alpha)) / alpha\n",
    "            es_val = mean_forecast - vol_forecast * norm.pdf(q) / alpha\n",
    "\n",
    "        elif dist_lower == 't':\n",
    "            if student_t is None: raise ImportError(\"Scipy student_t not available.\")\n",
    "            if dist_params is None or 'nu' not in dist_params or pd.isna(dist_params['nu']):\n",
    "                log.warning(\"Student's t degrees of freedom ('nu') missing or invalid. Cannot calc VaR/ES.\")\n",
    "                return np.nan, np.nan\n",
    "            df = dist_params['nu']\n",
    "            if df <= 2: # ES is undefined for df<=2\n",
    "                log.warning(f\"Student's t ES undefined for df={df}<=2. Returning NaN for ES.\")\n",
    "                es_val = np.nan\n",
    "            q = student_t.ppf(alpha, df)\n",
    "            var_thresh = mean_forecast + q * vol_forecast * np.sqrt(max(0,(df-2)/df)) if df > 2 else mean_forecast + q * vol_forecast # Adjust vol for df>2 var\n",
    "            # ES for Student's t: mu - sigma_adj * pdf(ppf(alpha,df),df)/alpha * (df + ppf(alpha,df)^2)/(df-1)\n",
    "            if df > 2: # ES requires df>2 for adjusted sigma\n",
    "                 sigma_adj = vol_forecast * np.sqrt((df-2)/df)\n",
    "                 es_val = mean_forecast - sigma_adj * (student_t.pdf(q,df)/alpha) * ((df+q**2)/(df-1))\n",
    "            elif df > 1: # VaR definable for df>1, ES for df>2\n",
    "                 var_thresh = mean_forecast + q * vol_forecast # Use unadjusted vol if df<=2\n",
    "                 # ES cannot be calculated as above for df<=2\n",
    "\n",
    "        elif dist_lower == 'skewt':\n",
    "            # Calculation for Skewed-t is more complex and depends on the specific parameterization.\n",
    "            # Using 'skewnorm' as an approximation or placeholder if skewt not fully implemented.\n",
    "            if skewnorm is None: raise ImportError(\"Scipy skewnorm not available.\")\n",
    "            if dist_params is None or 'lambda' not in dist_params or pd.isna(dist_params['lambda']):\n",
    "                 log.warning(\"Skew-t skewness parameter ('lambda') missing/invalid. Using skewnorm approx.\")\n",
    "                 skew_param = 0 # Default to normal if skew param missing\n",
    "            else:\n",
    "                 skew_param = dist_params['lambda']\n",
    "            # Note: skewnorm parameters are loc, scale, a (shape)\n",
    "            # We map mean, vol -> loc, scale based on skew_param\n",
    "            delta = skew_param / np.sqrt(1 + skew_param**2)\n",
    "            adj_std = np.sqrt(1 - (2 * delta**2 / np.pi))\n",
    "            if adj_std < 1e-6: adj_std = 1e-6 # Avoid division by zero\n",
    "            scale_sn = vol_forecast / adj_std\n",
    "            loc_sn = mean_forecast - scale_sn * delta * np.sqrt(2 / np.pi)\n",
    "\n",
    "            q = skewnorm.ppf(alpha, a=skew_param, loc=loc_sn, scale=scale_sn)\n",
    "            var_thresh = q\n",
    "            # ES for skew-normal is complex. Placeholder: return VaR. Need specific formula.\n",
    "            es_val = np.nan # Placeholder\n",
    "            log.warning(\"ES calculation for Skew-t distribution not fully implemented. Returning NaN for ES.\")\n",
    "\n",
    "        else:\n",
    "            log.warning(f\"Unsupported distribution '{distribution}' for parametric VaR/ES. Returning NaN.\")\n",
    "            return np.nan, np.nan\n",
    "\n",
    "        # Final check for NaN results\n",
    "        if pd.isna(var_thresh): es_val = np.nan # If VaR is NaN, ES must be NaN too\n",
    "\n",
    "    except ImportError as ie:\n",
    "        log.error(f\"Cannot calculate parametric VaR/ES: {ie}\")\n",
    "        return np.nan, np.nan\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error calculating parametric VaR/ES for dist='{distribution}': {e}\", exc_info=True)\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # VaR is the threshold (a loss, so typically negative), ES is the expected value beyond VaR (more negative)\n",
    "    return float(var_thresh), float(es_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343865a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_shortfall(returns: Any, alpha: float = 0.05) -> float:\n",
    "    \"\"\"Calculates the Expected Shortfall (ES) at level alpha.\"\"\"\n",
    "    arr = np.asarray(returns)\n",
    "    arr = arr[~np.isnan(arr)]\n",
    "    if arr.size == 0:\n",
    "        log.warning(\"ES: empty returns series.\")\n",
    "        return np.nan\n",
    "    var = value_at_risk(arr, alpha)\n",
    "    tail_losses = arr[arr <= var]\n",
    "    if tail_losses.size == 0:\n",
    "        # If no losses exceed VaR (e.g., alpha is very small or data has no tail), ES is just VaR\n",
    "        # Alternative approaches exist, but this is common.\n",
    "        return var # Or potentially np.nan if interpretation requires exceedances\n",
    "    return tail_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kupiec_test(violations: int, n: int, alpha: float = 0.05) -> Dict[str, float]:\n",
    "    \"\"\"Performs the Kupiec Proportion-of-Failures (POF) test.\"\"\"\n",
    "    # Ensure chi2 is available\n",
    "    if not scipy_chi2_available or chi2 is None:\n",
    "         log.error(\"Kupiec test requires scipy.stats.chi2.\")\n",
    "         return {'p_value': np.nan, 'LR_stat': np.nan, 'error': 'chi2 unavailable'}\n",
    "\n",
    "    if n == 0:\n",
    "        log.warning(\"Kupiec: zero observations.\")\n",
    "        return {'p_value': np.nan, 'LR_stat': np.nan, 'error': 'n=0'}\n",
    "    if violations < 0 or violations > n:\n",
    "        log.error(f\"Kupiec: Invalid violations count {violations} for n={n}.\")\n",
    "        return {'p_value': np.nan, 'LR_stat': np.nan, 'error': 'Invalid violations count'}\n",
    "    # Avoid log(0) issues with boundary cases\n",
    "    if violations == 0:\n",
    "        p_hat = 1e-9 # Use a tiny number instead of 0\n",
    "    elif violations == n:\n",
    "        p_hat = 1.0 - 1e-9 # Use a number very close to 1\n",
    "    else:\n",
    "        p_hat = violations / n\n",
    "\n",
    "    # Likelihood ratio statistic (handle potential log(0) or division by zero if p_hat or alpha are boundaries)\n",
    "    try:\n",
    "        loglik_unrestricted = 0\n",
    "        if violations > 0 and violations < n: # Only calculate if p_hat is not 0 or 1\n",
    "            loglik_unrestricted = violations * np.log(p_hat) + (n - violations) * np.log(1 - p_hat)\n",
    "\n",
    "        loglik_restricted = violations * np.log(alpha) + (n - violations) * np.log(1 - alpha)\n",
    "\n",
    "        LR = -2 * (loglik_restricted - loglik_unrestricted)\n",
    "        # LR statistic should be non-negative. Clamp numerical inaccuracies.\n",
    "        if LR < 0: LR = 0\n",
    "\n",
    "        p_value = 1 - chi2.cdf(LR, df=1)\n",
    "        return {'LR_stat': float(LR), 'p_value': float(p_value)}\n",
    "    except (ValueError, FloatingPointError) as e:\n",
    "        log.warning(f\"Kupiec numerical issue (p_hat={p_hat:.3f}, alpha={alpha:.3f}): {e}\")\n",
    "        return {'p_value': np.nan, 'LR_stat': np.nan, 'error': 'Numerical issue'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ce74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def christoffersen_test(violation_series: Any, alpha: float = 0.05) -> Dict[str, float]:\n",
    "    \"\"\"Performs Christoffersen conditional coverage test on a series of 0/1 violations.\"\"\"\n",
    "    # Ensure chi2 is available\n",
    "    if not scipy_chi2_available or chi2 is None:\n",
    "         log.error(\"Christoffersen test requires scipy.stats.chi2.\")\n",
    "         return {'p_value': np.nan, 'LR_stat': np.nan, 'error': 'chi2 unavailable'}\n",
    "\n",
    "    x = np.asarray(violation_series).astype(int)\n",
    "    n = len(x)\n",
    "    if n < 2:\n",
    "        log.warning(\"Christoffersen: too few observations.\")\n",
    "        return {'p_value': np.nan, 'LR_stat': np.nan, 'error': 'n<2'}\n",
    "    # Count transitions\n",
    "    n00 = np.sum((x[:-1] == 0) & (x[1:] == 0))\n",
    "    n01 = np.sum((x[:-1] == 0) & (x[1:] == 1))\n",
    "    n10 = np.sum((x[:-1] == 1) & (x[1:] == 0))\n",
    "    n11 = np.sum((x[:-1] == 1) & (x[1:] == 1))\n",
    "\n",
    "    # Transition probabilities\n",
    "    pi0 = n01 / (n00 + n01) if (n00 + n01) > 0 else 0\n",
    "    pi1 = n11 / (n10 + n11) if (n10 + n11) > 0 else 0\n",
    "    # Overall probability of violation\n",
    "    pi = (n01 + n11) / n if n > 0 else 0\n",
    "\n",
    "    # Check for edge cases where probabilities are 0 or 1\n",
    "    if pi == 0 or pi == 1:\n",
    "        log.warning(f\"Christoffersen: Unconditional exception rate is {pi*100}%. Independence test is not meaningful.\")\n",
    "        # Return Kupiec test result for conditional coverage in this case\n",
    "        kupiec_res = kupiec_test(violations=int(np.sum(x)), n=n, alpha=alpha)\n",
    "        kupiec_res['error'] = kupiec_res.get('error', f\"Independence test skipped (pi={pi})\")\n",
    "        return kupiec_res # Return Kupiec result as conditional coverage is not testable\n",
    "\n",
    "    # Likelihood under independence assumption (but observed unconditional probability pi)\n",
    "    L0 = 1.0\n",
    "    if pi > 0 and pi < 1:\n",
    "        try: L0 = ((1 - pi)**(n00 + n10)) * (pi**(n01 + n11))\n",
    "        except FloatingPointError: L0 = 1e-300 # Handle underflow\n",
    "\n",
    "    # Likelihood under dependence assumption (Markov chain)\n",
    "    term1 = (1 - pi0)**n00 if pi0 < 1 else 1.0 # Handle pi0=1 case\n",
    "    term2 = pi0**n01 if pi0 > 0 else 1.0 # Handle pi0=0 case\n",
    "    term3 = (1 - pi1)**n10 if pi1 < 1 else 1.0 # Handle pi1=1 case\n",
    "    term4 = pi1**n11 if pi1 > 0 else 1.0 # Handle pi1=0 case\n",
    "    L1 = term1 * term2 * term3 * term4\n",
    "\n",
    "    # Likelihood ratio test for independence\n",
    "    LR_ind = np.nan\n",
    "    if L0 > 1e-300 and L1 > 1e-300: # Check for potential zero likelihoods\n",
    "       try:\n",
    "            LR_ind = -2 * np.log(L0 / L1)\n",
    "            if LR_ind < 0: LR_ind = 0 # Clamp numerical errors\n",
    "       except (ValueError, FloatingPointError) as log_e:\n",
    "            log.warning(f\"Christoffersen LR_ind calculation error (L0={L0:.2e}, L1={L1:.2e}): {log_e}\")\n",
    "            LR_ind = np.nan\n",
    "    else:\n",
    "         log.warning(f\"Christoffersen likelihoods potentially too small (L0={L0:.2e}, L1={L1:.2e})\")\n",
    "\n",
    "    # Kupiec test for unconditional coverage\n",
    "    kupiec_res = kupiec_test(violations=int(np.sum(x)), n=n, alpha=alpha)\n",
    "    LR_uc = kupiec_res.get('LR_stat', np.nan)\n",
    "\n",
    "    # Combined test statistic (Conditional Coverage)\n",
    "    LR_cc = np.nan\n",
    "    if pd.notna(LR_uc) and pd.notna(LR_ind):\n",
    "        LR_cc = LR_uc + LR_ind\n",
    "    else:\n",
    "        log.warning(\"Cannot calculate combined Christoffersen LR_cc due to NaN components.\")\n",
    "\n",
    "    # P-value from Chi-squared distribution with 2 degrees of freedom\n",
    "    p_value = 1 - chi2.cdf(LR_cc, df=2) if pd.notna(LR_cc) else np.nan\n",
    "\n",
    "    return {'LR_stat': float(LR_cc), 'p_value': float(p_value), 'LR_ind': float(LR_ind), 'LR_uc': float(LR_uc)}\n",
    "\n",
    "\n",
    "# --- Diagnostic Tests ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d311a60",
   "metadata": {},
   "source": [
    "### 5.4 Statistical Tests (Ljung-Box, ARCH, Diebold-Mariano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36488381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ljung_box_test(r:pd.Series, l:Union[int,List[int]]=20, sig_level:float=0.05) -> Dict[str,Any]:\n",
    "    \"\"\"Performs the Ljung-Box test for autocorrelation in residuals.\"\"\"\n",
    "    log.debug(f\"Ljung-Box Test (Lags={l})...\")\n",
    "    r_c=r.dropna()\n",
    "    n=len(r_c)\n",
    "    lags_to_test:Optional[List[int]]=None\n",
    "    # Max lag feasible: n/2 - 1, must be > 0\n",
    "    max_lag_possible=max(1,n//2 - 1) if n > 3 else 0\n",
    "\n",
    "    if isinstance(l,int):\n",
    "        max_lag=min(l,max_lag_possible) if max_lag_possible > 0 else 0\n",
    "        lags_to_test=[max_lag] if max_lag>0 else None\n",
    "    elif isinstance(l,list):\n",
    "        valid_lags=sorted([lg for lg in l if 0<lg<=max_lag_possible]) if max_lag_possible > 0 else []\n",
    "        lags_to_test=valid_lags if valid_lags else None\n",
    "\n",
    "    if lags_to_test is None or n<2:\n",
    "        log.warning(f\"L-B skipped: n={n}, Lags={l}, max_lag_possible={max_lag_possible}.\")\n",
    "        return{'lb_pvalue':np.nan,'is_white_noise':np.nan,'lb_stat':np.nan,'lb_lag_tested':0}\n",
    "\n",
    "    try:\n",
    "        max_lag_tested=max(lags_to_test)\n",
    "        # Ensure lag is less than sample size\n",
    "        if max_lag_tested >= n -1:\n",
    "             log.warning(f\"L-B skipped: max Lag {max_lag_tested} >= n-1 ({n-1})\")\n",
    "             return{'lb_pvalue':np.nan,'is_white_noise':np.nan,'lb_stat':np.nan,'lb_lag_tested':0}\n",
    "\n",
    "        lb_result=acorr_ljungbox(r_c,lags=lags_to_test,return_df=True,boxpierce=False) # Use Ljung-Box, not Box-Pierce\n",
    "\n",
    "        if lb_result.empty or max_lag_tested not in lb_result.index:\n",
    "            raise ValueError(\"L-B result format error or lag not found.\")\n",
    "\n",
    "        lb_p_val=lb_result.loc[max_lag_tested,'lb_pvalue']\n",
    "        lb_stat=lb_result.loc[max_lag_tested,'lb_stat']\n",
    "        # Null hypothesis: No autocorrelation (residuals are white noise)\n",
    "        is_wn=bool(lb_p_val>sig_level)\n",
    "        log.info(f\"Ljung-Box Test (Lag={max_lag_tested}): p={lb_p_val:.4f}, White Noise={is_wn}\")\n",
    "        return{'lb_pvalue':float(lb_p_val),'is_white_noise':is_wn,'lb_stat':float(lb_stat),'lb_lag_tested':max_lag_tested}\n",
    "    except ValueError as ve:\n",
    "         # Catch potential numerical issues from statsmodels\n",
    "         if \"invalid value encountered in\" in str(ve).lower():\n",
    "             log.error(f\"L-B numerical error: {ve}\")\n",
    "         else:\n",
    "             log.error(f\"L-B Test failed: {ve}\")\n",
    "         return{'lb_pvalue':np.nan,'is_white_noise':np.nan,'lb_stat':np.nan,'lb_lag_tested':0}\n",
    "    except Exception as e:\n",
    "        log.error(f\"L-B Test unexpectedly failed: {e}\")\n",
    "        return{'lb_pvalue':np.nan,'is_white_noise':np.nan,'lb_stat':np.nan,'lb_lag_tested':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ab105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arch_test(r:pd.Series, l:Union[int,List[int]]=12, sig_level:float=0.05)->Dict[str,Any]:\n",
    "    \"\"\"Performs the ARCH LM test for heteroskedasticity.\"\"\"\n",
    "    log.debug(f\"ARCH LM Test (Lags={l})...\")\n",
    "    r_c=r.dropna()\n",
    "    n=len(r_c)\n",
    "    nlags:int=0\n",
    "    max_lag_possible=max(1,n//2 - 1) if n > 3 else 0\n",
    "\n",
    "    if isinstance(l,int):\n",
    "        nlags=min(l,max_lag_possible) if max_lag_possible > 0 else 0\n",
    "    elif isinstance(l,list):\n",
    "        valid_lags=[lg for lg in l if 0<lg<=max_lag_possible] if max_lag_possible > 0 else []\n",
    "        nlags=max(valid_lags) if valid_lags else 0\n",
    "\n",
    "    # Need at least nlags+1 observations\n",
    "    if nlags==0 or n<nlags+1:\n",
    "        log.warning(f\"ARCH LM skipped: n={n}, nlags={nlags} (needs > Lags).\")\n",
    "        return{'arch_pvalue':np.nan,'heteroskedastic':np.nan,'arch_stat':np.nan,'arch_lag_tested':0}\n",
    "\n",
    "    try:\n",
    "        # Use het_arch from statsmodels.stats.diagnostic\n",
    "        lm_stat,p_val,f_stat,fp_val=het_arch(r_c,nlags=nlags,store=False)\n",
    "        if np.isnan(p_val): # Check for NaN p-value explicitly\n",
    "            raise ValueError(\"NaN p-value returned by ARCH test.\")\n",
    "        # Null hypothesis: Homoskedasticity (No ARCH effects)\n",
    "        is_het=bool(p_val<sig_level)\n",
    "        log.info(f\"ARCH LM Test (Lag={nlags}): p={p_val:.4f}, Heteroskedastic={is_het}\")\n",
    "        return{'arch_pvalue':float(p_val),'heteroskedastic':is_het,'arch_stat':float(lm_stat),'arch_lag_tested':nlags}\n",
    "    except ValueError as ve:\n",
    "        if \"nlags must be smaller\" in str(ve).lower():\n",
    "             log.error(f\"ARCH LM failed (likely small N or large lag): {ve}\")\n",
    "        else:\n",
    "             log.error(f\"ARCH LM ValueError: {ve}\")\n",
    "        return{'arch_pvalue':np.nan,'heteroskedastic':np.nan,'arch_stat':np.nan,'arch_lag_tested':0}\n",
    "    except Exception as e:\n",
    "        log.error(f\"ARCH LM Test unexpectedly failed: {e}\")\n",
    "        return{'arch_pvalue':np.nan,'heteroskedastic':np.nan,'arch_stat':np.nan,'arch_lag_tested':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diebold_mariano_test(actuals: Union[list, np.ndarray, pd.Series],\n",
    "                         forecasts_model: Union[list, np.ndarray, pd.Series],\n",
    "                         forecasts_naive: Union[list, np.ndarray, pd.Series],\n",
    "                         h: int = 1,\n",
    "                         loss_type: str = \"Squared Error\",\n",
    "                         alternative: str = 'less', # 'less': model1 better, 'greater': model2 better, 'two_sided'\n",
    "                         qlike_epsilon: float = 1e-8) -> Dict[str, Any]:\n",
    "    \"\"\"Performs the Diebold-Mariano test using the 'dieboldmariano' library.\"\"\"\n",
    "    log.debug(f\"Starting Diebold-Mariano Test ('dieboldmariano' lib) for h={h}, loss='{loss_type}', alternative='{alternative}', var='bartlett'...\")\n",
    "    results = {'dm_stat': np.nan, 'p_value': np.nan, 'horizon': h, 'loss_type': loss_type, 'alternative': alternative, 'n_obs': 0, 'error': None}\n",
    "\n",
    "    if not DIEBOLDMARIANO_LIB_AVAILABLE or lib_dm_test is None:\n",
    "        results['error'] = \"dieboldmariano library not available.\"\n",
    "        log.error(results['error'])\n",
    "        return results\n",
    "\n",
    "    # Ensure consistent inputs and handle NaNs/Infs\n",
    "    actuals_s = pd.Series(actuals).copy()\n",
    "    fc1_s = pd.Series(forecasts_model).copy()\n",
    "    fc2_s = pd.Series(forecasts_naive).copy()\n",
    "\n",
    "    if not (len(actuals_s) == len(fc1_s) == len(fc2_s)):\n",
    "        results['error'] = f\"Input series must have the same length ({len(actuals_s)} vs {len(fc1_s)} vs {len(fc2_s)})\"\n",
    "        log.error(results['error'])\n",
    "        return results\n",
    "\n",
    "    valid_mask = actuals_s.notna() & fc1_s.notna() & fc2_s.notna() & np.isfinite(actuals_s) & np.isfinite(fc1_s) & np.isfinite(fc2_s)\n",
    "\n",
    "    # Additional check for QLIKE: requires positive values\n",
    "    if loss_type == \"QLIKE\":\n",
    "        pos_mask = (actuals_s > 0) & (fc1_s > 0) & (fc2_s > 0)\n",
    "        valid_mask = valid_mask & pos_mask\n",
    "        log.debug(f\"QLIKE DM Test: After positivity filtering, {valid_mask.sum()} / {len(actuals_s)} observations remain.\")\n",
    "\n",
    "\n",
    "    actuals_clean = actuals_s[valid_mask].tolist()\n",
    "    fc1_clean = fc1_s[valid_mask].tolist()\n",
    "    fc2_clean = fc2_s[valid_mask].tolist()\n",
    "\n",
    "    n_obs = len(actuals_clean)\n",
    "    results['n_obs'] = n_obs\n",
    "    min_obs_dm = 10 # Heuristic minimum for DM test stability\n",
    "    if n_obs < min_obs_dm:\n",
    "        results['error'] = f\"Not enough valid observation triplets ({n_obs}) for DM test (Minimum: {min_obs_dm})\"\n",
    "        log.warning(results['error'])\n",
    "        return results\n",
    "\n",
    "    if h < 1:\n",
    "        results['error'] = f\"Forecast horizon h must be >= 1 (given: {h})\"\n",
    "        log.error(results['error'])\n",
    "        return results\n",
    "\n",
    "    # Define loss function based on type\n",
    "    loss_func: Optional[Callable[[float, float], float]] = None\n",
    "    if loss_type == \"Squared Error\": loss_func = lambda act, pred: (act - pred) ** 2; log.debug(\"Using Squared Error loss for DM test.\")\n",
    "    elif loss_type == \"Absolute Error\": loss_func = lambda act, pred: abs(act - pred); log.debug(\"Using Absolute Error loss for DM test.\")\n",
    "    elif loss_type == \"QLIKE\": loss_func = lambda act, pred: qlike_loss_calc(act, pred, qlike_epsilon); log.debug(f\"Using QLIKE loss for DM test (epsilon={qlike_epsilon}).\")\n",
    "    else: log.warning(f\"Unknown loss_type '{loss_type}'. Using Squared Error.\"); loss_func = lambda act, pred: (act - pred) ** 2; results['loss_type'] = \"Squared Error\"\n",
    "\n",
    "    if loss_func is None: results['error'] = \"Could not define a valid loss function.\"; log.error(results['error']); return results\n",
    "\n",
    "    # Handle alternative hypothesis for the library (which tests P1 vs P2)\n",
    "    # We want to test if Model (fc1) is better than Naive (fc2) -> alternative='less'\n",
    "    is_one_sided = False\n",
    "    p1 = fc1_clean # Model forecast\n",
    "    p2 = fc2_clean # Naive/Benchmark forecast\n",
    "    if alternative == 'less': # H1: Loss(Model) < Loss(Naive)\n",
    "        is_one_sided = True\n",
    "    elif alternative == 'greater': # H1: Loss(Model) > Loss(Naive) -> test Loss(Naive) < Loss(Model)\n",
    "        is_one_sided = True\n",
    "        p1 = fc2_clean # Swap P1 and P2\n",
    "        p2 = fc1_clean\n",
    "    elif alternative == 'two_sided': # H1: Loss(Model) != Loss(Naive)\n",
    "        is_one_sided = False\n",
    "    else:\n",
    "        log.warning(f\"Invalid alternative '{alternative}', using 'less'.\")\n",
    "        alternative = 'less'\n",
    "        results['alternative'] = alternative\n",
    "        is_one_sided = True\n",
    "\n",
    "    try:\n",
    "        # Call the external library function\n",
    "        # Using Bartlett kernel ('harvey_correction=True' implies Bartlett for h>1 in the lib)\n",
    "        dm_stat, p_val = lib_dm_test(\n",
    "            V=actuals_clean,         # Actual values\n",
    "            P1=p1,                   # Forecasts from model 1 (potentially swapped based on alternative)\n",
    "            P2=p2,                   # Forecasts from model 2 (potentially swapped based on alternative)\n",
    "            h=h,                     # Forecast horizon\n",
    "            loss=loss_func,          # Loss function callable\n",
    "            one_sided=is_one_sided,  # Whether the test is one-sided\n",
    "            harvey_correction=True,  # Apply Harvey et al. (1997) correction for small samples\n",
    "            variance_estimator='bartlett' # Specify variance estimator explicitly\n",
    "        )\n",
    "\n",
    "        # Adjust DM stat sign if swapped P1/P2 for 'greater' alternative\n",
    "        if alternative == 'greater':\n",
    "            dm_stat = -dm_stat\n",
    "\n",
    "        results['dm_stat'] = float(dm_stat)\n",
    "        results['p_value'] = float(p_val)\n",
    "        log.info(f\"DM Test ('dieboldmariano' lib, h={h}, loss='{results['loss_type']}', var='bartlett'): Stat={results['dm_stat']:.4f}, p-value({alternative})={results['p_value']:.4f}, N={n_obs}\")\n",
    "\n",
    "    except ValueError as ve:\n",
    "         error_msg = str(ve)\n",
    "         if \"math domain error\" in error_msg.lower():\n",
    "             results['error'] = f\"DM Test (h={h}, loss='{results['loss_type']}', var='bartlett'): Math domain error (likely numerical instability in variance estimation)\"\n",
    "         elif \"log\" in error_msg.lower() and loss_type==\"QLIKE\":\n",
    "             results['error'] = f\"DM Test (h={h}, loss='{results['loss_type']}', var='bartlett'): ValueError, possibly QLIKE related? '{error_msg}'\"\n",
    "         else:\n",
    "             results['error'] = f\"DM Test ('dieboldmariano' lib) ValueError (h={h}, loss='{results['loss_type']}', var='bartlett'): {error_msg}\"\n",
    "         log.error(results['error'])\n",
    "         results['dm_stat'] = np.nan\n",
    "         results['p_value'] = np.nan\n",
    "    except NegativeVarianceException as nve:\n",
    "         # Specific exception from the library\n",
    "         results['error'] = f\"DM Test (h={h}, loss='{results['loss_type']}', var='bartlett'): NegativeVarianceException. Data might be unsuitable. Details: {nve}\"\n",
    "         log.error(results['error'])\n",
    "         results['dm_stat'] = np.nan\n",
    "         results['p_value'] = np.nan\n",
    "    except Exception as e:\n",
    "         error_type_name = type(e).__name__\n",
    "         results['error'] = f\"Unexpected error in DM Test ('dieboldmariano' lib, h={h}, loss='{results['loss_type']}', var='bartlett'): {error_type_name}: {e}\"\n",
    "         log.error(results['error'], exc_info=True)\n",
    "         results['dm_stat'] = np.nan\n",
    "         results['p_value'] = np.nan\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1df85",
   "metadata": {},
   "source": [
    "### 5.5 Data Acquisition and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_yahoo(coin_id: str, start: Optional[str]=None, end: Optional[str]=None) -> pd.DataFrame:\n",
    "    \"\"\"Fetches historical price data from Yahoo Finance.\"\"\"\n",
    "    log.info(f\"Fetching data for {coin_id} from Yahoo ({start} to {end})...\")\n",
    "    if coin_id not in CRYPTO_SYMBOLS: raise ValueError(f\"Unknown Coin ID: {coin_id}\")\n",
    "    ticker=CRYPTO_SYMBOLS[coin_id]\n",
    "\n",
    "    try:\n",
    "        data=yf.download(ticker,start=start,end=end,progress=False,auto_adjust=False, timeout=30)\n",
    "    except Exception as e:\n",
    "        log.error(f\"Download failed for {ticker}: {e}\")\n",
    "        raise ValueError(f\"Data download failed: {e}\")\n",
    "\n",
    "    if data.empty: raise ValueError(f\"No data received for {ticker}\")\n",
    "\n",
    "    # Find Date and Price columns robustly\n",
    "    df=data.copy()\n",
    "    date_source=None\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        log.debug(\"Date found in index.\")\n",
    "        date_source = df.index\n",
    "    else:\n",
    "        # Check common column names, reset index if necessary\n",
    "        potential_date_cols = ['Date', 'Datetime', 'index']\n",
    "        df_reset = df.reset_index()\n",
    "        for col in potential_date_cols:\n",
    "             if col in df_reset.columns:\n",
    "                 log.debug(f\"Date found in column: '{col}'\")\n",
    "                 date_source = df_reset[col]\n",
    "                 df = df_reset # Use reset df if date found in column\n",
    "                 break\n",
    "        # Check index name if not found in columns\n",
    "        if date_source is None:\n",
    "            if df.index.name in potential_date_cols:\n",
    "                 log.debug(\"Date found in index name.\")\n",
    "                 df_reset = df.reset_index()\n",
    "                 date_source = df_reset[df.index.name]\n",
    "                 df = df_reset\n",
    "            else:\n",
    "                raise KeyError(f\"No date source found. Columns: {list(df.columns)}, Index Name: {df.index.name}\")\n",
    "\n",
    "    price_source=None\n",
    "    potential_price_cols = ['Close', 'Adj Close']\n",
    "    col_names_list = list(df.columns) # Get column names (can be tuples if multi-index)\n",
    "    for potential_col in potential_price_cols:\n",
    "        found = False\n",
    "        for col_name in col_names_list:\n",
    "            # Handle simple and multi-index column names\n",
    "            current_col_label = col_name[0] if isinstance(col_name, tuple) else col_name\n",
    "            if current_col_label == potential_col:\n",
    "                price_source = df[col_name]\n",
    "                log.debug(f\"Using price column: '{col_name}'.\")\n",
    "                found = True\n",
    "                break\n",
    "        if found: break\n",
    "\n",
    "    # Try case-insensitive if not found\n",
    "    if price_source is None:\n",
    "        lower_mapping = {}\n",
    "        original_cols = {}\n",
    "        for i, col_name in enumerate(col_names_list):\n",
    "             label = col_name[0] if isinstance(col_name, tuple) else col_name\n",
    "             lower_label = label.lower()\n",
    "             lower_mapping[lower_label] = col_name\n",
    "             original_cols[i] = col_name # Store original name by position/label if needed\n",
    "        for potential_col in potential_price_cols:\n",
    "             potential_col_lower = potential_col.lower()\n",
    "             if potential_col_lower in lower_mapping:\n",
    "                 original_col_name = lower_mapping[potential_col_lower]\n",
    "                 price_source = df[original_col_name]\n",
    "                 log.debug(f\"Using price column (case-insensitive): '{original_col_name}'.\")\n",
    "                 break\n",
    "\n",
    "    if price_source is None: raise KeyError(f\"Could not find closing price column ({potential_price_cols}). Columns found: {list(df.columns)}\")\n",
    "\n",
    "    # Ensure price_source is 1D (Series or flattened array)\n",
    "    if isinstance(price_source, pd.DataFrame):\n",
    "        if price_source.shape[1] == 1:\n",
    "            log.warning(\"Price source is DataFrame, converting to Series.\")\n",
    "            price_source = price_source.iloc[:, 0]\n",
    "        else:\n",
    "            raise ValueError(f\"Price source is DataFrame with unexpected shape: {price_source.shape}\")\n",
    "    elif isinstance(price_source, np.ndarray) and price_source.ndim > 1:\n",
    "        if price_source.ndim == 2 and price_source.shape[1] == 1:\n",
    "            log.warning(f\"Price source is 2D NumPy ({price_source.shape}), flattening.\")\n",
    "            price_source = price_source.flatten()\n",
    "        else:\n",
    "            raise ValueError(f\"Price source is NumPy array with unexpected shape: {price_source.shape}\")\n",
    "\n",
    "    # Create final DataFrame\n",
    "    series_index = date_source if date_source is not None and len(date_source) == len(price_source) else None\n",
    "    if series_index is None and date_source is not None:\n",
    "        log.warning(f\"Length mismatch between date ({len(date_source)}) and price ({len(price_source)}). Index will not be set initially.\")\n",
    "\n",
    "    # Ensure price_source is a Series before creating DataFrame\n",
    "    if not isinstance(price_source, pd.Series):\n",
    "        price_source = pd.Series(price_source, index=series_index, name='price')\n",
    "\n",
    "    try:\n",
    "        if date_source is None: raise ValueError(\"Date source is None.\")\n",
    "        if not isinstance(date_source, (pd.Series, pd.Index)):\n",
    "            date_source = pd.Series(date_source, name='date')\n",
    "        # Build DataFrame\n",
    "        final_df = pd.DataFrame({'date': date_source, 'price': price_source})\n",
    "    except ValueError as e:\n",
    "        log.error(f\"DataFrame creation failed. Price type: {type(price_source)}, Shape: {getattr(price_source, 'shape', 'N/A')}, Ndim: {getattr(price_source, 'ndim', 'N/A')}, Date type: {type(date_source)}, Len: {len(date_source) if date_source is not None else 'None'}\")\n",
    "        raise e\n",
    "\n",
    "    # Standardize types\n",
    "    try: final_df['date']=pd.to_datetime(final_df['date'])\n",
    "    except Exception as e: raise ValueError(f\"Error parsing date column: {e}\")\n",
    "    try: final_df['price']=pd.to_numeric(final_df['price'],errors='coerce')\n",
    "    except Exception as e: log.error(f\"Price conversion to numeric error: {e}\"); raise TypeError(f\"Price conversion failed: {e}\")\n",
    "\n",
    "    if final_df['price'].isnull().all(): raise ValueError(\"Price column is entirely NaN.\")\n",
    "    if final_df['price'].isnull().any(): log.warning(f\"Found {final_df['price'].isnull().sum()} NaNs in price column.\")\n",
    "\n",
    "    # Sort, remove duplicates, reset index\n",
    "    final_df=final_df.sort_values(\"date\").drop_duplicates(subset='date', keep='first').reset_index(drop=True)\n",
    "    log.info(f\"{len(final_df)} rows fetched and prepared for {coin_id}.\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Preprocesses the raw data: handles NaNs, duplicates, calculates log returns.\"\"\"\n",
    "    log.info(\"Preprocessing data...\")\n",
    "    if df.empty: raise ValueError(\"Input DataFrame is empty.\")\n",
    "    df_copy=df.copy()\n",
    "    if 'date' not in df_copy.columns or 'price' not in df_copy.columns: raise KeyError(\"Requires 'date' and 'price' columns.\")\n",
    "\n",
    "    # Convert types, handle initial NaNs\n",
    "    df_copy['date']=pd.to_datetime(df_copy['date'])\n",
    "    df_copy['price']=pd.to_numeric(df_copy['price'],errors='coerce')\n",
    "    df_copy.dropna(subset=['date','price'],inplace=True)\n",
    "    if df_copy.empty: raise ValueError(\"DataFrame empty after dropping NaNs in date/price.\")\n",
    "\n",
    "    # Set index, sort, handle duplicates\n",
    "    df_copy=df_copy.set_index('date').sort_index()\n",
    "    if df_copy.index.has_duplicates:\n",
    "        num_dupes = df_copy.index.duplicated().sum()\n",
    "        log.warning(f\"Found {num_dupes} duplicate dates. Keeping first occurrence.\")\n",
    "        df_copy = df_copy[~df_copy.index.duplicated(keep='first')]\n",
    "\n",
    "    # Resample to daily frequency, filling potential gaps\n",
    "    df_copy = df_copy.resample('D').first() # Use first price if multiple on same day (after duplicate removal)\n",
    "\n",
    "    # Interpolate missing prices (e.g., weekends if original data had gaps)\n",
    "    nan_count=df_copy['price'].isnull().sum()\n",
    "    if nan_count>0:\n",
    "        log.info(f\"Interpolating {nan_count} missing daily prices (using time method)...\")\n",
    "        # Time interpolation is suitable for financial data\n",
    "        df_copy[\"price\"]=df_copy[\"price\"].interpolate(method=\"time\",limit_direction=\"both\", limit_area=None)\n",
    "        nan_count_after = df_copy['price'].isnull().sum()\n",
    "        if nan_count_after > 0:\n",
    "            log.warning(f\"{nan_count_after} NaNs remain after interpolation. Removing rows.\")\n",
    "            df_copy.dropna(subset=[\"price\"],inplace=True)\n",
    "\n",
    "    if df_copy.empty: raise ValueError(\"DataFrame empty after interpolation.\")\n",
    "\n",
    "    # Calculate log returns\n",
    "    if df_copy['price'].nunique() <= 1:\n",
    "        log.warning(\"Price series is constant or near-constant. Log returns will be 0/NaN.\")\n",
    "        df_copy['log_return'] = 0.0\n",
    "    else:\n",
    "        # Use log1p for slightly more numerical stability with small percentage changes\n",
    "        df_copy['log_return']=np.log1p(df_copy['price'].pct_change())\n",
    "        # Replace potential infinities from division by zero (e.g., price going from 0 to non-zero)\n",
    "        df_copy['log_return'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Drop first row (NaN log return) and any other rows with NaN returns\n",
    "    df_copy=df_copy.dropna(subset=['log_return'])\n",
    "    if df_copy.empty: raise ValueError(\"DataFrame empty after calculating log returns and dropping NaNs.\")\n",
    "\n",
    "    # Reset index and select final columns\n",
    "    final_df=df_copy.reset_index()[[\"date\",\"price\",\"log_return\"]]\n",
    "    # Ensure final dtypes are correct\n",
    "    final_df['date'] = pd.to_datetime(final_df['date'])\n",
    "    final_df['price'] = pd.to_numeric(final_df['price'])\n",
    "    final_df['log_return'] = pd.to_numeric(final_df['log_return'])\n",
    "\n",
    "    log.info(f\"Preprocessing finished. Final shape: {final_df.shape}\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46708cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df: pd.DataFrame, ratios: Tuple[float, float, float]=(0.7,0.15,0.15), min_test_size: int=30) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Splits the DataFrame into training, validation, and test sets based on ratios,\n",
    "       prioritizing ratios unless the resulting test set is smaller than min_test_size.\"\"\"\n",
    "    n=len(df)\n",
    "    log.info(f\"Splitting {n} rows, Ratios={ratios}, min_test_size={min_test_size}...\")\n",
    "    tr_ratio, val_ratio, test_ratio = ratios\n",
    "    if not np.isclose(tr_ratio + val_ratio + test_ratio, 1.0): raise ValueError(f\"Split ratios {ratios} must sum to 1.0\")\n",
    "    if n < min_test_size + 20: # Basic check for minimum data\n",
    "        log.warning(f\"Data size {n} potentially too small for split with min_test_size {min_test_size}. Proceeding cautiously.\")\n",
    "        # Fallback to simple min_test_size enforcement if n is very small\n",
    "        if n < min_test_size: raise ValueError(f\"Data size {n} is less than min_test_size {min_test_size}.\")\n",
    "        test_idx_start = max(0, n - min_test_size)\n",
    "        val_idx_start = max(0, test_idx_start - 1) # Minimal validation if possible\n",
    "        train_idx_end = val_idx_start\n",
    "        log.warning(f\"Fallback split due to small N: Train={train_idx_end}, Val=1, Test={n-test_idx_start}\")\n",
    "        return df.iloc[:train_idx_end].copy(), df.iloc[train_idx_end:test_idx_start].copy(), df.iloc[test_idx_start:].copy()\n",
    "\n",
    "\n",
    "    # 1. Calculate split points based *only* on ratios\n",
    "    train_idx_end_ratio = int(n * tr_ratio)\n",
    "    test_idx_start_ratio = int(n * (tr_ratio + val_ratio))\n",
    "    # Ensure indices are within bounds for ratio calculation\n",
    "    train_idx_end_ratio = max(0, min(train_idx_end_ratio, n))\n",
    "    test_idx_start_ratio = max(train_idx_end_ratio, min(test_idx_start_ratio, n))\n",
    "\n",
    "    # 2. Calculate the resulting test set size based on ratios\n",
    "    test_size_ratio = n - test_idx_start_ratio\n",
    "\n",
    "    # 3. Check if ratio-based test set meets minimum size\n",
    "    if test_size_ratio < min_test_size:\n",
    "        log.warning(f\"Ratio-based test set size ({test_size_ratio}) is smaller than min_test_size ({min_test_size}). Adjusting split to meet minimum.\")\n",
    "        # Enforce minimum test size by adjusting test_idx_start\n",
    "        test_idx_start = max(0, n - min_test_size)\n",
    "        # Adjust validation start to not overlap test, but try to respect original train ratio if possible\n",
    "        val_idx_start = min(train_idx_end_ratio, test_idx_start)\n",
    "        val_idx_start = max(0, val_idx_start) # Ensure validation start is not negative\n",
    "        test_idx_start = max(val_idx_start, test_idx_start) # Ensure test starts after validation\n",
    "\n",
    "        log.warning(f\"Adjusted split: Train end index = {val_idx_start}, Test start index = {test_idx_start}\")\n",
    "\n",
    "    else:\n",
    "        # Ratios result in a large enough test set, use ratio-based indices\n",
    "        log.info(\"Ratio-based split meets minimum test size requirement.\")\n",
    "        val_idx_start = train_idx_end_ratio\n",
    "        test_idx_start = test_idx_start_ratio\n",
    "\n",
    "    # Perform the final split using the determined indices\n",
    "    train = df.iloc[:val_idx_start].copy()\n",
    "    val = df.iloc[val_idx_start:test_idx_start].copy()\n",
    "    test = df.iloc[test_idx_start:].copy()\n",
    "\n",
    "    log.info(f\"Final split result: Train={len(train)}, Val={len(val)}, Test={len(test)}\")\n",
    "    # Final confirmation (should always be >= min_test_size now unless n was too small initially)\n",
    "    if len(test) < min_test_size and n >= min_test_size:\n",
    "         log.error(f\"Logic error: Final test size {len(test)} is still less than minimum {min_test_size}!\")\n",
    "\n",
    "    return train,val,test\n",
    "\n",
    "# --- Modeling Functions ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcff353",
   "metadata": {},
   "source": [
    "### 5.6 ARIMA-GARCH Model Fitting and Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d38bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_arima_garch(series:pd.Series,arima_order:Tuple[int,int,int],garch_order:Tuple[int,int],garch_type:str='GARCH',distribution:str='t',sf:float=100.0)->Tuple[Optional[Any],Optional[Any],float]:\n",
    "    \"\"\"Fits an ARIMA model followed by a GARCH model on the residuals.\"\"\"\n",
    "    # Note: arima_order d is ignored here, differencing is handled externally.\n",
    "    # Fit ARIMA(p, 0, q) on the (potentially already differenced) series.\n",
    "    p, _, q = arima_order # Unpack p, d, q but use only p, q for ARIMA fit\n",
    "    gp, gq = garch_order\n",
    "    log.debug(f\"Fitting ARIMA({p},0,{q}) + {garch_type}({gp},{gq}) dist={distribution}... Input length: {len(series)}\")\n",
    "\n",
    "    actual_vol_model = garch_type.upper()\n",
    "    # Determine if GARCH model requires asymmetry term 'o' (usually 1 for EGARCH, GJR etc.)\n",
    "    o_asym = 1 if actual_vol_model in ['GJR','TARCH','EGARCH','APARCH'] else 0\n",
    "\n",
    "    # Minimum length checks\n",
    "    min_len_arima = max(p, q) + 5 # Heuristic\n",
    "    min_len_garch = max(gp, gq, o_asym) + 5 # Heuristic\n",
    "    min_len_practical = 15 # Absolute minimum\n",
    "    min_len = max(min_len_arima, min_len_garch, min_len_practical)\n",
    "\n",
    "    if len(series)<min_len:\n",
    "        log.error(f\"Series length {len(series)} < required minimum {min_len} for orders {(p,q), (gp,gq)}.\")\n",
    "        raise ValueError(f\"Series length {len(series)} < required minimum {min_len}\")\n",
    "\n",
    "    # Scale the series (often helps convergence)\n",
    "    s_sc=series*sf\n",
    "    arima_result=None\n",
    "    garch_result=None\n",
    "\n",
    "    try:\n",
    "        # Fit ARIMA(p, 0, q) model\n",
    "        log.debug(f\"Fitting ARIMA({p},0,{q}) with trend='c'...\")\n",
    "        # Ensure 1D input for ARIMA\n",
    "        if hasattr(s_sc, 'ndim') and s_sc.ndim != 1:\n",
    "             log.warning(f\"ARIMA input not 1D (Shape {s_sc.shape}), flattening.\")\n",
    "             s_sc = s_sc.iloc[:, 0] if isinstance(s_sc, pd.DataFrame) and s_sc.shape[1] == 1 else np.asarray(s_sc).flatten()\n",
    "             s_sc = pd.Series(s_sc, index=getattr(s_sc, 'index', None)) # Try to preserve index\n",
    "        if not isinstance(s_sc, (pd.Series, np.ndarray)): s_sc = pd.Series(s_sc) # Ensure Series/ndarray\n",
    "\n",
    "        # Instantiate ARIMA model\n",
    "        # enforce_stationarity/invertibility=False allows fitting non-stationary models if needed,\n",
    "        # though we aim for stationarity via external differencing. 'trend=c' includes a constant.\n",
    "        arima_model=ARIMA(s_sc,order=(p,0,q),trend='c',enforce_stationarity=False,enforce_invertibility=False)\n",
    "        try:\n",
    "            # Fit the model with potentially tighter tolerances for convergence\n",
    "            arima_result=arima_model.fit(method_kwargs={\"xtol\": 1e-6, \"ftol\": 1e-6, \"maxiter\": 1000})\n",
    "            log.debug(f\"ARIMA fit successful: LLF={arima_result.llf:.2f}\")\n",
    "        except (np.linalg.LinAlgError, ValueError, ConvergenceWarning, HessianInversionWarning) as arima_fit_e:\n",
    "            log.warning(f\"ARIMA fit failed: {type(arima_fit_e).__name__}: {arima_fit_e}\")\n",
    "            return None, None, sf # Return None if ARIMA fails\n",
    "        except Exception as arima_fit_generic_e:\n",
    "            log.error(f\"Unexpected ARIMA fit error: {arima_fit_generic_e}\")\n",
    "            return None, None, sf\n",
    "\n",
    "        # Fit GARCH model on ARIMA residuals\n",
    "        residuals_scaled=arima_result.resid.copy()\n",
    "        if residuals_scaled.isnull().any() or not np.all(np.isfinite(residuals_scaled)):\n",
    "            log.error(\"ARIMA residuals contain NaN or Inf. Cannot fit GARCH.\")\n",
    "            return arima_result,None,sf # Return ARIMA result, but no GARCH\n",
    "\n",
    "        # Check residual length for GARCH\n",
    "        min_len_garch_resid = max(gp, gq, o_asym) + 5\n",
    "        if len(residuals_scaled)<min_len_garch_resid:\n",
    "            log.error(f\"Residual length {len(residuals_scaled)} < GARCH required min {min_len_garch_resid}.\")\n",
    "            return arima_result,None,sf\n",
    "\n",
    "        # Ensure 1D input for GARCH\n",
    "        if hasattr(residuals_scaled, 'ndim') and residuals_scaled.ndim != 1:\n",
    "             log.warning(f\"GARCH input (residuals) not 1D (Shape {residuals_scaled.shape}), flattening.\")\n",
    "             residuals_scaled = residuals_scaled.iloc[:, 0] if isinstance(residuals_scaled, pd.DataFrame) and residuals_scaled.shape[1] == 1 else np.asarray(residuals_scaled).flatten()\n",
    "             residuals_scaled = pd.Series(residuals_scaled, index=getattr(residuals_scaled, 'index', None))\n",
    "        if not isinstance(residuals_scaled, (pd.Series, np.ndarray)): residuals_scaled = pd.Series(residuals_scaled)\n",
    "\n",
    "        # Instantiate GARCH model using arch_model from 'arch' library\n",
    "        # mean='Zero' because when modeling residuals from ARIMA which should have zero mean\n",
    "        # rescale=False as scaled manually\n",
    "        if actual_vol_model == 'EGARCH':\n",
    "            log.debug(f\"Fitting EGARCH(p={gp},o={o_asym},q={gq}) dist={distribution} mean=Zero...\")\n",
    "            garch_model=arch_model(residuals_scaled,p=gp,o=o_asym,q=gq, vol='EGARCH', dist=distribution,mean='Zero',rescale=False)\n",
    "        elif actual_vol_model == 'FIGARCH':\n",
    "            log.debug(f\"Fitting FIGARCH(p={gp},q={gq}) dist={distribution} mean=Zero...\")\n",
    "            # FIGARCH has specific constraints (p<=1, q<=1 in arch implementation?) handled internally\n",
    "            garch_model=arch_model(residuals_scaled,p=gp, q=gq, vol='FIGARCH', dist=distribution,mean='Zero',rescale=False)\n",
    "        else: # Standard GARCH, GJR, TARCH etc.\n",
    "             log.debug(f\"Fitting {actual_vol_model}(p={gp},o={o_asym},q={gq}) dist={distribution} mean=Zero...\")\n",
    "             # arch_model uses vol='GARCH' for GJR/TARCH, distinguished by o>0\n",
    "             vol_param_name = 'GARCH' if actual_vol_model in ['GJR', 'TARCH'] else actual_vol_model\n",
    "             garch_model=arch_model(residuals_scaled,p=gp,o=o_asym,q=gq,vol=vol_param_name,dist=distribution,mean='Zero',rescale=False)\n",
    "\n",
    "        try:\n",
    "            # Fit GARCH model, suppress output, set max iterations\n",
    "             garch_result=garch_model.fit(disp='off',show_warning=False,options={'maxiter':500})\n",
    "             log.debug(f\"GARCH fit successful: LLF={garch_result.loglikelihood:.2f}\")\n",
    "             # Check convergence flag (0 is success)\n",
    "             if garch_result.convergence_flag!=0:\n",
    "                 log.warning(f\"GARCH did not converge (Flag:{garch_result.convergence_flag}). Results may be unreliable.\")\n",
    "                 # Keep the result but be aware it might be invalid\n",
    "        except (ValueError, np.linalg.LinAlgError, ConvergenceWarning, HessianInversionWarning, RuntimeError) as garch_fit_e:\n",
    "             # Handle common GARCH fit errors\n",
    "             if \"xt contain the same number\" in str(garch_fit_e): log.error(f\"GARCH fit failed: Input alignment issue: {garch_fit_e}\")\n",
    "             elif \"Inputs have different indexes\" in str(garch_fit_e): log.error(f\"GARCH fit failed: Index mismatch: {garch_fit_e}\")\n",
    "             elif \"optimization failed\" in str(garch_fit_e).lower(): log.warning(f\"GARCH fit failed: Explicit convergence failure: {garch_fit_e}\")\n",
    "             elif \"p and q must be either 0 or 1\" in str(garch_fit_e) and actual_vol_model == 'FIGARCH': log.warning(f\"FIGARCH p/q constraint likely failed: {garch_fit_e}\") # Specific FIGARCH warning\n",
    "             else: log.warning(f\"GARCH fit failed: {type(garch_fit_e).__name__}: {garch_fit_e}\")\n",
    "             garch_result = None # Set GARCH result to None on failure\n",
    "        except Exception as garch_fit_generic_e:\n",
    "            log.error(f\"Unexpected GARCH fit error: {garch_fit_generic_e}\")\n",
    "            garch_result = None\n",
    "\n",
    "    except (ValueError,TypeError,IndexError) as model_setup_e:\n",
    "        # Catch errors during model instantiation (e.g., invalid orders, data types)\n",
    "        if \"1-dimensional\" in str(model_setup_e):\n",
    "            log.critical(f\"Fit Setup failed: Dimension error: {model_setup_e}\")\n",
    "        else:\n",
    "            log.warning(f\"Model Setup failed: {type(model_setup_e).__name__}: {model_setup_e}\")\n",
    "        arima_result = None\n",
    "        garch_result = None\n",
    "    except Exception as e:\n",
    "        log.error(f\"Unexpected error during fitting process: {e}\")\n",
    "        arima_result=None\n",
    "        garch_result=None\n",
    "\n",
    "    return arima_result,garch_result,sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_arima_garch(arima_res: Any, garch_res: Any, steps: int = 1, sf: float = 100.0) -> Tuple[pd.Series, pd.Series]: # <-- Return Tuple of Series\n",
    "    \"\"\"Generates forecasts from fitted ARIMA and GARCH models. Returns mean_return Series and variance Series.\"\"\"\n",
    "    MAX_VARIANCE_CLIP = 1.0 # Define the missing constant\n",
    "    fc_index = pd.RangeIndex(1, steps + 1)\n",
    "    # Initialize Series directly\n",
    "    mean_ret_fc = pd.Series(np.nan, index=fc_index, name='mean_return')\n",
    "    variance_fc = pd.Series(np.nan, index=fc_index, name='variance')\n",
    "\n",
    "    if arima_res is None:\n",
    "        log.warning(\"ARIMA result missing, cannot generate forecast.\")\n",
    "        return mean_ret_fc, variance_fc # Return tuple of NaNs\n",
    "    if steps <= 0:\n",
    "        log.error(\"Forecast steps must be > 0.\")\n",
    "        return mean_ret_fc, variance_fc # Return tuple of NaNs\n",
    "\n",
    "    try:\n",
    "        # 1. ARIMA Mean Forecast\n",
    "        log.debug(f\"Generating {steps}-step ARIMA mean forecast...\")\n",
    "        arima_forecast_obj = arima_res.get_forecast(steps=steps)\n",
    "        mean_scaled = arima_forecast_obj.predicted_mean\n",
    "        mean_forecast = mean_scaled / sf\n",
    "\n",
    "        if len(mean_forecast) != steps:\n",
    "             log.warning(f\"ARIMA forecast length ({len(mean_forecast)}) != requested steps ({steps}). Adjusting.\")\n",
    "             mean_forecast_vals = mean_forecast.values[:steps]\n",
    "             if len(mean_forecast_vals) < steps:\n",
    "                 mean_forecast_vals = np.pad(mean_forecast_vals, (0, steps - len(mean_forecast_vals)), constant_values=np.nan)\n",
    "        else:\n",
    "            mean_forecast_vals = mean_forecast.values\n",
    "        mean_ret_fc[:] = mean_forecast_vals # Assign values to the Series\n",
    "\n",
    "        # 2. GARCH Variance Forecast\n",
    "        variance_forecast_vals = np.full(steps, np.nan) # Use numpy array for calculation\n",
    "        if garch_res is not None:\n",
    "             log.debug(f\"Generating {steps}-step GARCH variance forecast...\")\n",
    "             try:\n",
    "                 fc_method = 'simulation' if steps > 1 else 'analytic'\n",
    "                 garch_forecast_obj = garch_res.forecast(horizon=steps, reindex=False, method=fc_method)\n",
    "                 var_data = None\n",
    "                 # Correct way to access ARCH forecast variance: .variance attribute returns a DataFrame\n",
    "                 if hasattr(garch_forecast_obj, 'variance') and not garch_forecast_obj.variance.empty:\n",
    "                     var_data = garch_forecast_obj.variance.iloc[0].values # Get first row (forecasts)\n",
    "                 # Fallback for older versions or different model types\n",
    "                 elif hasattr(garch_forecast_obj, 'residual_variance') and not garch_forecast_obj.residual_variance.empty:\n",
    "                     log.warning(\"Using 'residual_variance' from GARCH forecast - check if this is intended.\")\n",
    "                     var_data = garch_forecast_obj.residual_variance.iloc[0].values\n",
    "                 else:\n",
    "                     log.warning(f\"No variance data found in GARCH forecast object (method='{fc_method}'). Attributes: {dir(garch_forecast_obj)}\")\n",
    "\n",
    "\n",
    "                 if var_data is not None:\n",
    "                      variance_scaled = np.asarray(var_data).flatten()[:steps]\n",
    "                      if len(variance_scaled) < steps: variance_scaled = np.pad(variance_scaled, (0, steps-len(variance_scaled)), constant_values=np.nan)\n",
    "                      var_scaled_clean = np.where(np.isnan(variance_scaled), np.nan, np.maximum(0, variance_scaled))\n",
    "                      variance_forecast_raw = var_scaled_clean / (sf**2)\n",
    "                      variance_forecast_vals = np.clip(variance_forecast_raw, a_min=0, a_max=MAX_VARIANCE_CLIP) # Use defined constant\n",
    "                      if np.any(variance_forecast_raw > MAX_VARIANCE_CLIP): log.warning(f\"Variance clipped at steps {np.where(variance_forecast_raw > MAX_VARIANCE_CLIP)[0]+1}\")\n",
    "                 else: variance_forecast_vals = np.full(steps, np.nan)\n",
    "             except Exception as ge: log.error(f\"GARCH forecast error: {ge}\")\n",
    "        variance_fc[:] = variance_forecast_vals # Assign values to the Series\n",
    "\n",
    "        m0 = f\"{mean_ret_fc.iloc[0]:.6f}\" if not mean_ret_fc.empty and pd.notna(mean_ret_fc.iloc[0]) else \"NaN\"\n",
    "        v0 = f\"{variance_fc.iloc[0]:.8f}\" if not variance_fc.empty and pd.notna(variance_fc.iloc[0]) else \"NaN\"\n",
    "        log.debug(f\"{steps}-step fc generated. Mean[h=1]={m0}, Var[h=1]={v0}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"Overall forecast generation error: {e}\")\n",
    "        # Return Series of NaNs on error\n",
    "        return mean_ret_fc.fillna(np.nan), variance_fc.fillna(np.nan)\n",
    "\n",
    "    return mean_ret_fc, variance_fc # Return tuple of Series\n",
    "\n",
    "# --- Model Selection / Tuning Helpers ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1603574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_candidate_on_val(val_series:pd.Series,arima_order_tune:Tuple[int,int,int],garch_order_tune:Tuple[int,int],garch_type_tune:str,garch_dist_tune:str,sf:float=100.0,tune_criterion:str='BIC')->Tuple[Optional[float],Optional[str]]:\n",
    "    \"\"\"Fits a candidate model on validation data and returns its information criterion.\"\"\"\n",
    "    p, _, q = arima_order_tune # d is handled externally\n",
    "    gp, gq = garch_order_tune\n",
    "    min_len_val = max(sum(arima_order_tune[:1]+arima_order_tune[2:])+10, max(garch_order_tune)+10, 25) # Heuristic min length\n",
    "\n",
    "    if val_series.empty or len(val_series)< min_len_val:\n",
    "        return None, f\"Validation data too short ({len(val_series)}<{min_len_val})\"\n",
    "\n",
    "    description=f\"ARIMA({p},0,{q})+{garch_type_tune}({gp},{gq})({garch_dist_tune})\"\n",
    "    try:\n",
    "        # Fit the model on the validation series (which should be already differenced if needed)\n",
    "        ar_val,gr_val,_ = fit_arima_garch(val_series,(p,0,q),(gp,gq),garch_type_tune,garch_dist_tune,sf)\n",
    "\n",
    "        # Check fit results\n",
    "        if ar_val is None: return None, \"ARIMA Fit Failed on Validation\"\n",
    "        # If GARCH was expected but failed\n",
    "        if gr_val is None and garch_type_tune.upper() != 'NONE': return None,\"GARCH Fit Failed on Validation\"\n",
    "        # Check GARCH convergence if applicable\n",
    "        if gr_val is not None and hasattr(gr_val,'convergence_flag') and gr_val.convergence_flag!=0:\n",
    "            return None,f\"GARCH Did Not Converge on Validation (Flag:{gr_val.convergence_flag})\"\n",
    "\n",
    "       # Calculate information criterion\n",
    "        log_likelihood = np.nan\n",
    "        num_params = 0\n",
    "        num_obs = 0\n",
    "        if gr_val is not None: # Use GARCH results if available (preferred for joint likelihood)\n",
    "            log_likelihood=getattr(gr_val,'loglikelihood',np.nan)\n",
    "            k_arima=len(getattr(ar_val,'params',[])) # ARIMA params (incl constant)\n",
    "            k_garch=len(getattr(gr_val,'params',[])) # GARCH params (vol + dist)\n",
    "            num_params=k_arima+k_garch\n",
    "            num_obs=getattr(gr_val,'nobs',len(val_series))\n",
    "        elif ar_val is not None: # Fallback to ARIMA-only stats if no GARCH\n",
    "             log_likelihood=getattr(ar_val,'llf',np.nan)\n",
    "             num_params=len(getattr(ar_val,'params',[]))\n",
    "             num_obs=getattr(ar_val,'nobs',len(val_series))\n",
    "        else:\n",
    "             return None, \"No model results available for criterion calculation\"\n",
    "\n",
    "        # Validate inputs for criterion calculation\n",
    "        if not np.isfinite(log_likelihood) or num_params<=0 or num_obs<=num_params:\n",
    "            reason = f\"Invalid criterion inputs (LLF:{log_likelihood}, k:{num_params}, n:{num_obs})\"\n",
    "            log.warning(f\"{description}: {reason}\")\n",
    "            return None, reason\n",
    "\n",
    "        criterion_value=np.inf\n",
    "        criterion_used=tune_criterion.upper()\n",
    "        if criterion_used=='BIC':\n",
    "            criterion_value=-2*log_likelihood+num_params*np.log(num_obs)\n",
    "        elif criterion_used=='AIC':\n",
    "            criterion_value=-2*log_likelihood+2*num_params\n",
    "        else:\n",
    "            log.warning(f\"Unknown tuning criterion '{tune_criterion}', using BIC.\")\n",
    "            criterion_value=-2*log_likelihood+num_params*np.log(num_obs)\n",
    "\n",
    "        if not np.isfinite(criterion_value):\n",
    "            return None,\"NaN/Inf criterion result\"\n",
    "\n",
    "        # Return criterion value and None for error message\n",
    "        return float(criterion_value),None\n",
    "\n",
    "    except ValueError as ve:\n",
    "        log.warning(f\"Evaluation ValueError for {description}: {ve}\")\n",
    "        return None, f\"ValueError: {ve}\"\n",
    "    except Exception as e:\n",
    "        log.error(f\"Unexpected evaluation error for {description}: {e}\")\n",
    "        return None,f\"Exception: {type(e).__name__}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2486fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_differencing(series:pd.Series,adf_result:Dict,kpss_result:Dict,manual_d:Optional[int],auto_add_d:Optional[int],max_d:int=2,adf_alpha:float=0.05,kpss_alpha:float=0.05)->Tuple[pd.Series,int,str]:\n",
    "    \"\"\"Determines the differencing order 'd' based on tests, manual input, or tuning suggestion.\"\"\"\n",
    "    final_d=0\n",
    "    reason=\"Initial stationarity tests\"\n",
    "    base_d=0 # d based on initial tests\n",
    "    add_d=0 # Additional d from tuning\n",
    "\n",
    "    if manual_d is not None and manual_d >= 0:\n",
    "        # Manual override\n",
    "        final_d=max(0,min(manual_d,max_d)) # Apply max_d constraint\n",
    "        reason=f\"Manual d={manual_d}\"\n",
    "        if final_d!=manual_d: reason += f\" (limited to {final_d} by max_d={max_d})\"\n",
    "        # Check if manual choice contradicts tests\n",
    "        adf_stat=adf_result.get('is_stationary', None)\n",
    "        kpss_stat=kpss_result.get('is_stationary', None)\n",
    "        if adf_stat is True and kpss_stat is True and final_d > 0:\n",
    "            log.warning(f\"{reason}, but initial tests suggest d=0.\")\n",
    "        elif adf_stat is False and kpss_stat is False and final_d == 0:\n",
    "            log.warning(f\"{reason}, but initial tests suggest d>0.\")\n",
    "        log.info(f\"Applying: {reason}\")\n",
    "    else:\n",
    "        # Automatic differencing based on tests and tuning\n",
    "        adf_stat=adf_result.get('is_stationary',False)\n",
    "        kpss_stat=kpss_result.get('is_stationary',False)\n",
    "        adf_p=adf_result.get('p_value',np.nan)\n",
    "        kpss_p=kpss_result.get('p_value',np.nan)\n",
    "\n",
    "        adf_msg = f\"ADF Stat={adf_stat}(p={adf_p:.3f})\" if pd.notna(adf_p) else f\"ADF Stat={adf_stat}(p=N/A)\"\n",
    "        kpss_msg= f\"KPSS Stat={kpss_stat}(p={kpss_p:.3f})\" if pd.notna(kpss_p) else f\"KPSS Stat={kpss_stat}(p=N/A)\"\n",
    "\n",
    "        # Determine base differencing order from tests\n",
    "        if adf_stat and kpss_stat: # Both suggest stationary\n",
    "            base_d = 0\n",
    "            reason=f\"Base d=0 ({adf_msg}, {kpss_msg})\"\n",
    "        elif not adf_stat and not kpss_stat: # Both suggest non-stationary\n",
    "            base_d = 1\n",
    "            reason=f\"Base d=1 ({adf_msg}, {kpss_msg})\"\n",
    "        else: # Tests conflict, often safer to difference once\n",
    "            base_d = 1\n",
    "            reason=f\"Base d=1 (Conflict: {adf_msg}, {kpss_msg})\"\n",
    "            log.warning(f\"ADF/KPSS tests conflict. Defaulting to base d=1.\")\n",
    "\n",
    "        # Add differencing order suggested by auto-tuning (if applicable)\n",
    "        add_d = auto_add_d if auto_add_d is not None else 0\n",
    "        if add_d > 0:\n",
    "            reason += f\" + AutoTune add_d={add_d}\"\n",
    "\n",
    "        recommended_d = base_d + add_d\n",
    "        final_d = max(0, min(recommended_d, max_d)) # Apply max_d constraint\n",
    "\n",
    "        if final_d != recommended_d:\n",
    "            reason += f\" = {recommended_d} (limited to {final_d} by max_d={max_d})\"\n",
    "        else:\n",
    "            reason += f\" = {final_d}\"\n",
    "\n",
    "        log.info(f\"Final differencing order d={final_d} determined by: {reason}\")\n",
    "\n",
    "    diff_reason_final=f\"Applied d={final_d} ({reason})\"\n",
    "\n",
    "    # Apply differencing\n",
    "    if final_d > 0:\n",
    "        try:\n",
    "            differenced_series=difference_series(series,order=final_d)\n",
    "            # Check if differencing resulted in empty series (can happen if order > length)\n",
    "            if differenced_series.empty and not series.empty:\n",
    "                 log.error(f\"Differencing d={final_d} resulted in empty series. Falling back to d=0.\")\n",
    "                 return series.copy(),0,f\"{diff_reason_final} - ERROR: Differencing failed, using d=0.\"\n",
    "            # Optional: Check stationarity *after* differencing\n",
    "            if not differenced_series.empty:\n",
    "                 log.info(f\"Checking stationarity after applying d={final_d}...\")\n",
    "                 adf_post=adf_test(differenced_series,significance=adf_alpha)\n",
    "                 kpss_post=kpss_test(differenced_series,significance=kpss_alpha)\n",
    "                 adf_s_post=adf_post.get('is_stationary',False)\n",
    "                 kpss_s_post=kpss_post.get('is_stationary',False)\n",
    "                 log.info(f\"  ADF after d={final_d}: p={adf_post.get('p_value',np.nan):.4f}, Stationary={adf_s_post}\")\n",
    "                 log.info(f\" KPSS after d={final_d}: p={kpss_post.get('p_value',np.nan):.4f}, Stationary={kpss_s_post}\")\n",
    "                 if not adf_s_post or not kpss_s_post:\n",
    "                     log.warning(f\"Series potentially still not stationary after d={final_d} based on tests.\")\n",
    "            return differenced_series, final_d, diff_reason_final\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error during differencing d={final_d}: {e}. Falling back to d=0.\")\n",
    "            return series.copy(),0,f\"{diff_reason_final} - ERROR: Differencing exception, using d=0.\"\n",
    "    else:\n",
    "        # No differencing applied\n",
    "        return series.copy(), 0, diff_reason_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_tune_arima_garch(\n",
    "    data: pd.Series, min_p: int, max_p: int, min_q: int, max_q: int, min_d: int, max_d: int, # ARIMA orders (d is *additional*)\n",
    "    min_gp: int, max_gp: int, min_gq: int, max_gq: int, # GARCH orders\n",
    "    garch_types_list: List[str], # List of GARCH types ('GARCH', 'EGARCH'...)\n",
    "    sf: float = 100.0, tune_criterion: str = 'BIC',\n",
    "    param_sig_level: float = 0.10, verbose: bool = False # <-- Added verbose parameter here\n",
    ") -> Dict:\n",
    "    \"\"\"Automatically tunes ARIMA-GARCH model orders based on criterion and significance.\"\"\"\n",
    "\n",
    "    if data.isnull().any(): raise ValueError(\"Input data for tuning contains NaN.\")\n",
    "    min_len_tune = 30 # Minimum length for reliable tuning fits\n",
    "    if len(data) < min_len_tune: raise ValueError(f\"Series length {len(data)} < {min_len_tune} required for tuning.\")\n",
    "\n",
    "    best_model = {\n",
    "        'arima': (1, 0, 1), # (p, 0, q) - d is handled separately\n",
    "        'add_d_recommended': 0, # Recommended *additional* differencing\n",
    "        'garch_order': (1, 1),\n",
    "        'garch_type': 'GARCH',\n",
    "        'garch_dist': 't',\n",
    "        'criterion_value': np.inf,\n",
    "        'criterion_used': tune_criterion.upper(),\n",
    "        'stable_and_significant': False, # Flag if a good model was found\n",
    "        'error': None\n",
    "    }\n",
    "\n",
    "    # Define ranges for orders\n",
    "    p_range = range(min_p, max_p + 1)\n",
    "    q_range = range(min_q, max_q + 1)\n",
    "    add_d_range = range(min_d, max_d + 1) # Range for *additional* d\n",
    "    actual_min_gp = max(0, min_gp) # GARCH p,q can be 0 for pure ARIMA\n",
    "    actual_min_gq = max(0, min_gq)\n",
    "    gp_range = range(actual_min_gp, max_gp + 1)\n",
    "    gq_range = range(actual_min_gq, max_gq + 1)\n",
    "    dist_options = ['normal', 't'] if scipy_stats_available else ['normal', 't']\n",
    "    if not garch_types_list: garch_types_list = ['GARCH'] # Default if empty\n",
    "\n",
    "    # Create combinations, applying FIGARCH constraints\n",
    "    combo_iter = itertools.product(p_range, add_d_range, q_range, gp_range, gq_range, garch_types_list, dist_options)\n",
    "    valid_combinations = []\n",
    "    for combo in combo_iter:\n",
    "        p_c, add_d_c, q_c, gp_c, gq_c, garch_type_c, g_dist_c = combo\n",
    "        is_figarch = garch_type_c.upper() == 'FIGARCH'\n",
    "        is_pure_arima_candidate = (gp_c == 0 and gq_c == 0)\n",
    "\n",
    "        if is_figarch:\n",
    "            # FIGARCH in 'arch' might have constraints like p<=1, q<=1. Check documentation.\n",
    "            # Assume p=0 or q=0 is allowed if the other is 1. Allow (1,1). Don't allow (0,0).\n",
    "            if gp_c <= 1 and gq_c <= 1 and not is_pure_arima_candidate:\n",
    "                valid_combinations.append(combo)\n",
    "        elif not is_pure_arima_candidate: # Standard GARCH models require p>0 or q>0\n",
    "             valid_combinations.append(combo)\n",
    "        elif is_pure_arima_candidate: # Allow pure ARIMA (gp=0, gq=0)\n",
    "             # Ensure only one GARCH type ('None') and one dist ('None') for pure ARIMA\n",
    "             temp_combo = (p_c, add_d_c, q_c, 0, 0, 'None', 'None')\n",
    "             if temp_combo not in valid_combinations: # Add only once\n",
    "                 valid_combinations.append(temp_combo)\n",
    "\n",
    "\n",
    "    combinations = valid_combinations # Final list of combinations to test\n",
    "    total_combinations = len(combinations)\n",
    "\n",
    "    if total_combinations == 0:\n",
    "        log.warning(\"No valid model combinations generated for tuning (check constraints/ranges).\")\n",
    "        best_model['error'] = \"No valid combinations\"\n",
    "        return best_model\n",
    "\n",
    "    log.info(f\"Auto-Tuning ~{total_combinations} valid combinations (Criterion: {best_model['criterion_used']}, Sig Level: <{param_sig_level})...\")\n",
    "\n",
    "    # Counters for tracking progress/issues\n",
    "    count = 0; fits_ok = 0; skips_data = 0; skips_fit = 0; skips_conv = 0; skips_sig = 0; skips_crit = 0\n",
    "\n",
    "    for p, add_d, q, gp, gq, garch_type, g_dist in combinations:\n",
    "        count += 1\n",
    "        arima_order_current = (p, 0, q) # Use 0 for d within fit function\n",
    "        garch_order_current = (gp, gq)\n",
    "        is_pure_arima = (garch_type == 'None') # Check our marker\n",
    "        model_desc = f\"ARIMA({p},{add_d},{q})\" + (f\" + {garch_type}({gp},{gq}) ({g_dist})\" if not is_pure_arima else \" (Pure ARIMA)\")\n",
    "\n",
    "        if verbose or (count % max(1, total_combinations // 1) == 0): # Log progress periodically\n",
    "            prog = (count / total_combinations * 100) if total_combinations > 0 else 0\n",
    "            best_v_str = f\"{best_model['criterion_value']:.2f}\" if best_model['stable_and_significant'] else \"inf\"\n",
    "            log.info(f\" Tuning: {count}/{total_combinations} ({prog:.1f}%) | Testing: {model_desc} | Best ({best_model['criterion_used']}): {best_v_str}\")\n",
    "\n",
    "        # 1. Apply additional differencing for this candidate\n",
    "        try:\n",
    "            series_diff = difference_series(data, order=add_d) if add_d > 0 else data.copy()\n",
    "            # Check length after differencing\n",
    "            min_len_req_arima = p + q + 10 if p>0 or q>0 else 0\n",
    "            min_len_req_garch = max(1, gp, gq) + 10 if not is_pure_arima else 0 # Need >0 for max()\n",
    "            min_len_req = max(min_len_req_arima, min_len_req_garch, min_len_tune)\n",
    "\n",
    "            if series_diff.empty or len(series_diff) < min_len_req:\n",
    "                if verbose: log.debug(f\" Skip {model_desc}: Data too short ({len(series_diff)}<{min_len_req}) after add_d={add_d}\")\n",
    "                skips_data += 1; continue\n",
    "        except Exception as e:\n",
    "            log.warning(f\" Skip {model_desc}: Differencing d={add_d} failed ({e})\")\n",
    "            skips_data += 1; continue\n",
    "\n",
    "        # 2. Fit the model (ARIMA or ARIMA-GARCH)\n",
    "        ar = None; gr = None; fit_ok = False\n",
    "        try:\n",
    "            if is_pure_arima:\n",
    "                 # Fit ARIMA only, handle p=0, q=0 case\n",
    "                 am = ARIMA(series_diff,order=(0,0,0),trend='c') if p==0 and q==0 else ARIMA(series_diff,order=(p,0,q),trend='c')\n",
    "                 ar=am.fit(); fit_ok = (ar is not None); gr = None\n",
    "                 if fit_ok and verbose: log.debug(f\"Pure ARIMA fit OK: LLF={ar.llf:.2f}\")\n",
    "            else:\n",
    "                 # Fit combined ARIMA-GARCH\n",
    "                 ar, gr, _ = fit_arima_garch(series_diff, arima_order_current, garch_order_current, garch_type, g_dist, sf)\n",
    "                 fit_ok = (ar is not None and gr is not None)\n",
    "                 # Check GARCH convergence explicitly\n",
    "                 if fit_ok and hasattr(gr, 'convergence_flag') and gr.convergence_flag != 0:\n",
    "                      skips_conv += 1\n",
    "                      if verbose: log.debug(f\" Skip {model_desc}: GARCH Did Not Converge (Flag: {gr.convergence_flag})\")\n",
    "                      fit_ok = False # Treat non-convergence as failure for selection\n",
    "\n",
    "            if not fit_ok:\n",
    "                skips_fit += 1\n",
    "                if verbose: log.debug(f\" Skip {model_desc}: Fit returned None or GARCH did not converge\")\n",
    "                continue # Skip to next combination if fit failed\n",
    "            fits_ok += 1 # Count successful fits (incl. converged GARCH)\n",
    "\n",
    "        except Exception as e:\n",
    "            skips_fit += 1\n",
    "            if verbose: log.debug(f\" Skip {model_desc}: Fit raised exception ({type(e).__name__})\")\n",
    "            continue\n",
    "\n",
    "        # 3. Check Parameter Significance (only for successfully fitted models)\n",
    "        try:\n",
    "            params_significant = True\n",
    "            insignificant_list = []\n",
    "            # Get p-values safely using getattr\n",
    "            ar_pvals = getattr(ar, 'pvalues', pd.Series(dtype=float))\n",
    "            gr_pvals = getattr(gr, 'pvalues', pd.Series(dtype=float)) if gr is not None else pd.Series(dtype=float)\n",
    "\n",
    "            # Check highest lag AR parameter\n",
    "            if p > 0:\n",
    "                ar_p_name = f\"ar.L{p}\"\n",
    "                # Check if param exists and p-value >= threshold (or is NaN)\n",
    "                if ar_p_name not in ar_pvals or pd.isna(ar_pvals[ar_p_name]) or ar_pvals[ar_p_name] >= param_sig_level:\n",
    "                    params_significant = False\n",
    "                    insignificant_list.append(f\"{ar_p_name}(p={ar_pvals.get(ar_p_name, np.nan):.3f})\")\n",
    "            # Check highest lag MA parameter\n",
    "            if q > 0:\n",
    "                ma_q_name = f\"ma.L{q}\"\n",
    "                if ma_q_name not in ar_pvals or pd.isna(ar_pvals[ma_q_name]) or ar_pvals[ma_q_name] >= param_sig_level:\n",
    "                    params_significant = False\n",
    "                    insignificant_list.append(f\"{ma_q_name}(p={ar_pvals.get(ma_q_name, np.nan):.3f})\")\n",
    "\n",
    "            # Check sigma2 for pure ARIMA\n",
    "            if is_pure_arima and 'sigma2' in ar_pvals and ar_pvals['sigma2'] >= param_sig_level:\n",
    "                 params_significant = False; insignificant_list.append(f\"sigma2(p={ar_pvals['sigma2']:.3f})\")\n",
    "\n",
    "            # Check GARCH parameters if not pure ARIMA\n",
    "            if not is_pure_arima and gr is not None:\n",
    "                current_garch_type_upper = garch_type.upper()\n",
    "                # Check highest lag ARCH (alpha) parameter\n",
    "                if gp > 0:\n",
    "                    alpha_gp_name = f\"alpha[{gp}]\"\n",
    "                    if alpha_gp_name in gr_pvals:\n",
    "                         if pd.isna(gr_pvals[alpha_gp_name]) or gr_pvals[alpha_gp_name] >= param_sig_level:\n",
    "                             params_significant = False; insignificant_list.append(f\"{alpha_gp_name}(p={gr_pvals[alpha_gp_name]:.3f})\")\n",
    "                    # FIGARCH doesn't always have standard alpha/beta names? Need careful checking based on arch lib.\n",
    "                    # Assume standard naming for now, except for FIGARCH.\n",
    "                    elif current_garch_type_upper != 'FIGARCH':\n",
    "                         params_significant = False; insignificant_list.append(f\"{alpha_gp_name}(missing)\")\n",
    "\n",
    "                # Check highest lag GARCH (beta) parameter\n",
    "                if gq > 0:\n",
    "                    beta_gq_name = f\"beta[{gq}]\"\n",
    "                    if beta_gq_name in gr_pvals:\n",
    "                         if pd.isna(gr_pvals[beta_gq_name]) or gr_pvals[beta_gq_name] >= param_sig_level:\n",
    "                             params_significant = False; insignificant_list.append(f\"{beta_gq_name}(p={gr_pvals[beta_gq_name]:.3f})\")\n",
    "                    elif current_garch_type_upper != 'FIGARCH':\n",
    "                         params_significant = False; insignificant_list.append(f\"{beta_gq_name}(missing)\")\n",
    "\n",
    "                # Check asymmetry term (o) if expected (gamma, eta, delta...)\n",
    "                o_asym_tune = 1 if current_garch_type_upper in ['GJR','TARCH','EGARCH','APARCH'] else 0\n",
    "                if o_asym_tune > 0:\n",
    "                     asym_param_found = False; asym_param_sig = False\n",
    "                     for pname in gr_pvals.index:\n",
    "                         # Common asymmetry parameter names\n",
    "                         if pname.startswith(('gamma[', 'eta[', 'delta[')): # Check prefixes\n",
    "                             asym_param_found = True\n",
    "                             if pd.notna(gr_pvals[pname]) and gr_pvals[pname] < param_sig_level:\n",
    "                                 asym_param_sig = True; break # Found one significant asym term\n",
    "                             else:\n",
    "                                 insignificant_list.append(f\"{pname}(p={gr_pvals[pname]:.3f})\")\n",
    "                     # If asymmetry term was expected and found, but none were significant\n",
    "                     if asym_param_found and not asym_param_sig:\n",
    "                         params_significant = False\n",
    "\n",
    "                # Check FIGARCH 'd' parameter specifically\n",
    "                elif current_garch_type_upper == 'FIGARCH':\n",
    "                    d_name = 'd'\n",
    "                    if d_name in gr_pvals:\n",
    "                         if pd.isna(gr_pvals[d_name]) or gr_pvals[d_name] >= param_sig_level:\n",
    "                             params_significant = False; insignificant_list.append(f\"{d_name}(p={gr_pvals[d_name]:.3f})\")\n",
    "                    else: # 'd' parameter should exist for FIGARCH\n",
    "                         params_significant = False; insignificant_list.append(f\"{d_name}(missing)\")\n",
    "\n",
    "            # If any required parameter is insignificant, skip this model\n",
    "            if not params_significant:\n",
    "                skips_sig += 1\n",
    "                if verbose: log.debug(f\" Skip {model_desc}: Insignificant params (p>={param_sig_level}): {', '.join(insignificant_list)}\")\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            # Error during significance check\n",
    "            skips_sig += 1\n",
    "            if verbose: log.debug(f\" Skip {model_desc}: Significance check failed ({type(e).__name__})\")\n",
    "            continue\n",
    "\n",
    "        # 4. Calculate Information Criterion (only for fitted models with significant params)\n",
    "        try:\n",
    "            log_likelihood = np.nan; num_params = 0; num_obs = 0\n",
    "            if is_pure_arima:\n",
    "                num_obs = getattr(ar, 'nobs', 0)\n",
    "                num_params = len(getattr(ar,'params',[]))\n",
    "                log_likelihood = getattr(ar, 'llf', np.nan)\n",
    "            else: # Combined model\n",
    "                num_obs = getattr(gr, 'nobs', 0) # Use GARCH nobs if available\n",
    "                k_arima = len(getattr(ar,'params',[]))\n",
    "                k_garch = len(getattr(gr,'params',[]))\n",
    "                num_params = k_arima + k_garch\n",
    "                log_likelihood = getattr(gr, 'loglikelihood', np.nan) # Use GARCH LLF\n",
    "\n",
    "            # Validate inputs for criterion calculation\n",
    "            if not np.isfinite(log_likelihood) or num_params <= 0 or num_obs <= num_params:\n",
    "                skips_crit += 1\n",
    "                if verbose: log.debug(f\" Skip {model_desc}: Invalid criterion inputs (LLF:{log_likelihood},k:{num_params},n:{num_obs})\")\n",
    "                continue\n",
    "\n",
    "            criterion_value = np.inf\n",
    "            crit = best_model['criterion_used']\n",
    "            if crit == 'BIC': criterion_value = -2 * log_likelihood + num_params * np.log(num_obs)\n",
    "            elif crit == 'AIC': criterion_value = -2 * log_likelihood + 2 * num_params\n",
    "            else: # Should not happen based on config validation, but fallback\n",
    "                  log.warning(f\"Unknown criterion {crit} during tuning, using BIC.\")\n",
    "                  criterion_value = -2 * log_likelihood + num_params * np.log(num_obs)\n",
    "                  crit = 'BIC'\n",
    "\n",
    "            if not np.isfinite(criterion_value):\n",
    "                skips_crit += 1\n",
    "                if verbose: log.debug(f\" Skip {model_desc}: Criterion calculation resulted in NaN/Inf\")\n",
    "                continue\n",
    "\n",
    "            # 5. Update best model if current one is better\n",
    "            if criterion_value < best_model['criterion_value']:\n",
    "                best_model.update({\n",
    "                    'arima': arima_order_current,\n",
    "                    'add_d_recommended': add_d,\n",
    "                    'garch_order': garch_order_current,\n",
    "                    'garch_type': garch_type if not is_pure_arima else 'None',\n",
    "                    'garch_dist': g_dist if not is_pure_arima else 'None',\n",
    "                    'criterion_value': criterion_value,\n",
    "                    'criterion_used': crit,\n",
    "                    'stable_and_significant': True, # Mark that we found a valid model\n",
    "                    'error': None\n",
    "                })\n",
    "                log_msg = f\"  >> New best significant model found: {model_desc} -> {crit}={criterion_value:.2f}\"\n",
    "                if verbose: log.info(log_msg)\n",
    "                else:\n",
    "                     # Log less frequently if not verbose\n",
    "                     if count % max(1, total_combinations // 5) == 0:\n",
    "                         log.info(f\"  New best significant model found ({crit}={criterion_value:.2f})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            skips_crit += 1\n",
    "            if verbose: log.debug(f\" Skip {model_desc}: Criterion calculation error ({type(e).__name__})\")\n",
    "            continue\n",
    "\n",
    "    # --- Tuning finished ---\n",
    "    log.info(f\"Auto-Tuning finished. Evaluated {count}/{total_combinations} combinations. Successful Fits: {fits_ok}\")\n",
    "    log.info(f\"  Skips - Data:{skips_data}, FitErr:{skips_fit}, Convergence:{skips_conv}, Insignificant:{skips_sig}, CriterionErr:{skips_crit}\")\n",
    "\n",
    "    if not best_model['stable_and_significant']:\n",
    "        log.warning(f\"Auto-Tuning failed: No model converged with all parameters significant (p<{param_sig_level}). Check ranges or data.\")\n",
    "        best_model['error'] = \"No model satisfied criteria\"\n",
    "    else:\n",
    "        log.info(f\"--- Best Model Found (Criterion: {best_model['criterion_used']}={best_model['criterion_value']:.2f}, Sig p<{param_sig_level}) ---\")\n",
    "        log.info(f\"  ARIMA (p,q): ({best_model['arima'][0]},{best_model['arima'][2]}), Recommended additional d: {best_model['add_d_recommended']}\")\n",
    "        if best_model['garch_type'] != 'None':\n",
    "             log.info(f\"  GARCH Type: {best_model['garch_type']}, Order (gp,gq): {best_model['garch_order']}, Dist: {best_model['garch_dist']}\")\n",
    "        else:\n",
    "             log.info(\"  Pure ARIMA model selected (no GARCH component).\")\n",
    "\n",
    "    # Clean up internal flag before returning\n",
    "    best_model.pop('stable', None) # Remove deprecated key if exists\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99072dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_price_forecast(last_actual_price: float, log_return_forecasts: np.ndarray, diff_order: int, recent_actual_log_returns: Optional[List[float]] = None) -> List[Optional[float]]:\n",
    "    \"\"\"Converts differenced log return forecasts back to price level forecasts.\"\"\"\n",
    "    horizon = len(log_return_forecasts)\n",
    "    price_forecasts: List[Optional[float]] = [None] * horizon # Initialize with None\n",
    "\n",
    "    if pd.isna(last_actual_price):\n",
    "        log.error(\"Cannot invert price forecast: Last actual price is NaN.\")\n",
    "        return price_forecasts\n",
    "\n",
    "    log_returns_undifferenced = np.full(horizon, np.nan)\n",
    "\n",
    "    # Step 1: Undifference the log return forecasts\n",
    "    if diff_order == 0:\n",
    "        log_returns_undifferenced = log_return_forecasts.copy()\n",
    "    elif diff_order > 0:\n",
    "        if recent_actual_log_returns is None or len(recent_actual_log_returns) < diff_order:\n",
    "            log.error(f\"Cannot undifference d={diff_order}: Need {diff_order} recent actual log returns, only have {len(recent_actual_log_returns) if recent_actual_log_returns else 0}.\")\n",
    "            return price_forecasts # Cannot proceed\n",
    "\n",
    "        # Use the provided recent actual log returns as history\n",
    "        history = list(recent_actual_log_returns[-diff_order:])\n",
    "\n",
    "        for i in range(horizon):\n",
    "            forecast_diff = log_return_forecasts[i]\n",
    "            if pd.isna(forecast_diff):\n",
    "                log.warning(f\"NaN found in differenced log return forecast at step {i+1}. Stopping undifferencing.\")\n",
    "                break # Stop if forecast is NaN\n",
    "\n",
    "            try:\n",
    "                undiff_val = np.nan\n",
    "                if diff_order == 1:\n",
    "                    # X_t = X_{t-1} + diff_forecast_t\n",
    "                    undiff_val = history[-1] + forecast_diff\n",
    "                elif diff_order == 2:\n",
    "                    # diff2_forecast_t = diff1_t - diff1_{t-1}\n",
    "                    # diff1_t = diff1_{t-1} + diff2_forecast_t\n",
    "                    # X_t = X_{t-1} + diff1_t\n",
    "                    if len(history) < 2: # Should not happen if initial check passed, but safety first\n",
    "                        log.error(f\"Undifferencing d=2 failed: Not enough history at step {i+1}\")\n",
    "                        break\n",
    "                    diff_1_lag1 = history[-1] - history[-2] # Reconstruct diff1_{t-1}\n",
    "                    undiff_level_1 = diff_1_lag1 + forecast_diff # Calculate diff1_t\n",
    "                    undiff_val = history[-1] + undiff_level_1 # Calculate X_t\n",
    "                else:\n",
    "                    # Extend this logic for higher orders if needed\n",
    "                    log.error(f\"Undifferencing order d={diff_order} not implemented.\")\n",
    "                    break\n",
    "\n",
    "                # Check for NaN/Inf during calculation\n",
    "                if not pd.notna(undiff_val) or not np.isfinite(undiff_val):\n",
    "                    log.warning(f\"NaN or Inf result during undifferencing calculation at step {i+1}.\")\n",
    "                    break\n",
    "\n",
    "                log_returns_undifferenced[i] = undiff_val\n",
    "                # Update history for next step\n",
    "                history.append(undiff_val)\n",
    "                if len(history) > diff_order:\n",
    "                    history.pop(0) # Keep history length equal to diff_order\n",
    "\n",
    "            except Exception as e:\n",
    "                log.error(f\"Error during undifferencing calculation at step {i+1}: {e}\")\n",
    "                break # Stop on error\n",
    "\n",
    "    # Step 2: Convert undifferenced log returns to prices\n",
    "    last_price = last_actual_price\n",
    "    for i in range(horizon):\n",
    "        log_ret = log_returns_undifferenced[i]\n",
    "        if pd.notna(log_ret) and pd.notna(last_price):\n",
    "            # Clip log return forecast to prevent extreme price explosion from exp()\n",
    "            clipped_log_ret = np.clip(log_ret, -5.0, 5.0) # +/- 5 log return is huge\n",
    "            if clipped_log_ret != log_ret:\n",
    "                 log.debug(f\"Clipped Log-Return forecast {log_ret:.4f} to {clipped_log_ret:.4f} at step {i+1} for price inversion.\")\n",
    "\n",
    "            try:\n",
    "                 # P_t = P_{t-1} * exp(log_return_t)\n",
    "                 current_price_forecast = last_price * np.exp(clipped_log_ret)\n",
    "            except OverflowError:\n",
    "                 log.warning(f\"OverflowError during exp({clipped_log_ret:.2f}) at step {i+1}. Setting price forecast to NaN.\")\n",
    "                 current_price_forecast = np.nan\n",
    "\n",
    "            # Check if forecast is valid (non-NaN, finite, non-negative)\n",
    "            if pd.notna(current_price_forecast) and np.isfinite(current_price_forecast) and current_price_forecast >= 0:\n",
    "                 price_forecasts[i] = current_price_forecast\n",
    "                 last_price = current_price_forecast # Update last price for next step\n",
    "            else:\n",
    "                 log.warning(f\"Invalid price forecast ({current_price_forecast}) calculated at step {i+1}. Stopping price inversion.\")\n",
    "                 break # Stop if price becomes invalid\n",
    "        else:\n",
    "            log.warning(f\"Cannot calculate price at step {i+1} due to missing LogRet ({log_ret}) or previous Price ({last_price}).\")\n",
    "            break # Stop if inputs are missing\n",
    "\n",
    "    return price_forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804540b",
   "metadata": {},
   "source": [
    "### 5.7 Full Analysis Loop (per coin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis_for_coin(coin_id: str, config: Dict) -> Dict:\n",
    "    \"\"\"Runs the complete ARIMA-GARCH analysis workflow for a single coin.\"\"\"\n",
    "    start_time_coin = time.time()\n",
    "    results = {\n",
    "        \"coin_id\": coin_id,\n",
    "        \"status\": \"OK\",\n",
    "        \"error_message\": None,\n",
    "        \"benchmarks\": {\"ewma_var\": {}}, # For EWMA results\n",
    "        \"variance_dm\": {\"backtest\": {}, \"horizon\": {}}, # DM test results for variance\n",
    "        \"multi_horizon_eval\": {\"metrics\":{}, \"price_dm_tests\":{}, \"variance_dm_tests\":{}}, # Horizon eval specific\n",
    "        \"roll_fc\":{} # Backtest specific\n",
    "    }\n",
    "    current_coin_filter.coin_id = coin_id # Set coin ID for logging context\n",
    "    log.info(f\"--- Starting Analysis for {coin_id.upper()} ---\")\n",
    "\n",
    "    # --- Setup Output Directory ---\n",
    "    coin_plot_dir = os.path.join(config['plot_dir_base'], coin_id)\n",
    "    is_plot_available = config.get('plot_available', PLOT_AVAILABLE) # Use config flag if present\n",
    "    ewma_lambda = config['ewma_lambda']\n",
    "    dm_var_loss = config['dm_test_variance_loss_type']\n",
    "    qlike_eps = config['qlike_epsilon']\n",
    "\n",
    "    if is_plot_available and not os.path.exists(coin_plot_dir):\n",
    "        try:\n",
    "            os.makedirs(coin_plot_dir, exist_ok=True)\n",
    "            log.info(f\"Output directory created: {coin_plot_dir}\")\n",
    "        except Exception as e:\n",
    "            log.error(f\"Could not create plot directory {coin_plot_dir}: {e}\")\n",
    "            is_plot_available = False # Disable plotting for this coin if dir fails\n",
    "            log.warning(f\"Plotting disabled for {coin_id} due to directory creation error.\")\n",
    "    results[\"plot_directory\"] = coin_plot_dir if is_plot_available else \"Directory Creation Failed\"\n",
    "\n",
    "    # Config flags for optional outputs\n",
    "    gen_tabs = config.get(\"generate_parameter_tables\", True) and is_plot_available\n",
    "    gen_console = config.get(\"generate_console_parameter_output\", True)\n",
    "    gen_stab = config.get(\"generate_stability_plots\", True) and is_plot_available and config['forecast_mode'] == 'backtest'\n",
    "    # Parameters to track for stability plot\n",
    "    stab_params = ['const','ar.L1','ma.L1','sigma2', # Common ARIMA params\n",
    "                   'mu','omega', 'rho', 'phi', 'd', # Common GARCH core params (incl FIGARCH 'd')\n",
    "                   'alpha[1]','q[1]', # Arch lib specific alpha/beta if needed (q[1] for FIGARCH)\n",
    "                   'beta[1]', # Common GARCH beta\n",
    "                   'gamma[1]','eta[1]','delta[1]', # Common asymmetry params\n",
    "                   'nu','lambda'] # Common distribution params (t-dist, skew-t)\n",
    "\n",
    "    # --- Data Loading and Preprocessing ---\n",
    "    try:\n",
    "        log.info(f\"[{coin_id}] Fetching data ({config['start_date']} to {config['end_date'] or 'Latest'})...\")\n",
    "        raw_data=fetch_data_yahoo(coin_id, config['start_date'], config['end_date'])\n",
    "        processed_data=preprocess_data(raw_data)\n",
    "        if len(processed_data)<config['min_data_length']:\n",
    "            raise ValueError(f\"Data too short after preprocessing ({len(processed_data)} < {config['min_data_length']}).\")\n",
    "        results['total_rows_preprocessed']=len(processed_data)\n",
    "        log.info(f\"[{coin_id}] Data fetched/preprocessed: {len(processed_data)} rows.\")\n",
    "\n",
    "        # Calculate and print descriptive stats for the whole period\n",
    "        desc_stats=compute_descriptive_stats(processed_data)\n",
    "        results['descriptive_stats_full']=desc_stats\n",
    "        print(f\"\\n[{coin_id.upper()}] --- Descriptive Statistics (Full Period) ---\")\n",
    "        for k,v in desc_stats.items():\n",
    "             print(f\"  {k:<20}: {v:>15.4f}\" if isinstance(v,float) else f\"  {k:<20}: {str(v):>15}\")\n",
    "        print(\"-\"*40)\n",
    "\n",
    "    except Exception as e:\n",
    "        log.critical(f\"[{coin_id}] Data Fetching/Preprocessing failed: {type(e).__name__}: {e}\", exc_info=True)\n",
    "        results[\"status\"]=\"ERROR\"\n",
    "        results[\"error_message\"]=f\"Data Fetching/Preprocessing: {type(e).__name__}: {e}\"\n",
    "        current_coin_filter.coin_id=\"N/A\" # Reset logger context\n",
    "        return results\n",
    "\n",
    "    # --- Data Splitting ---\n",
    "    try:\n",
    "        train_df,val_df,test_df = train_val_test_split(processed_data, config['split_ratios'], config['min_test_set_size'])\n",
    "        results['train_size']=len(train_df)\n",
    "        results['val_size']=len(val_df)\n",
    "        results['test_size']=len(test_df)\n",
    "        log.info(f\"[{coin_id}] Data split: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "\n",
    "        # Sanity checks for split sizes relative to mode\n",
    "        if config['forecast_mode']=='backtest' and len(test_df) < config.get('refit_interval', 1):\n",
    "            log.warning(f\"Test set size {len(test_df)} < refit interval {config.get('refit_interval', 1)}. Backtest might end early or behave unexpectedly.\")\n",
    "        valid_horizons_check = [h for h in config.get('evaluation_horizons', []) if isinstance(h, int) and h > 0]\n",
    "        max_h_check = max(valid_horizons_check) if valid_horizons_check else 0\n",
    "        if config['forecast_mode']=='horizon_evaluation' and len(test_df) <= (max_h_check + 10): # 10 is buffer for DM test\n",
    "            log.warning(f\"Test set size {len(test_df)} might be small for max horizon {max_h_check} + DM test buffer (10).\")\n",
    "        if config['forecast_mode']=='future' and (len(train_df) + len(val_df)) == 0:\n",
    "            raise ValueError(\"Train+Validation sets are empty, cannot fit model for future forecast.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log.critical(f\"[{coin_id}] Data Splitting failed: {e}\", exc_info=True)\n",
    "        results[\"status\"]=\"ERROR\"\n",
    "        results[\"error_message\"]=f\"Data Splitting: {e}\"\n",
    "        current_coin_filter.coin_id=\"N/A\"\n",
    "        return results\n",
    "\n",
    "    # --- Prepare Data for Initial Fit/Tuning (Train + Validation) ---\n",
    "    # Concatenate train and validation for model selection/initial fit\n",
    "    fit_base_df = pd.concat([train_df, val_df], ignore_index=True).sort_values('date').reset_index(drop=True)\n",
    "    if fit_base_df.empty:\n",
    "        log.critical(f\"[{coin_id}] Combined Train+Validation data is empty.\")\n",
    "        results[\"status\"]=\"ERROR\"; results[\"error_message\"]=\"Train+Validation data empty.\"\n",
    "        current_coin_filter.coin_id=\"N/A\"; return results\n",
    "\n",
    "    # --- Initial Stationarity Check (on Train+Val data) ---\n",
    "    try:\n",
    "        log.info(f\"[{coin_id}] Performing initial stationarity check (on Train+Val log returns)...\")\n",
    "        adf_init_result={}; kpss_init_result={}\n",
    "        base_logret = fit_base_df['log_return'].dropna()\n",
    "        if not base_logret.empty:\n",
    "            adf_init_result=adf_test(base_logret, config['adf_significance'])\n",
    "            kpss_init_result=kpss_test(base_logret, config['kpss_significance'], 'c') # 'c' for level stationarity\n",
    "        else:\n",
    "            log.warning(\"Train+Val log returns are empty for stationarity check.\")\n",
    "            adf_init_result={'error':'Empty log returns'}; kpss_init_result={'error':'Empty log returns'}\n",
    "\n",
    "        if adf_init_result.get('error'): log.error(f\"ADF Test failed on initial data: {adf_init_result['error']}\")\n",
    "        if kpss_init_result.get('error'): log.error(f\"KPSS Test failed on initial data: {kpss_init_result['error']}\")\n",
    "\n",
    "        results['initial_adf_p_value']=adf_init_result.get('p_value',np.nan)\n",
    "        results['initial_adf_is_stationary']=adf_init_result.get('is_stationary',False)\n",
    "        results['initial_kpss_p_value']=kpss_init_result.get('p_value',np.nan)\n",
    "        results['initial_kpss_is_stationary']=kpss_init_result.get('is_stationary',False)\n",
    "        log.info(f\"Initial Stationarity (Train+Val): ADF Stationary={results['initial_adf_is_stationary']} (p={results['initial_adf_p_value']:.4f}), KPSS Stationary={results['initial_kpss_is_stationary']} (p={results['initial_kpss_p_value']:.4f})\")\n",
    "    except Exception as e:\n",
    "        log.critical(f\"[{coin_id}] Stationarity Check failed: {e}\", exc_info=True)\n",
    "        results[\"status\"]=\"ERROR\"; results[\"error_message\"]=f\"Stationarity Check: {e}\"\n",
    "        current_coin_filter.coin_id=\"N/A\"; return results\n",
    "\n",
    "    # --- Determine Model Structure (Manual or Auto-Tune) ---\n",
    "    p = 0; q = 0; gp = 0; gq = 0; d_final = 0 # Final orders\n",
    "    final_dist = config['garch_distribution_manual_choice']\n",
    "    final_garch_type = config['garch_vol_model_manual']\n",
    "    tuning_criterion = config['tune_criterion'].upper()\n",
    "    param_sig_level = config.get('param_significance_level', 0.10)\n",
    "    final_model_desc = \"N/A\"\n",
    "    recommended_add_d = None # Additional d from tuning\n",
    "    manual_d_input = None # Manual d input from config\n",
    "    initial_arima_fit = None # Store initial fit on train+val\n",
    "    initial_garch_fit = None\n",
    "    results['selection_method'] = 'Manual' # Default\n",
    "    auto_tune_successful = False\n",
    "    differenced_series = None # Initialize variable\n",
    "\n",
    "    try:\n",
    "        # Use Train+Val log returns for model selection\n",
    "        selection_data = fit_base_df['log_return'].copy()\n",
    "        log.info(f\"[{coin_id}] Determining model structure using Train+Val data ({len(selection_data)} points)...\")\n",
    "\n",
    "        if config['use_auto_tune']:\n",
    "            results['selection_method'] = 'Auto-Tune'\n",
    "            log.info(f\"[{coin_id}] Starting Auto-Tuning (Criterion: {tuning_criterion}, Sig Level: {param_sig_level})...\")\n",
    "            garch_types_for_tuning = config.get('garch_types_to_tune', ['GARCH'])\n",
    "            # Call the tuning function\n",
    "            tuning_result = auto_tune_arima_garch(\n",
    "                selection_data,\n",
    "                config['tune_min_p'], config['tune_max_p'], config['tune_min_q'], config['tune_max_q'],\n",
    "                config['tune_min_d'], config['tune_max_d'], # Note: this is *additional* d\n",
    "                config['tune_min_gp'], config['tune_max_gp'], config['tune_min_gq'], config['tune_max_gq'],\n",
    "                garch_types_for_tuning,\n",
    "                sf=config['default_scale_factor'], tune_criterion=tuning_criterion,\n",
    "                param_sig_level=param_sig_level, verbose=config['verbose_tuning']\n",
    "            )\n",
    "\n",
    "            if tuning_result.get('error') or not tuning_result.get('stable_and_significant'):\n",
    "                log.warning(f\"[{coin_id}] Auto-Tune failed or found no significant model: {tuning_result.get('error','No suitable model found')}. Falling back to manual configuration.\")\n",
    "                results['selection_method']='Manual (Auto-Tune Failed/Insignificant)'\n",
    "                p,manual_d_cfg,q = config['manual_arima_order'] # Unpack manual orders\n",
    "                manual_d_input = manual_d_cfg # Set manual d\n",
    "                gp,gq = config['manual_garch_order']\n",
    "                final_garch_type=config['garch_vol_model_manual']\n",
    "                final_dist=config['garch_distribution_manual_choice']\n",
    "                recommended_add_d=None\n",
    "                results['autotune_criterion_value']=np.nan\n",
    "            else:\n",
    "                # Use results from tuning\n",
    "                auto_tune_successful = True\n",
    "                p,_,q = tuning_result['arima'] # Get tuned p, q (d is separate)\n",
    "                gp,gq = tuning_result['garch_order']\n",
    "                final_garch_type = tuning_result['garch_type']\n",
    "                final_dist = tuning_result['garch_dist']\n",
    "                recommended_add_d = tuning_result['add_d_recommended'] # Get recommended additional d\n",
    "                manual_d_input = None # Not using manual d\n",
    "                tuning_criterion = tuning_result.get('criterion_used', tuning_criterion) # Update criterion if changed by tuning\n",
    "                results['autotune_criterion_value'] = tuning_result.get('criterion_value')\n",
    "                log.info(f\"Auto-Tune recommended additional differencing d={recommended_add_d}\")\n",
    "\n",
    "        else: # Manual Mode\n",
    "            p,manual_d_cfg,q = config['manual_arima_order']\n",
    "            manual_d_input = manual_d_cfg # Set manual d\n",
    "            gp,gq = config['manual_garch_order']\n",
    "            final_garch_type=config['garch_vol_model_manual']\n",
    "            final_dist=config['garch_distribution_manual_choice']\n",
    "            recommended_add_d=None\n",
    "            log.info(f\"Using Manual Configuration: ARIMA({p},d={manual_d_input},{q}) + {final_garch_type}({gp},{gq}) Dist={final_dist}\")\n",
    "\n",
    "        # --- Determine and Apply Final Differencing Order ---\n",
    "        # Use the handle_differencing function which considers tests, manual input, and tuning recommendation\n",
    "        series_to_difference = selection_data # Use train+val data for this decision\n",
    "        differenced_series, d_final, diff_reason = handle_differencing(\n",
    "            series_to_difference, adf_init_result, kpss_init_result,\n",
    "            manual_d_input, recommended_add_d,\n",
    "            config['max_differencing_order'], config['adf_significance'], config['kpss_significance']\n",
    "        )\n",
    "        results[\"applied_differencing_order\"]=d_final\n",
    "        results[\"recommended_add_d\"]=recommended_add_d if config['use_auto_tune'] and auto_tune_successful else 'N/A'\n",
    "        results[\"differencing_reason\"]=diff_reason\n",
    "\n",
    "        min_len_after_diff = 30 # Need sufficient data after differencing to fit models\n",
    "        if differenced_series.empty or len(differenced_series)<min_len_after_diff:\n",
    "            raise ValueError(f\"Differenced series (d={d_final}) is unusable ({len(differenced_series)}<{min_len_after_diff}). Check data or differencing order.\")\n",
    "        log.info(f\"Final differencing order set to d={d_final}. Reason: {diff_reason}\")\n",
    "\n",
    "\n",
    "        # --- Compare GARCH Distributions (if not auto-tuned and GARCH model is used) ---\n",
    "        skip_distribution_comparison = (config['use_auto_tune'] and auto_tune_successful) or (final_garch_type == 'None')\n",
    "        if config['compare_garch_dists'] and scipy_stats_available and not skip_distribution_comparison:\n",
    "            crit_dist_comp = tuning_criterion if tuning_criterion in ['AIC','BIC'] else 'AIC' # Use AIC/BIC for comparison\n",
    "            log.info(f\"[{coin_id}] Comparing GARCH Distributions for {final_garch_type}({gp},{gq}) using {crit_dist_comp} (on Train+Val differenced d={d_final} data)...\")\n",
    "            best_criterion_dist=np.inf\n",
    "            best_dist_found=final_dist # Start with the current choice\n",
    "            dist_options_comp=['normal','t']\n",
    "            dist_results_values={}\n",
    "\n",
    "            try:\n",
    "                # Need ARIMA residuals first. Fit ARIMA part once.\n",
    "                temp_arima_order_comp = (p, 0, q)\n",
    "                arima_temp_comp_res, _, _ = fit_arima_garch(differenced_series, temp_arima_order_comp, (1,1), 'GARCH', 'normal', sf=config['default_scale_factor']) # GARCH part is dummy here\n",
    "                if arima_temp_comp_res is None: raise ValueError(\"Temporary ARIMA fit failed for distribution comparison.\")\n",
    "\n",
    "                temp_residuals = arima_temp_comp_res.resid.copy().dropna()\n",
    "                if temp_residuals.empty or not np.all(np.isfinite(temp_residuals)): raise ValueError(\"Invalid residuals obtained for distribution comparison.\")\n",
    "                # Ensure 1D\n",
    "                if hasattr(temp_residuals, 'ndim') and temp_residuals.ndim != 1: temp_residuals = temp_residuals.iloc[:, 0] if isinstance(temp_residuals, pd.DataFrame) else temp_residuals.flatten()\n",
    "                if not isinstance(temp_residuals, (pd.Series, np.ndarray)): temp_residuals = pd.Series(temp_residuals)\n",
    "\n",
    "                # Now fit GARCH with different distributions on these residuals\n",
    "                temp_garch_order_comp = (gp, gq)\n",
    "                temp_garch_type_comp = final_garch_type\n",
    "                o_asym_temp_comp = 1 if temp_garch_type_comp.upper() in ['GJR','TARCH','EGARCH','APARCH'] else 0\n",
    "\n",
    "                # Determine vol model name for arch_model call\n",
    "                if temp_garch_type_comp.upper() == 'FIGARCH': vol_model_temp_comp = 'FIGARCH'; extra_args_comp = {}\n",
    "                elif temp_garch_type_comp.upper() == 'EGARCH': vol_model_temp_comp = 'EGARCH'; extra_args_comp = {}\n",
    "                elif temp_garch_type_comp.upper() in ['GJR', 'TARCH']: vol_model_temp_comp = 'GARCH'; extra_args_comp = {} # Handled by 'o' param\n",
    "                else: vol_model_temp_comp = temp_garch_type_comp.upper(); extra_args_comp = {}\n",
    "\n",
    "                for dist_option in dist_options_comp:\n",
    "                     try:\n",
    "                         gm_comp=arch_model(temp_residuals, p=gp, o=o_asym_temp_comp, q=gq, vol=vol_model_temp_comp, dist=dist_option, mean='Zero', rescale=False, **extra_args_comp)\n",
    "                         gr_comp=gm_comp.fit(disp='off',show_warning=False,options={'maxiter':500})\n",
    "                         if gr_comp.convergence_flag!=0: raise RuntimeError(f\"GARCH dist='{dist_option}' did not converge.\")\n",
    "                         # Get the criterion value\n",
    "                         criterion_val=getattr(gr_comp,crit_dist_comp.lower(),np.inf)\n",
    "                         if not np.isfinite(criterion_val): raise ValueError(f\"NaN criterion value for dist='{dist_option}'.\")\n",
    "\n",
    "                         dist_results_values[dist_option]=criterion_val\n",
    "                         log.info(f\"  Distribution '{dist_option}' -> {crit_dist_comp}: {criterion_val:.2f}\")\n",
    "                         if criterion_val<best_criterion_dist:\n",
    "                             best_criterion_dist=criterion_val\n",
    "                             best_dist_found=dist_option\n",
    "                     except Exception as de:\n",
    "                         log.warning(f\" Fitting/Evaluation failed for dist='{dist_option}': {de}\")\n",
    "                         dist_results_values[dist_option]=np.inf # Assign inf on failure\n",
    "\n",
    "                # Update final distribution if comparison found a better one\n",
    "                if best_criterion_dist != np.inf and final_dist != best_dist_found:\n",
    "                    log.info(f\"Distribution Comparison: Best distribution changed from '{final_dist}' to '{best_dist_found}' ({crit_dist_comp}={best_criterion_dist:.2f}).\")\n",
    "                    final_dist = best_dist_found\n",
    "                elif best_criterion_dist != np.inf:\n",
    "                     log.info(f\"Distribution Comparison: Best distribution remains '{final_dist}' ({crit_dist_comp}={best_criterion_dist:.2f}).\")\n",
    "                else:\n",
    "                     log.warning(f\"Distribution Comparison failed to find a valid best distribution. Keeping '{final_dist}'.\")\n",
    "                results[\"distribution_comparison\"]={\"results\":dist_results_values,\"best_dist\":final_dist,\"criterion_used\":crit_dist_comp,\"skipped\":False}\n",
    "\n",
    "            except Exception as comp_e:\n",
    "                log.error(f\"Distribution comparison process failed: {comp_e}. Keeping original distribution '{final_dist}'.\")\n",
    "                results[\"distribution_comparison\"]={\"error\":str(comp_e),\"best_dist\":final_dist,\"skipped\":False}\n",
    "\n",
    "        elif skip_distribution_comparison:\n",
    "            reason_skip = \"Auto-Tune OK\" if (config['use_auto_tune'] and auto_tune_successful) else \"Pure ARIMA model\"\n",
    "            log.info(f\"Skipping Distribution Comparison: {reason_skip}. Using distribution '{final_dist}'.\")\n",
    "            results[\"distribution_comparison\"]={\"skipped\":True,\"reason\":reason_skip}\n",
    "        elif not scipy_stats_available:\n",
    "             log.warning(\"Skipping Distribution Comparison: SciPy not available for extended distributions.\")\n",
    "             results[\"distribution_comparison\"]={\"skipped\":True,\"reason\":\"Scipy not available\"}\n",
    "        else: # compare_garch_dists is False\n",
    "             log.info(f\"Skipping Distribution Comparison as per config. Using distribution '{final_dist}'.\")\n",
    "             results[\"distribution_comparison\"]={\"skipped\":True,\"reason\":\"Config disabled\"}\n",
    "\n",
    "        # --- Final Model Specification ---\n",
    "        chosen_garch_dist_final = final_dist if final_garch_type != 'None' else 'N/A'\n",
    "        chosen_garch_type_final = final_garch_type if final_garch_type != 'None' else 'N/A'\n",
    "        chosen_gp_final = gp if final_garch_type != 'None' else 0\n",
    "        chosen_gq_final = gq if final_garch_type != 'None' else 0\n",
    "\n",
    "        # Store final orders including the applied differencing order\n",
    "        arima_order_final = (p, d_final, q)\n",
    "        garch_order_fit_final = (chosen_gp_final, chosen_gq_final) # (gp, gq) used for fitting\n",
    "\n",
    "        # Create description string\n",
    "        final_model_desc = f\"ARIMA({p},{d_final},{q})\"\n",
    "        if chosen_garch_type_final != 'N/A':\n",
    "            final_model_desc += f\" - {chosen_garch_type_final}({chosen_gp_final},{chosen_gq_final}) - Dist='{chosen_garch_dist_final}'\"\n",
    "\n",
    "        # Store final model parameters in results\n",
    "        results[\"final_model_description\"]=final_model_desc\n",
    "        results[\"arima_order_p\"]=p; results[\"arima_order_q\"]=q; results[\"applied_differencing_order\"]=d_final\n",
    "        results[\"garch_order_p\"]=chosen_gp_final; results[\"garch_order_q\"]=chosen_gq_final\n",
    "        results['garch_type']=chosen_garch_type_final\n",
    "        results['garch_distribution']=chosen_garch_dist_final\n",
    "        results['tuning_criterion']=tuning_criterion\n",
    "        results['param_significance_level']=param_sig_level\n",
    "        log.info(f\"--- Final Model Structure ({results['selection_method']}): {final_model_desc} ---\")\n",
    "\n",
    "        # --- Initial Fit of Final Model on Train+Val (for reporting/diagnostics) ---\n",
    "        log.info(f\"Performing initial fit of final model ({final_model_desc}) on Train+Val data (using differenced d={d_final})...\")\n",
    "        try:\n",
    "             if not differenced_series.empty:\n",
    "                 if chosen_garch_type_final == 'N/A': # Pure ARIMA\n",
    "                      log.debug(\"Fitting pure ARIMA model for initial report...\")\n",
    "                      am_init=ARIMA(differenced_series,order=(p,0,q),trend='c')\n",
    "                      initial_arima_fit = am_init.fit()\n",
    "                      initial_garch_fit = None\n",
    "                 else: # ARIMA-GARCH\n",
    "                      initial_arima_fit, initial_garch_fit, _ = fit_arima_garch(\n",
    "                           differenced_series,\n",
    "                           (p,0,q), # Use p,0,q for internal ARIMA fit\n",
    "                           garch_order_fit_final,\n",
    "                           chosen_garch_type_final,\n",
    "                           chosen_garch_dist_final,\n",
    "                           sf=config['default_scale_factor']\n",
    "                      )\n",
    "             else:\n",
    "                  log.warning(\"Skipping initial fit: differenced series is empty.\")\n",
    "                  initial_arima_fit=None; initial_garch_fit=None\n",
    "\n",
    "             if initial_arima_fit is None and chosen_garch_type_final != 'N/A':\n",
    "                 log.warning(\"Initial fit of combined ARIMA-GARCH model failed.\")\n",
    "             elif initial_arima_fit is None and chosen_garch_type_final == 'N/A':\n",
    "                 log.warning(\"Initial fit of pure ARIMA model failed.\")\n",
    "             else:\n",
    "                 log.info(\"Initial fit successful.\")\n",
    "\n",
    "                 if initial_arima_fit:\n",
    "                     results['arima_initial_aic'] = getattr(initial_arima_fit, 'aic', np.nan)\n",
    "                     results['arima_initial_bic'] = getattr(initial_arima_fit, 'bic', np.nan)\n",
    "                     results['arima_initial_llf'] = getattr(initial_arima_fit, 'llf', np.nan)\n",
    "                 if initial_garch_fit:\n",
    "                     results['garch_initial_aic'] = getattr(initial_garch_fit, 'aic', np.nan)\n",
    "                     results['garch_initial_bic'] = getattr(initial_garch_fit, 'bic', np.nan)\n",
    "                     results['garch_initial_llf'] = getattr(initial_garch_fit, 'loglikelihood', np.nan)\n",
    "\n",
    "        except Exception as init_fit_e:\n",
    "            log.error(f\"Initial fit of the final model failed: {init_fit_e}\")\n",
    "            initial_arima_fit=None; initial_garch_fit=None\n",
    "\n",
    "    except Exception as model_select_e:\n",
    "        log.critical(f\"Model selection/setup phase failed: {model_select_e}\",exc_info=True)\n",
    "        results[\"status\"]=\"ERROR\"\n",
    "        results[\"error_message\"]=f\"Model Selection/Setup: {model_select_e}\"\n",
    "        current_coin_filter.coin_id=\"N/A\"; return results\n",
    "\n",
    "    # --- Generate Initial Fit Outputs (Tables, Console, Plots) ---\n",
    "    if gen_tabs and (initial_arima_fit is not None or initial_garch_fit is not None):\n",
    "        table_filename=os.path.join(coin_plot_dir, f\"{coin_id}_params_initial_fit_d{d_final}.png\")\n",
    "        table_title=f\"{coin_id.upper()} Initial Parameters (Train+Val Fit)\\n({final_model_desc})\"\n",
    "        try:\n",
    "            table_created=create_parameter_table_png(initial_arima_fit, initial_garch_fit, table_filename, table_title)\n",
    "            results[\"initial_parameter_table_path\"]=table_filename if table_created else \"Failed\"\n",
    "        except Exception as table_e:\n",
    "            log.error(f\"Error generating initial parameter PNG: {table_e}\")\n",
    "            results[\"initial_parameter_table_path\"]=f\"Error: {table_e}\"\n",
    "    elif gen_tabs:\n",
    "        log.warning(\"Skipping initial parameter PNG: Initial fit failed.\")\n",
    "        results[\"initial_parameter_table_path\"]=\"Skipped (Fit Failed)\"\n",
    "\n",
    "    if gen_console:\n",
    "        try:\n",
    "            print_model_summary_console(initial_arima_fit, initial_garch_fit, f\"{coin_id} (Initial Fit on Train+Val)\")\n",
    "        except Exception as e:\n",
    "            log.error(f\"Console output for initial fit failed: {e}\")\n",
    "\n",
    "    # Initial Diagnostic Plots (ACF/PACF on differenced Train+Val, QQ on original Train+Val LogRet)\n",
    "    # --- Initial Diagnostic Plots (ACF/PACF on differenced Train+Val, QQ on original Train+Val LogRet) ---\n",
    "    # ANSTATT die Plots hier zu speichern, speichern wir die Daten fÃ¼r spÃ¤ter.\n",
    "    plotting_data = {\n",
    "        'qq_series': fit_base_df['log_return'].dropna(),\n",
    "        'acf_pacf_series': differenced_series.copy() if differenced_series is not None else pd.Series(dtype=float),\n",
    "        'd_final': d_final,\n",
    "        'coin_id': coin_id\n",
    "    }\n",
    "    results['plotting_data'] = plotting_data # FÃ¼gen Sie dies dem results-Dictionary hinzu\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "    # --- FORECASTING & EVALUATION BLOCK ---\n",
    "    # ==========================================================================\n",
    "    forecast_mode = config['forecast_mode']\n",
    "\n",
    "    # --- Mode 1: Rolling 1-Step Forecast ('backtest') ---\n",
    "    if forecast_mode == 'backtest':\n",
    "        # BACKTEST LOGIC \n",
    "        log.info(f\"--- [{coin_id}] Starting Rolling 1-Step Forecast (Backtest) ---\")\n",
    "        log.info(f\" Fitting Window: {config['fitting_window_size']} days, Refit Interval: {config['refit_interval']} days\")\n",
    "        log.info(f\" EWMA Lambda (Benchmark): {ewma_lambda}\")\n",
    "        log.info(f\" Variance DM Test Loss: {dm_var_loss}\")\n",
    "        alpha = config.get(\"dm_test_alpha\", 0.05) # Use same alpha for VaR/ES/Tests\n",
    "        log.info(f\" VaR/ES/Backtest Alpha: {alpha:.2%}\")\n",
    "        t_roll_start=time.time()\n",
    "        results[\"roll_fc\"]={} # Initialize dict for backtest results\n",
    "\n",
    "        # Use full processed data, indexed by date\n",
    "        full_data_indexed = processed_data.set_index('date').sort_index()\n",
    "        test_start_date = test_df['date'].iloc[0] if not test_df.empty else None\n",
    "        if test_start_date is None:\n",
    "            raise ValueError(\"Test Set is empty, cannot perform backtest.\")\n",
    "\n",
    "        # Find the integer index corresponding to the start of the test set\n",
    "        try:\n",
    "            test_start_loc = full_data_indexed.index.get_loc(test_start_date)\n",
    "            test_start_idx = test_start_loc.start if isinstance(test_start_loc, slice) else test_start_loc\n",
    "        except KeyError:\n",
    "            # Fallback if exact date not found (e.g., due to resampling) - find next available date\n",
    "            try:\n",
    "                test_start_idx = full_data_indexed.index.get_indexer([test_start_date], method='bfill')[0]\n",
    "            except IndexError:\n",
    "                raise ValueError(f\"Test start date {test_start_date} could not be located in the indexed data.\")\n",
    "        if test_start_idx < 0: # get_indexer returns -1 if no date found\n",
    "            raise ValueError(\"Invalid test start index found.\")\n",
    "\n",
    "        n_total=len(full_data_indexed)\n",
    "        n_rolling_steps=n_total-test_start_idx # Number of steps to forecast in the test set\n",
    "\n",
    "        if n_rolling_steps<=0:\n",
    "            raise ValueError(\"No steps available in the test set for rolling forecast.\")\n",
    "\n",
    "        log.info(f\"Rolling forecast over {n_rolling_steps} steps in the test set, starting from {full_data_indexed.index[test_start_idx].date()}\")\n",
    "        log.info(f\" Using model: {final_model_desc}\")\n",
    "\n",
    "        # Lists to store results for each step\n",
    "        forecast_dates_list=[]\n",
    "        actual_prices_list=[]\n",
    "        forecast_prices_ag_list=[] # AG = ARIMA-GARCH\n",
    "        forecast_prices_naive_list=[]\n",
    "        actual_log_returns_list=[]\n",
    "        forecast_log_returns_ag_list=[]\n",
    "        forecast_log_returns_naive_list = []\n",
    "        forecast_volatility_ag_list=[]\n",
    "        forecast_variance_ag_list=[]\n",
    "        forecast_variance_ewma_list=[]\n",
    "        actual_variance_proxies_list=[] # Using squared log return as proxy\n",
    "        # --- Lists for VaR/ES/Violations ---\n",
    "        var_thresh_ag_list = []\n",
    "        es_ag_list = []\n",
    "        violation_ag_list = []\n",
    "        var_thresh_ewma_list = []\n",
    "        es_ewma_list = []\n",
    "        violation_ewma_list = []\n",
    "\n",
    "        # For parameter stability plot\n",
    "        stability_param_data = {par:[] for par in stab_params if gen_stab}\n",
    "        stability_dates_list = []\n",
    "\n",
    "        # Cache last fitted models to avoid refitting every step if interval > 1\n",
    "        last_fit_arima_cached = None\n",
    "        last_fit_garch_cached = None\n",
    "        last_fit_step_idx = -config['refit_interval'] # Ensure initial fit\n",
    "\n",
    "        # State for EWMA calculation\n",
    "        last_ewma_variance_state = np.nan\n",
    "\n",
    "        # Counters\n",
    "        skipped_steps_count = 0\n",
    "        fit_errors_count = 0\n",
    "        # --- Define min length for diagnostics/backtests ---\n",
    "        min_resid_len_diag = 25 # Minimum length for reliable Ljung-Box/ARCH/VaR tests\n",
    "\n",
    "        # --- Rolling Forecast Loop ---\n",
    "        for i in range(n_rolling_steps):\n",
    "            current_step_test_idx = test_start_idx + i # Index in the full dataset\n",
    "            current_date = full_data_indexed.index[current_step_test_idx]\n",
    "            step_number = i + 1\n",
    "\n",
    "            # Log progress periodically\n",
    "            log_freq=max(1,n_rolling_steps//10)\n",
    "            if step_number==1 or step_number%log_freq==0 or step_number==n_rolling_steps:\n",
    "                log.info(f\"Rolling Backtest: Step {step_number}/{n_rolling_steps} (Forecasting for {current_date.date()})...\")\n",
    "\n",
    "            # Determine if model needs refitting\n",
    "            should_refit = (i % config['refit_interval'] == 0)\n",
    "\n",
    "            # Define the window for fitting the model\n",
    "            # Fit window ends *before* the current step we want to forecast\n",
    "            fit_window_end_idx = current_step_test_idx # Data up to t-1\n",
    "            fit_window_start_idx = max(0, fit_window_end_idx - config['fitting_window_size'])\n",
    "            fitting_window_data_df = full_data_indexed.iloc[fit_window_start_idx:fit_window_end_idx]\n",
    "\n",
    "            # Get actual values for the current step (t)\n",
    "            actual_price_t = full_data_indexed.iloc[current_step_test_idx]['price']\n",
    "            actual_log_return_t = full_data_indexed.iloc[current_step_test_idx]['log_return']\n",
    "            actual_variance_proxy_t = actual_log_return_t**2 if pd.notna(actual_log_return_t) else np.nan\n",
    "\n",
    "            # Get naive forecasts (price = last actual price, log return = 0)\n",
    "            last_actual_price_t_minus_1 = full_data_indexed.iloc[current_step_test_idx - 1]['price'] if current_step_test_idx > 0 else np.nan\n",
    "            forecast_price_naive_t = last_actual_price_t_minus_1\n",
    "            forecast_log_return_naive_t = 0.0\n",
    "\n",
    "            # Initialize forecasts for this step\n",
    "            fc_price_ag_t = np.nan\n",
    "            fc_log_return_ag_t = np.nan # This will hold the *undifferenced* log return forecast\n",
    "            fc_volatility_ag_t = np.nan\n",
    "            fc_variance_ag_t = np.nan\n",
    "            fc_variance_ewma_t = np.nan\n",
    "            fc_volatility_ewma_t = np.nan # Initialize EWMA volatility\n",
    "            arima_params_current_step = {}\n",
    "            garch_params_current_step = {}\n",
    "            # --- Initialize VaR/ES/Violation for this step ---\n",
    "            var_thresh_ag_t = np.nan\n",
    "            es_ag_t = np.nan\n",
    "            violation_ag_t = np.nan # Use np.nan to indicate not calculable vs. 0 (no violation)\n",
    "            var_thresh_ewma_t = np.nan\n",
    "            es_ewma_t = np.nan\n",
    "            violation_ewma_t = np.nan\n",
    "            dist_params_model_t = None # Store distribution parameters for this step if refit\n",
    "\n",
    "            append_nan_ag = False # Flag to append NaN if AG model fails\n",
    "            append_nan_ewma = False # Flag to append NaN if EWMA fails\n",
    "\n",
    "            try:\n",
    "                # --- Fit/Load ARIMA-GARCH Model ---\n",
    "                if should_refit:\n",
    "                    log.debug(f\" Step {step_number}: Refitting model...\")\n",
    "                    # Check if fitting window is large enough\n",
    "                    if len(fitting_window_data_df) < config['min_fitting_window_size']:\n",
    "                         log.warning(f\" Step {step_number}: Skipping AG Fit/Forecast. Window size ({len(fitting_window_data_df)}) < Minimum ({config['min_fitting_window_size']}).\")\n",
    "                         skipped_steps_count += 1; append_nan_ag = True\n",
    "                         last_fit_arima_cached = None; last_fit_garch_cached = None # Invalidate cache\n",
    "                    else:\n",
    "                        fit_log_returns_series = fitting_window_data_df['log_return'].copy()\n",
    "                        # Apply differencing to the window data\n",
    "                        fit_series_differenced = difference_series(fit_log_returns_series, order=d_final) if d_final > 0 else fit_log_returns_series.copy()\n",
    "                        fit_series_differenced = fit_series_differenced.dropna() # Drop NaNs from differencing\n",
    "\n",
    "                        # Check length *after* differencing\n",
    "                        min_len_fit_diff_arima = p + q + 5 if p > 0 or q > 0 else 0\n",
    "                        min_len_fit_diff_garch = chosen_gp_final + chosen_gq_final + 5 if chosen_garch_type_final != 'N/A' else 0\n",
    "                        min_len_fit_diff = max(min_len_fit_diff_arima, min_len_fit_diff_garch, 15) # Min length required for fit\n",
    "\n",
    "                        if fit_series_differenced.empty or len(fit_series_differenced) < min_len_fit_diff:\n",
    "                            log.warning(f\" Step {step_number}: Skipping AG Fit/Forecast. Differenced series too short ({len(fit_series_differenced)}<{min_len_fit_diff}) for d={d_final}.\")\n",
    "                            skipped_steps_count += 1; append_nan_ag = True\n",
    "                            last_fit_arima_cached = None; last_fit_garch_cached = None # Invalidate cache\n",
    "                        else:\n",
    "                            # Fit the final chosen model structure\n",
    "                            arima_order_for_fit = (p, 0, q) # d=0 as series is already differenced\n",
    "                            try:\n",
    "                                if chosen_garch_type_final == 'N/A': # Pure ARIMA\n",
    "                                    log.debug(f\"Step {step_number}: Fitting pure ARIMA({p},{d_final},{q})...\")\n",
    "                                    arima_model_step = ARIMA(fit_series_differenced, order=arima_order_for_fit, trend='c')\n",
    "                                    arima_result_step = arima_model_step.fit(method_kwargs={\"xtol\": 1e-6, \"ftol\": 1e-6, \"maxiter\": 1000})\n",
    "                                    garch_result_step = None\n",
    "                                else: # Combined ARIMA-GARCH\n",
    "                                    arima_result_step, garch_result_step, scale_factor_step = fit_arima_garch(\n",
    "                                         fit_series_differenced,\n",
    "                                         arima_order_for_fit,\n",
    "                                         garch_order_fit_final,\n",
    "                                         chosen_garch_type_final,\n",
    "                                         chosen_garch_dist_final,\n",
    "                                         sf=config['default_scale_factor']\n",
    "                                    )\n",
    "\n",
    "                                # Check if fit was successful\n",
    "                                fit_successful_flag = False\n",
    "                                if chosen_garch_type_final == 'N/A' and arima_result_step is not None:\n",
    "                                    fit_successful_flag = True\n",
    "                                elif chosen_garch_type_final != 'N/A' and arima_result_step is not None and garch_result_step is not None:\n",
    "                                     # Check GARCH convergence\n",
    "                                     if hasattr(garch_result_step,'convergence_flag') and garch_result_step.convergence_flag != 0:\n",
    "                                         log.warning(f\" Step {step_number}: GARCH did not converge during refit (Flag: {garch_result_step.convergence_flag}). Treating as fit failure.\")\n",
    "                                         # Fit failed even if objects exist\n",
    "                                     else:\n",
    "                                         fit_successful_flag = True\n",
    "\n",
    "                                if fit_successful_flag:\n",
    "                                     log.debug(f\" Step {step_number}: Refit successful.\")\n",
    "                                     last_fit_arima_cached = arima_result_step\n",
    "                                     last_fit_garch_cached = garch_result_step\n",
    "                                     last_fit_step_idx = i # Update index of last successful fit\n",
    "                                     # --- Extract distribution parameters if GARCH fit ---\n",
    "                                     if garch_result_step is not None:\n",
    "                                          params_garch_t = getattr(garch_result_step, 'params', {})\n",
    "                                          if chosen_garch_dist_final == 't' and 'nu' in params_garch_t:\n",
    "                                               dist_params_model_t = {'nu': params_garch_t['nu']}\n",
    "                                          elif chosen_garch_dist_final == 'skewt' and 'nu' in params_garch_t and 'lambda' in params_garch_t:\n",
    "                                               dist_params_model_t = {'nu': params_garch_t['nu'], 'lambda': params_garch_t['lambda']}\n",
    "                                          else: dist_params_model_t = None\n",
    "                                     else: dist_params_model_t = None\n",
    "\n",
    "                                else:\n",
    "                                     log.warning(f\" Step {step_number}: Model refit failed.\")\n",
    "                                     fit_errors_count += 1; append_nan_ag = True\n",
    "                                     last_fit_arima_cached = None; last_fit_garch_cached = None # Invalidate cache\n",
    "                                     dist_params_model_t = None # No params if fit failed\n",
    "\n",
    "                            except Exception as fit_exception:\n",
    "                                 log.error(f\" Step {step_number}: Error during model refitting: {fit_exception}\")\n",
    "                                 fit_errors_count += 1; append_nan_ag = True\n",
    "                                 last_fit_arima_cached = None; last_fit_garch_cached = None # Invalidate cache\n",
    "                                 dist_params_model_t = None\n",
    "                else:\n",
    "                     # Use cached model if not refitting\n",
    "                     log.debug(f\" Step {step_number}: Using cached model from step {last_fit_step_idx+1}\")\n",
    "                     arima_result_step = last_fit_arima_cached\n",
    "                     garch_result_step = last_fit_garch_cached\n",
    "                     scale_factor_step = config['default_scale_factor'] # Assume default scale if using cache\n",
    "                     # --- Get dist params from cached GARCH fit ---\n",
    "                     if last_fit_garch_cached is not None:\n",
    "                          params_garch_t = getattr(last_fit_garch_cached, 'params', {})\n",
    "                          if chosen_garch_dist_final == 't' and 'nu' in params_garch_t:\n",
    "                               dist_params_model_t = {'nu': params_garch_t['nu']}\n",
    "                          elif chosen_garch_dist_final == 'skewt' and 'nu' in params_garch_t and 'lambda' in params_garch_t:\n",
    "                               dist_params_model_t = {'nu': params_garch_t['nu'], 'lambda': params_garch_t['lambda']}\n",
    "                          else: dist_params_model_t = None\n",
    "                     else: dist_params_model_t = None\n",
    "\n",
    "\n",
    "                # --- Generate 1-step Forecast using fitted/cached model ---\n",
    "                if not append_nan_ag and arima_result_step is not None:\n",
    "                     # Get recent actual log returns needed for undifferencing\n",
    "                     recent_log_returns_history = None\n",
    "                     if d_final > 0:\n",
    "                          history_end_idx = current_step_test_idx # Use data up to t-1\n",
    "                          history_start_idx = max(0, history_end_idx - d_final)\n",
    "                          if history_end_idx - history_start_idx == d_final:\n",
    "                              recent_log_returns_history = full_data_indexed['log_return'].iloc[history_start_idx : history_end_idx].tolist()\n",
    "                          else:\n",
    "                              log.warning(f\"Step {step_number}: Not enough history ({current_step_test_idx} < {d_final}) for undifferencing d={d_final}. Log return forecast will be NaN.\")\n",
    "\n",
    "                     # Generate 1-step forecast (returns mean and variance)\n",
    "                     fc_mean_series_ag_t, fc_variance_series_ag_t = forecast_arima_garch(\n",
    "                         arima_result_step, garch_result_step, steps=1, sf=scale_factor_step\n",
    "                     )\n",
    "                     fc_log_return_diff_1 = fc_mean_series_ag_t.iloc[0] # Forecast of differenced log return\n",
    "                     fc_variance_1 = fc_variance_series_ag_t.iloc[0]\n",
    "                     fc_volatility_1 = np.sqrt(max(0, fc_variance_1)) if pd.notna(fc_variance_1) else np.nan\n",
    "\n",
    "                     # Validate variance forecast\n",
    "                     if not np.isfinite(fc_variance_1):\n",
    "                          log.error(f\"Step {step_number}: Non-finite AG variance forecast ({fc_variance_1}) received!\")\n",
    "                          append_nan_ag = True # Treat as failure\n",
    "                     elif fc_variance_1 < 0:\n",
    "                          log.warning(f\"Step {step_number}: Negative AG variance forecast ({fc_variance_1:.6f}), clamping to 0.\")\n",
    "                          fc_variance_1 = 0.0\n",
    "                          fc_volatility_1 = 0.0\n",
    "                     elif fc_variance_1 > 0.5: # Arbitrary threshold for very high daily variance\n",
    "                          log.warning(f\"Step {step_number}: Very high AG variance forecast ({fc_variance_1:.6f}).\")\n",
    "\n",
    "                     if not append_nan_ag:\n",
    "                         # Undifference log return forecast to get LEVEL log return forecast\n",
    "                         if pd.notna(fc_log_return_diff_1):\n",
    "                             if d_final == 0:\n",
    "                                 fc_log_return_ag_t = fc_log_return_diff_1 # Already level forecast\n",
    "                             elif d_final == 1 and recent_log_returns_history is not None:\n",
    "                                 fc_log_return_ag_t = recent_log_returns_history[-1] + fc_log_return_diff_1\n",
    "                             elif d_final == 2 and recent_log_returns_history is not None and len(recent_log_returns_history)==2:\n",
    "                                 diff_1_lag1 = recent_log_returns_history[-1] - recent_log_returns_history[-2]\n",
    "                                 undiff_level_1 = diff_1_lag1 + fc_log_return_diff_1\n",
    "                                 fc_log_return_ag_t = recent_log_returns_history[-1] + undiff_level_1\n",
    "                             else:\n",
    "                                 # Handles missing history or unimplemented d order\n",
    "                                 fc_log_return_ag_t = np.nan # Set undifferenced forecast to NaN\n",
    "                                 log.warning(f\"Step {step_number}: Could not undifference AG log return forecast for d={d_final}.\")\n",
    "                         else:\n",
    "                              fc_log_return_ag_t = np.nan # If differenced forecast is NaN, level is NaN\n",
    "\n",
    "                         # Convert log return forecast to price forecast\n",
    "                         if pd.notna(fc_log_return_ag_t) and pd.notna(last_actual_price_t_minus_1):\n",
    "                              try:\n",
    "                                   fc_price_ag_t = last_actual_price_t_minus_1 * np.exp(np.clip(fc_log_return_ag_t, -5, 5)) # Clip logret before exp\n",
    "                              except OverflowError:\n",
    "                                   log.warning(f\"OverflowError during exp() for AG price forecast step {step_number}.\")\n",
    "                                   fc_price_ag_t = np.nan\n",
    "                         else:\n",
    "                              fc_price_ag_t = np.nan\n",
    "\n",
    "                         # Assign variance/volatility\n",
    "                         fc_variance_ag_t = fc_variance_1\n",
    "                         fc_volatility_ag_t = fc_volatility_1 # Already calculated from potentially clamped variance\n",
    "\n",
    "                         # Store parameters if refit and stability plot enabled\n",
    "                         if gen_stab and should_refit:\n",
    "                             if arima_result_step is not None: arima_params_current_step = getattr(arima_result_step,'params',pd.Series(dtype=float)).to_dict()\n",
    "                             if garch_result_step is not None: garch_params_current_step = getattr(garch_result_step,'params',pd.Series(dtype=float)).to_dict()\n",
    "\n",
    "                else:\n",
    "                     # Case where AG model fit failed or was skipped\n",
    "                     if not append_nan_ag: # Only log if not already flagged\n",
    "                        log.warning(f\" Step {step_number}: No valid AG model available for forecast. Appending NaNs.\")\n",
    "                        append_nan_ag = True\n",
    "\n",
    "                # --- Calculate EWMA Variance Forecast ---\n",
    "                try:\n",
    "                    if i == 0: # Initialize EWMA state on the first step\n",
    "                        # Use variance of log returns in the initial fitting window\n",
    "                        initial_fit_log_rets = fitting_window_data_df['log_return'].dropna()\n",
    "                        if len(initial_fit_log_rets) > 1:\n",
    "                            initial_variance = initial_fit_log_rets.var()\n",
    "                            # Ensure initial variance is non-negative and not NaN\n",
    "                            last_ewma_variance_state = max(0.0, initial_variance) if pd.notna(initial_variance) else np.nan\n",
    "                            log.debug(f\"Step {step_number}: Initialized EWMA variance state with {last_ewma_variance_state:.8f} (from initial window N={len(initial_fit_log_rets)})\")\n",
    "                        else:\n",
    "                            log.warning(f\"Step {step_number}: Not enough data in initial window ({len(initial_fit_log_rets)}) to initialize EWMA variance.\")\n",
    "                            last_ewma_variance_state = np.nan\n",
    "                        # EWMA forecast for t+1 is the state at time t\n",
    "                        fc_variance_ewma_t = last_ewma_variance_state\n",
    "                    else:\n",
    "                        # Update EWMA state using previous step's actual log return\n",
    "                        previous_actual_log_return = full_data_indexed.iloc[current_step_test_idx - 1]['log_return']\n",
    "                        previous_actual_var_proxy = previous_actual_log_return**2 if pd.notna(previous_actual_log_return) else np.nan\n",
    "\n",
    "                        if pd.notna(last_ewma_variance_state) and pd.notna(previous_actual_var_proxy):\n",
    "                            # EWMA update: var_t = lambda * var_{t-1} + (1-lambda) * actual_proxy_{t-1}\n",
    "                            # Forecast for t+1 is the state calculated using info up to t\n",
    "                            current_ewma_variance_state = ewma_lambda * last_ewma_variance_state + (1 - ewma_lambda) * previous_actual_var_proxy\n",
    "                            fc_variance_ewma_t = current_ewma_variance_state # Forecast for t+1 is state at t\n",
    "                            last_ewma_variance_state = current_ewma_variance_state # Update state for next iteration\n",
    "                        else:\n",
    "                            fc_variance_ewma_t = np.nan\n",
    "                            last_ewma_variance_state = np.nan # Reset state if calculation failed\n",
    "                            log.warning(f\"Step {step_number}: EWMA calculation failed due to NaN (last state: {last_ewma_variance_state}, prev proxy: {previous_actual_var_proxy}).\")\n",
    "\n",
    "                    # Ensure EWMA forecast is non-negative\n",
    "                    if pd.notna(fc_variance_ewma_t) and fc_variance_ewma_t < 0:\n",
    "                         log.warning(f\"Step {step_number}: Negative EWMA variance forecast ({fc_variance_ewma_t:.6f}), clamping to 0.\")\n",
    "                         fc_variance_ewma_t = 0.0\n",
    "                         last_ewma_variance_state = 0.0 # Also clamp the state\n",
    "                    elif pd.notna(fc_variance_ewma_t):\n",
    "                         # Calculate EWMA volatility forecast\n",
    "                         fc_volatility_ewma_t = np.sqrt(fc_variance_ewma_t)\n",
    "                    else:\n",
    "                         fc_volatility_ewma_t = np.nan\n",
    "\n",
    "\n",
    "                except Exception as ewma_exception:\n",
    "                    log.error(f\" Step {step_number}: Error during EWMA calculation: {ewma_exception}\")\n",
    "                    fc_variance_ewma_t = np.nan\n",
    "                    fc_volatility_ewma_t = np.nan\n",
    "                    last_ewma_variance_state = np.nan\n",
    "                    append_nan_ewma = True\n",
    "\n",
    "                if pd.isna(fc_variance_ewma_t): append_nan_ewma = True\n",
    "\n",
    "                # --- Calculate VaR/ES and Violations for step t ---\n",
    "                if not append_nan_ag:\n",
    "                     # Use the UNDIFFERENCED log return forecast (fc_log_return_ag_t)\n",
    "                     # and the calculated volatility (fc_volatility_ag_t)\n",
    "                     var_thresh_ag_t, es_ag_t = calculate_parametric_var_es(\n",
    "                         fc_log_return_ag_t, fc_volatility_ag_t, chosen_garch_dist_final, alpha, dist_params_model_t\n",
    "                     )\n",
    "                     # Determine violation if actual return and VaR threshold are valid\n",
    "                     if pd.notna(actual_log_return_t) and pd.notna(var_thresh_ag_t):\n",
    "                         violation_ag_t = 1 if actual_log_return_t < var_thresh_ag_t else 0\n",
    "\n",
    "                if not append_nan_ewma:\n",
    "                     # Use 0 mean for EWMA benchmark VaR\n",
    "                     var_thresh_ewma_t, es_ewma_t = calculate_parametric_var_es(\n",
    "                         0.0, fc_volatility_ewma_t, 'normal', alpha # Assume normal dist for EWMA benchmark\n",
    "                     )\n",
    "                     if pd.notna(actual_log_return_t) and pd.notna(var_thresh_ewma_t):\n",
    "                         violation_ewma_t = 1 if actual_log_return_t < var_thresh_ewma_t else 0\n",
    "\n",
    "\n",
    "            except Exception as step_exception:\n",
    "                 log.error(f\" Step {step_number}: Unexpected error in rolling step: {step_exception}\", exc_info=True)\n",
    "                 append_nan_ag = True; append_nan_ewma = True; skipped_steps_count += 1 # Count as skipped\n",
    "\n",
    "            finally:\n",
    "                # Append results for this step\n",
    "                forecast_dates_list.append(current_date)\n",
    "                actual_prices_list.append(actual_price_t)\n",
    "                actual_log_returns_list.append(actual_log_return_t)\n",
    "                actual_variance_proxies_list.append(actual_variance_proxy_t)\n",
    "                # Append naive forecasts\n",
    "                forecast_prices_naive_list.append(forecast_price_naive_t)\n",
    "                forecast_log_returns_naive_list.append(forecast_log_return_naive_t)\n",
    "                # Append AG forecasts (or NaN if failed)\n",
    "                forecast_prices_ag_list.append(fc_price_ag_t if not append_nan_ag else np.nan)\n",
    "                forecast_log_returns_ag_list.append(fc_log_return_ag_t if not append_nan_ag else np.nan) # Append undifferenced forecast\n",
    "                forecast_volatility_ag_list.append(fc_volatility_ag_t if not append_nan_ag else np.nan)\n",
    "                forecast_variance_ag_list.append(fc_variance_ag_t if not append_nan_ag else np.nan)\n",
    "                # Append EWMA forecast (or NaN if failed)\n",
    "                forecast_variance_ewma_list.append(fc_variance_ewma_t if not append_nan_ewma else np.nan)\n",
    "                # Append VaR/ES/Violation results ---\n",
    "                var_thresh_ag_list.append(var_thresh_ag_t)\n",
    "                es_ag_list.append(es_ag_t)\n",
    "                violation_ag_list.append(violation_ag_t)\n",
    "                var_thresh_ewma_list.append(var_thresh_ewma_t)\n",
    "                es_ewma_list.append(es_ewma_t)\n",
    "                violation_ewma_list.append(violation_ewma_t)\n",
    "\n",
    "\n",
    "                # Append parameters for stability plot if refit occurred successfully\n",
    "                if gen_stab and should_refit and not append_nan_ag and (last_fit_arima_cached is not None or last_fit_garch_cached is not None):\n",
    "                     stability_dates_list.append(current_date)\n",
    "                     # Combine parameters from both models\n",
    "                     combined_params_step = {}\n",
    "                     if last_fit_arima_cached is not None: combined_params_step.update(getattr(last_fit_arima_cached,'params',pd.Series(dtype=float)).to_dict())\n",
    "                     if last_fit_garch_cached is not None: combined_params_step.update(getattr(last_fit_garch_cached,'params',pd.Series(dtype=float)).to_dict())\n",
    "                     # Append each tracked parameter, using NaN if not present\n",
    "                     for p_name in stab_params:\n",
    "                         stability_param_data[p_name].append(combined_params_step.get(p_name, np.nan))\n",
    "                elif gen_stab and should_refit: # Handle case where refit was attempted but failed\n",
    "                     stability_dates_list.append(current_date)\n",
    "                     for p_name in stab_params:\n",
    "                          stability_param_data[p_name].append(np.nan)\n",
    "\n",
    "        # --- End of Rolling Forecast Loop ---\n",
    "\n",
    "        results['rolling_forecast_duration_min']=(time.time()-t_roll_start)/60\n",
    "        log.info(f\"--- Rolling 1-Step Forecast Finished: {results['rolling_forecast_duration_min']:.2f} min ---\")\n",
    "        if fit_errors_count>0: log.warning(f\"{fit_errors_count} ARIMA/GARCH fit errors encountered during rolling forecast.\")\n",
    "        if skipped_steps_count>0: log.warning(f\"{skipped_steps_count}/{n_rolling_steps} steps skipped/failed during rolling forecast.\")\n",
    "        results['rolling_fit_errors']=fit_errors_count\n",
    "        results['rolling_skipped_steps']=skipped_steps_count\n",
    "\n",
    "        # --- Evaluate Rolling Forecast Results ---\n",
    "        evaluation_df_dict = {\n",
    "            'date':forecast_dates_list,\n",
    "            'actual_price':actual_prices_list,\n",
    "            'forecast_price_ag':forecast_prices_ag_list,\n",
    "            'forecast_price_naive':forecast_prices_naive_list,\n",
    "            'actual_log_return':actual_log_returns_list,\n",
    "            'forecast_log_return_ag':forecast_log_returns_ag_list,\n",
    "            'forecast_log_return_naive': forecast_log_returns_naive_list,\n",
    "            'forecast_volatility_ag':forecast_volatility_ag_list,\n",
    "            'forecast_variance_ag':forecast_variance_ag_list,\n",
    "            'forecast_variance_ewma':forecast_variance_ewma_list,\n",
    "            'actual_variance_proxy':actual_variance_proxies_list,\n",
    "            # --- Add VaR/ES/Violation columns ---\n",
    "            'var_thresh_ag': var_thresh_ag_list,\n",
    "            'es_ag': es_ag_list,\n",
    "            'violation_ag': violation_ag_list,\n",
    "            'var_thresh_ewma': var_thresh_ewma_list,\n",
    "            'es_ewma': es_ewma_list,\n",
    "            'violation_ewma': violation_ewma_list\n",
    "\n",
    "        }\n",
    "        # Check if all lists have the same length before creating DataFrame\n",
    "        list_lengths = {k: len(v) for k, v in evaluation_df_dict.items()}\n",
    "        if len(set(list_lengths.values())) > 1:\n",
    "            log.error(f\"List length mismatch for evaluation DataFrame: {list_lengths}\")\n",
    "            # Handle error: maybe return partial results or raise exception\n",
    "            raise ValueError(\"Inconsistent list lengths for evaluation DataFrame creation.\")\n",
    "\n",
    "        evaluation_df = pd.DataFrame(evaluation_df_dict).set_index('date')\n",
    "\n",
    "\n",
    "        # --- Evaluate Price Forecasts ---\n",
    "        required_cols_price=['actual_price','forecast_price_ag','forecast_price_naive']\n",
    "        eval_df_price=evaluation_df.dropna(subset=required_cols_price)\n",
    "        valid_price_steps=len(eval_df_price)\n",
    "        results['rolling_valid_steps_price']=valid_price_steps\n",
    "        print(f\"\\n--- Rolling 1-Step Performance (Test Set, d={d_final}) ---\")\n",
    "        log.info(f\"Evaluating 1-step price forecasts over {valid_price_steps} valid steps.\")\n",
    "        if valid_price_steps > 0:\n",
    "\n",
    "            yt_p=eval_df_price['actual_price'].values\n",
    "            yp_p_ag=eval_df_price['forecast_price_ag'].values\n",
    "            yp_p_n=eval_df_price['forecast_price_naive'].values\n",
    "            # Calculate metrics\n",
    "            results['price_mae_ag']=mean_absolute_error(yt_p,yp_p_ag)\n",
    "            results['price_rmse_ag']=root_mean_squared_error(yt_p,yp_p_ag)\n",
    "            results['price_mape_ag']=mean_absolute_percentage_error(yt_p,yp_p_ag)\n",
    "            results['price_mae_naive']=mean_absolute_error(yt_p,yp_p_n)\n",
    "            results['price_rmse_naive']=root_mean_squared_error(yt_p,yp_p_n)\n",
    "            results['price_mape_naive']=mean_absolute_percentage_error(yt_p,yp_p_n)\n",
    "            # Print metrics\n",
    "            print(\"  1-Step Price Accuracy:\")\n",
    "            print(f\"    AG    MAE:{results['price_mae_ag']:,.4f} RMSE:{results['price_rmse_ag']:,.4f} MAPE:{results['price_mape_ag']:.2f}%\")\n",
    "            print(f\"    Naive MAE:{results['price_mae_naive']:,.4f} RMSE:{results['price_rmse_naive']:,.4f} MAPE:{results['price_mape_naive']:.2f}%\")\n",
    "\n",
    "            # Perform Diebold-Mariano Test for Price Forecasts (AG vs Naive)\n",
    "            print(f\"\\n--- Diebold-Mariano Test for Prices (Backtest, h=1, Compare AG vs. Naive) ---\")\n",
    "            dm_results_price_bt = {}\n",
    "            dm_loss_type_price = config.get(\"dm_test_loss_type\", \"Squared Error\") # Define loss type here\n",
    "            if not DIEBOLDMARIANO_LIB_AVAILABLE:\n",
    "                 dm_results_price_bt = {'error': \"Library not found\"}\n",
    "                 print(\"  DM Test skipped: 'dieboldmariano' library not installed.\")\n",
    "            else:\n",
    "                 actuals_dm = eval_df_price['actual_price']\n",
    "                 forecasts_ag_dm = eval_df_price['forecast_price_ag']\n",
    "                 forecasts_naive_dm = eval_df_price['forecast_price_naive']\n",
    "                 # H1: AG model has lower loss than Naive model (alternative='less')\n",
    "                 dm_test_output_p = diebold_mariano_test(actuals_dm, forecasts_ag_dm, forecasts_naive_dm, h=1, loss_type=dm_loss_type_price, alternative='less')\n",
    "                 dm_results_price_bt = dm_test_output_p.copy()\n",
    "                 dm_results_price_bt['is_significant'] = False # Initialize\n",
    "                 if dm_test_output_p['error'] is not None:\n",
    "                     dm_results_price_bt['interpretation'] = f\"Error: {dm_test_output_p['error']}\"\n",
    "                 elif pd.notna(dm_test_output_p['p_value']):\n",
    "                     p_val, dm_stat = dm_test_output_p['p_value'], dm_test_output_p['dm_stat']\n",
    "                     if p_val < alpha: # Use defined alpha\n",
    "                         if dm_stat < 0:\n",
    "                             dm_results_price_bt['interpretation'] = f\"AG significantly better (p={p_val:.4f} < {alpha})\"\n",
    "                             dm_results_price_bt['is_significant'] = True\n",
    "                         else:\n",
    "                             dm_results_price_bt['interpretation'] = f\"Significant (p={p_val:.4f}), but DM Stat >= 0 ({dm_stat:.2f}) - Contradiction?\"\n",
    "                     else:\n",
    "                         dm_results_price_bt['interpretation'] = f\"No significant difference (p={p_val:.4f} >= {alpha})\"\n",
    "                 else:\n",
    "                     dm_results_price_bt['interpretation'] = \"Test result invalid (NaN p-value)\"\n",
    "                 print(f\"  Loss='{dm_loss_type_price}' | N={dm_results_price_bt['n_obs']:<5} | DM Stat={dm_results_price_bt['dm_stat']:>8.3f} | p-value={dm_results_price_bt['p_value']:>8.4f} | {dm_results_price_bt['interpretation']}\")\n",
    "            results['roll_fc_price_dm_test'] = dm_results_price_bt\n",
    "        else:\n",
    "             log.warning(\"No valid steps for 1-step price evaluation (including DM test).\")\n",
    "             results.update({k:np.nan for k in ['price_mae_ag','price_rmse_ag','price_mape_ag','price_mae_naive','price_rmse_naive','price_mape_naive']})\n",
    "             results['roll_fc_price_dm_test'] = {'error': 'No valid steps for price evaluation'}\n",
    "\n",
    "        # --- Evaluate Log Return Forecasts --- \n",
    "        required_cols_lr = ['actual_log_return', 'forecast_log_return_ag', 'forecast_log_return_naive']\n",
    "        eval_df_lr = evaluation_df.dropna(subset=required_cols_lr)\n",
    "        valid_lr_steps = len(eval_df_lr)\n",
    "        results['rolling_valid_steps_lr'] = valid_lr_steps\n",
    "        print(\"\\n  1-Step Log Return Accuracy:\")\n",
    "        if valid_lr_steps > 0:\n",
    "\n",
    "            yt_lr = eval_df_lr['actual_log_return'].values\n",
    "            yp_lr_ag = eval_df_lr['forecast_log_return_ag'].values\n",
    "            yp_lr_n = eval_df_lr['forecast_log_return_naive'].values # Should be all zeros\n",
    "            results['logret_mae_ag'] = mean_absolute_error(yt_lr, yp_lr_ag)\n",
    "            results['logret_rmse_ag'] = root_mean_squared_error(yt_lr, yp_lr_ag)\n",
    "            results['logret_mae_naive'] = mean_absolute_error(yt_lr, yp_lr_n)\n",
    "            results['logret_rmse_naive'] = root_mean_squared_error(yt_lr, yp_lr_n)\n",
    "            print(f\"    AG    MAE:{results['logret_mae_ag']:.6f} RMSE:{results['logret_rmse_ag']:.6f}\")\n",
    "            print(f\"    Naive MAE:{results['logret_mae_naive']:.6f} RMSE:{results['logret_rmse_naive']:.6f} (vs. 0 Forecast)\")\n",
    "\n",
    "            # Perform Diebold-Mariano Test for Log Return Forecasts (AG vs Naive=0)\n",
    "            print(f\"\\n--- Diebold-Mariano Test for Log Returns (Backtest, h=1, Compare AG vs. Naive=0) ---\")\n",
    "            dm_results_lr_bt = {}\n",
    "            if not DIEBOLDMARIANO_LIB_AVAILABLE:\n",
    "                 dm_results_lr_bt = {'error': \"Library not found\"}\n",
    "                 print(\"  DM Test for Log Returns skipped: 'dieboldmariano' library not installed.\")\n",
    "            else:\n",
    "                 actuals_lr_dm = eval_df_lr['actual_log_return']\n",
    "                 forecasts_ag_lr_dm = eval_df_lr['forecast_log_return_ag']\n",
    "                 forecasts_naive_lr_dm = eval_df_lr['forecast_log_return_naive'] # Series of zeros\n",
    "                 # H1: AG model has lower loss than Naive=0 model (alternative='less')\n",
    "                 dm_test_lr_output = diebold_mariano_test(actuals_lr_dm, forecasts_ag_lr_dm, forecasts_naive_lr_dm, h=1, loss_type=dm_loss_type_price, alternative='less')\n",
    "                 dm_results_lr_bt = dm_test_lr_output.copy()\n",
    "                 dm_results_lr_bt['is_significant'] = False # Initialize\n",
    "                 if dm_test_lr_output['error'] is not None:\n",
    "                     dm_results_lr_bt['interpretation'] = f\"Error: {dm_test_lr_output['error']}\"\n",
    "                 elif pd.notna(dm_test_lr_output['p_value']):\n",
    "                     p_val, dm_stat = dm_test_lr_output['p_value'], dm_test_lr_output['dm_stat']\n",
    "                     if p_val < alpha: # Use defined alpha\n",
    "                         if dm_stat < 0:\n",
    "                             dm_results_lr_bt['interpretation'] = f\"AG significantly better (p={p_val:.4f} < {alpha})\"\n",
    "                             dm_results_lr_bt['is_significant'] = True\n",
    "                         else:\n",
    "                              dm_results_lr_bt['interpretation'] = f\"Significant (p={p_val:.4f}), but DM Stat >= 0 ({dm_stat:.2f}) - Contradiction?\"\n",
    "                     else:\n",
    "                         dm_results_lr_bt['interpretation'] = f\"No significant difference (p={p_val:.4f} >= {alpha})\"\n",
    "                 else:\n",
    "                     dm_results_lr_bt['interpretation'] = \"Test result invalid (NaN p-value)\"\n",
    "                 print(f\"  Loss='{dm_loss_type_price}' | N={dm_results_lr_bt['n_obs']:<5} | DM Stat={dm_results_lr_bt['dm_stat']:>8.3f} | p-value={dm_results_lr_bt['p_value']:>8.4f} | {dm_results_lr_bt['interpretation']}\")\n",
    "            results['roll_fc_logret_dm_test'] = dm_results_lr_bt\n",
    "        else:\n",
    "            log.warning(\"No valid steps for 1-step log return evaluation (including DM test).\")\n",
    "            results.update({k: np.nan for k in ['logret_mae_ag', 'logret_rmse_ag', 'logret_mae_naive', 'logret_rmse_naive']})\n",
    "            results['roll_fc_logret_dm_test'] = {'error': 'No valid steps for LogRet evaluation'}\n",
    "\n",
    "        # --- Evaluate Variance Forecasts --- \n",
    "        required_cols_var = ['actual_variance_proxy','forecast_variance_ag', 'forecast_variance_ewma']\n",
    "        eval_df_var = evaluation_df.dropna(subset=required_cols_var)\n",
    "        # Further filter for non-negative values required for variance metrics\n",
    "        eval_df_var = eval_df_var[(eval_df_var['actual_variance_proxy'] >= 0) & (eval_df_var['forecast_variance_ag'] >= 0) & (eval_df_var['forecast_variance_ewma'] >= 0)]\n",
    "        valid_var_steps = len(eval_df_var)\n",
    "        results['rolling_valid_steps_variance'] = valid_var_steps\n",
    "        print(\"\\n  1-Step Variance Accuracy (vs Proxy=LogRet^2):\")\n",
    "        if valid_var_steps > 0:\n",
    "\n",
    "            yt_v = eval_df_var['actual_variance_proxy'].values\n",
    "            yp_v_ag = eval_df_var['forecast_variance_ag'].values\n",
    "            yp_v_ewma = eval_df_var['forecast_variance_ewma'].values\n",
    "            # Calculate metrics for AG\n",
    "            results['vol_rmse_ag'] = root_mean_squared_error_variance(yt_v, yp_v_ag)\n",
    "            results['vol_qlike_ag'] = qlike_loss(yt_v, yp_v_ag, epsilon=qlike_eps)\n",
    "            print(f\"    AG    RMSE:{results['vol_rmse_ag']:.8f} QLIKE:{results['vol_qlike_ag']:.6f} ({valid_var_steps} steps)\")\n",
    "            # Calculate metrics for EWMA benchmark\n",
    "            results['benchmarks']['ewma_var']['rmse'] = root_mean_squared_error_variance(yt_v, yp_v_ewma)\n",
    "            results['benchmarks']['ewma_var']['qlike'] = qlike_loss(yt_v, yp_v_ewma, epsilon=qlike_eps)\n",
    "            print(f\"    EWMA  RMSE:{results['benchmarks']['ewma_var']['rmse']:.8f} QLIKE:{results['benchmarks']['ewma_var']['qlike']:.6f} ({valid_var_steps} steps)\")\n",
    "\n",
    "            # Perform Diebold-Mariano Test for Variance Forecasts (AG vs EWMA)\n",
    "            print(f\"\\n--- Diebold-Mariano Test for Variance (Backtest, h=1, Compare AG vs. EWMA) ---\")\n",
    "            dm_results_var_bt = {}\n",
    "            if not DIEBOLDMARIANO_LIB_AVAILABLE:\n",
    "                 dm_results_var_bt = {'error': \"Library not found\"}\n",
    "                 print(\"  Variance DM Test skipped: 'dieboldmariano' library not installed.\")\n",
    "            else:\n",
    "                 actuals_v_dm = eval_df_var['actual_variance_proxy']\n",
    "                 forecasts_ag_v_dm = eval_df_var['forecast_variance_ag']\n",
    "                 forecasts_ewma_v_dm = eval_df_var['forecast_variance_ewma']\n",
    "                 # Pass epsilon if using QLIKE\n",
    "                 eps_arg_dm = {'qlike_epsilon': qlike_eps} if dm_var_loss == \"QLIKE\" else {} # Use defined dm_var_loss\n",
    "                 # H1: AG model has lower loss than EWMA model (alternative='less')\n",
    "                 dm_test_var_output = diebold_mariano_test(\n",
    "                     actuals_v_dm, forecasts_ag_v_dm, forecasts_ewma_v_dm,\n",
    "                     h=1, loss_type=dm_var_loss, alternative='less', **eps_arg_dm # Use defined dm_var_loss\n",
    "                 )\n",
    "                 dm_results_var_bt = dm_test_var_output.copy()\n",
    "                 dm_results_var_bt['is_significant'] = False # Initialize\n",
    "                 if dm_test_var_output['error'] is not None:\n",
    "                     dm_results_var_bt['interpretation'] = f\"Error: {dm_test_var_output['error']}\"\n",
    "                 elif pd.notna(dm_test_var_output['p_value']):\n",
    "                     p_val, dm_stat = dm_test_var_output['p_value'], dm_test_var_output['dm_stat']\n",
    "                     if p_val < alpha: # Use defined alpha\n",
    "                         if dm_stat < 0:\n",
    "                             dm_results_var_bt['interpretation'] = f\"AG significantly better (p={p_val:.4f} < {alpha})\"\n",
    "                             dm_results_var_bt['is_significant'] = True\n",
    "                         else:\n",
    "                             dm_results_var_bt['interpretation'] = f\"Significant (p={p_val:.4f}), but DM Stat >= 0 ({dm_stat:.2f}) - Contradiction?\"\n",
    "                     else:\n",
    "                         dm_results_var_bt['interpretation'] = f\"No significant difference (p={p_val:.4f} >= {alpha})\"\n",
    "                 else:\n",
    "                     dm_results_var_bt['interpretation'] = \"Test result invalid (NaN p-value)\"\n",
    "                 print(f\"  Loss='{dm_var_loss}' | N={dm_results_var_bt['n_obs']:<5} | DM Stat={dm_results_var_bt['dm_stat']:>8.3f} | p-value={dm_results_var_bt['p_value']:>8.4f} | {dm_results_var_bt['interpretation']}\")\n",
    "            results['variance_dm']['backtest'] = dm_results_var_bt\n",
    "        else:\n",
    "            log.warning(\"No valid steps for 1-step variance evaluation (including DM test).\")\n",
    "            results.update({k:np.nan for k in ['vol_rmse_ag','vol_qlike_ag']})\n",
    "            results['benchmarks']['ewma_var'] = {'rmse': np.nan, 'qlike': np.nan}\n",
    "            results['variance_dm']['backtest'] = {'error': 'No valid steps for variance evaluation'}\n",
    "\n",
    "        # --- Evaluate VaR/ES Backtest ---\n",
    "        print(f\"\\n--- VaR Backtesting (Backtest, alpha={alpha:.2%}) ---\")\n",
    "        # Extract violation series (dropping NaNs which indicate calculation failure)\n",
    "        violation_ag_series = evaluation_df['violation_ag'].dropna().astype(int).values\n",
    "        violation_ewma_series = evaluation_df['violation_ewma'].dropna().astype(int).values\n",
    "        n_backtest_ag = len(violation_ag_series)\n",
    "        n_backtest_ewma = len(violation_ewma_series)\n",
    "\n",
    "        results['backtest_n_obs_var'] = n_backtest_ag # Store number of observations for backtest\n",
    "\n",
    "\n",
    "\n",
    "        if n_backtest_ag < min_resid_len_diag: # Use same min length as residual diagnostics\n",
    "            log.warning(f\"Skipping VaR backtests: Not enough valid observations for AG model ({n_backtest_ag} < {min_resid_len_diag}).\")\n",
    "            results.update({k: np.nan for k in ['backtest_violations_ag', 'backtest_kupiec_p_ag', 'backtest_christ_p_ag']})\n",
    "        else:\n",
    "            n_violations_ag = int(np.sum(violation_ag_series))\n",
    "            results['backtest_violations_ag'] = n_violations_ag\n",
    "            print(f\"  AG Model: Violations={n_violations_ag}/{n_backtest_ag} (Expected: {n_backtest_ag*alpha:.1f})\")\n",
    "            kupiec_ag_res = kupiec_test(n_violations_ag, n_backtest_ag, alpha)\n",
    "            christ_ag_res = christoffersen_test(violation_ag_series, alpha)\n",
    "            results['backtest_kupiec_p_ag'] = kupiec_ag_res.get('p_value')\n",
    "            results['backtest_christ_p_ag'] = christ_ag_res.get('p_value')\n",
    "            results['backtest_kupiec_stat_ag'] = kupiec_ag_res.get('LR_stat')\n",
    "            results['backtest_christ_stat_ag'] = christ_ag_res.get('LR_stat')\n",
    "            print(f\"    Kupiec p={results['backtest_kupiec_p_ag']:.4f}, Christoffersen p={results['backtest_christ_p_ag']:.4f}\")\n",
    "\n",
    "        if n_backtest_ewma < min_resid_len_diag:\n",
    "            log.warning(f\"Skipping VaR backtests: Not enough valid observations for EWMA model ({n_backtest_ewma} < {min_resid_len_diag}).\")\n",
    "            results.update({k: np.nan for k in ['backtest_violations_ewma', 'backtest_kupiec_p_ewma', 'backtest_christ_p_ewma']})\n",
    "        else:\n",
    "            n_violations_ewma = int(np.sum(violation_ewma_series))\n",
    "            results['backtest_violations_ewma'] = n_violations_ewma\n",
    "            print(f\"  EWMA Benchmark: Violations={n_violations_ewma}/{n_backtest_ewma} (Expected: {n_backtest_ewma*alpha:.1f})\")\n",
    "            kupiec_ewma_res = kupiec_test(n_violations_ewma, n_backtest_ewma, alpha)\n",
    "            christ_ewma_res = christoffersen_test(violation_ewma_series, alpha)\n",
    "            results['backtest_kupiec_p_ewma'] = kupiec_ewma_res.get('p_value')\n",
    "            results['backtest_christ_p_ewma'] = christ_ewma_res.get('p_value')\n",
    "            results['backtest_kupiec_stat_ewma'] = kupiec_ewma_res.get('LR_stat')\n",
    "            results['backtest_christ_stat_ewma'] = christ_ewma_res.get('LR_stat')\n",
    "            print(f\"    Kupiec p={results['backtest_kupiec_p_ewma']:.4f}, Christoffersen p={results['backtest_christ_p_ewma']:.4f}\")\n",
    "\n",
    "        # Calculate Average VaR/ES over the backtest period\n",
    "        results['backtest_avg_var_ag'] = evaluation_df['var_thresh_ag'].mean()\n",
    "        results['backtest_avg_es_ag'] = evaluation_df['es_ag'].mean()\n",
    "        results['backtest_avg_var_ewma'] = evaluation_df['var_thresh_ewma'].mean()\n",
    "        results['backtest_avg_es_ewma'] = evaluation_df['es_ewma'].mean()\n",
    "        print(f\"  Avg VaR AG={results['backtest_avg_var_ag']:.4f}, Avg ES AG={results['backtest_avg_es_ag']:.4f}\")\n",
    "        print(f\"  Avg VaR EWMA={results['backtest_avg_var_ewma']:.4f}, Avg ES EWMA={results['backtest_avg_es_ewma']:.4f}\")\n",
    "\n",
    "                # --- Store Backtest VaR/ES/Backtest metrics directly in results or roll_fc ---\n",
    "        if n_backtest_ag >= min_resid_len_diag:\n",
    "            results['roll_fc']['backtest_violations_ag'] = n_violations_ag # Store in sub-dict\n",
    "            results['backtest_kupiec_p_ag'] = kupiec_ag_res.get('p_value') # Store directly\n",
    "            results['backtest_christ_p_ag'] = christ_ag_res.get('p_value')\n",
    "        else:\n",
    "             results['roll_fc']['backtest_violations_ag'] = np.nan\n",
    "             results['backtest_kupiec_p_ag'] = np.nan\n",
    "             results['backtest_christ_p_ag'] = np.nan\n",
    "        # Do the same for EWMA results\n",
    "        if n_backtest_ewma >= min_resid_len_diag:\n",
    "             results['roll_fc']['backtest_violations_ewma'] = n_violations_ewma\n",
    "             results['backtest_kupiec_p_ewma'] = kupiec_ewma_res.get('p_value')\n",
    "             results['backtest_christ_p_ewma'] = christ_ewma_res.get('p_value')\n",
    "        else:\n",
    "             results['roll_fc']['backtest_violations_ewma'] = np.nan\n",
    "             results['backtest_kupiec_p_ewma'] = np.nan\n",
    "             results['backtest_christ_p_ewma'] = np.nan\n",
    "\n",
    "        results['roll_fc']['backtest_avg_var_ag'] = evaluation_df['var_thresh_ag'].mean()\n",
    "        results['roll_fc']['backtest_avg_es_ag'] = evaluation_df['es_ag'].mean()\n",
    "        results['roll_fc']['backtest_avg_var_ewma'] = evaluation_df['var_thresh_ewma'].mean()\n",
    "        results['roll_fc']['backtest_avg_es_ewma'] = evaluation_df['es_ewma'].mean()\n",
    "        results['roll_fc']['backtest_n_obs_var'] = n_backtest_ag \n",
    "\n",
    "\n",
    "                # --- Generate and Store Thesis Recommendations for Backtest Mode ---\n",
    "        alpha_bt = config.get(\"dm_test_alpha\", 0.05)\n",
    "        dm_loss_price_bt = config.get(\"dm_test_loss_type\", \"Squared Error\")\n",
    "        dm_loss_var_bt = config.get(\"dm_test_variance_loss_type\", \"QLIKE\")\n",
    "\n",
    "        # --- Price DM Recommendation ---\n",
    "        reco_price_text_bt = f\"Diebold-Mariano Test (Price, AG vs Naive, h=1, alpha={alpha_bt:.2%}, H1: AG better, Loss={dm_loss_price_bt}):\\n\"\n",
    "        dm_p_res_bt = results.get('roll_fc_price_dm_test', {}) \n",
    "        if dm_p_res_bt.get('error'):\n",
    "            reco_price_text_bt += f\"- Test Error: {dm_p_res_bt['error']}.\\n\"\n",
    "        elif pd.notna(dm_p_res_bt.get('p_value')):\n",
    "            p_val = dm_p_res_bt['p_value']\n",
    "            dm_stat = dm_p_res_bt.get('dm_stat', 0)\n",
    "            if p_val < alpha_bt and dm_stat < 0:\n",
    "                 reco_price_text_bt += f\"- AG significantly better (p={p_val:.3f}).\\n\"\n",
    "            else:\n",
    "                 reco_price_text_bt += f\"- No significant difference (p={p_val:.3f}).\\n\"\n",
    "        else: reco_price_text_bt += \"- Test Skipped or Failed.\\n\"\n",
    "\n",
    "        results[\"thesis_reco_price_dm\"] = reco_price_text_bt \n",
    "\n",
    "        # --- Variance DM Recommendation ---\n",
    "        reco_var_text_bt = f\"Diebold-Mariano Test (Variance, AG vs EWMA, h=1, alpha={alpha_bt:.2%}, H1: AG better, Loss={dm_loss_var_bt}):\\n\"\n",
    "        dm_v_res_bt = results.get('variance_dm', {}).get('backtest', {}) \n",
    "        if dm_v_res_bt.get('error'):\n",
    "             reco_var_text_bt += f\"- Test Error: {dm_v_res_bt['error']}.\\n\"\n",
    "        elif pd.notna(dm_v_res_bt.get('p_value')):\n",
    "             p_val = dm_v_res_bt['p_value']\n",
    "             dm_stat = dm_v_res_bt.get('dm_stat', 0)\n",
    "             if p_val < alpha_bt and dm_stat < 0:\n",
    "                  reco_var_text_bt += f\"- AG significantly better (p={p_val:.3f}).\\n\"\n",
    "             else:\n",
    "                  reco_var_text_bt += f\"- No significant difference (p={p_val:.3f}).\\n\"\n",
    "        else: reco_var_text_bt += \"- Test Skipped or Failed.\\n\"\n",
    "\n",
    "        results[\"thesis_reco_var_dm\"] = reco_var_text_bt \n",
    "\n",
    "\n",
    "        # --- VaR Backtesting Recommendation ---\n",
    "        reco_var_backtest_text_bt = f\"VaR Backtesting (AG vs EWMA, h=1, alpha={alpha_bt:.2%}):\\n\"\n",
    "\n",
    "        kup_p_ag = results.get('roll_fc', {}).get('bt_kupiec_p_ag', results.get('bt_kupiec_p_ag')) \n",
    "        chr_p_ag = results.get('roll_fc', {}).get('bt_christ_p_ag', results.get('bt_christ_p_ag')) \n",
    "        kup_p_ewma = results.get('roll_fc', {}).get('bt_kupiec_p_ewma', results.get('bt_kupiec_p_ewma')) \n",
    "        kup_p_ag_str = f\"{kup_p_ag:.3f}\" if pd.notna(kup_p_ag) else \"N/A\"\n",
    "        chr_p_ag_str = f\"{chr_p_ag:.3f}\" if pd.notna(chr_p_ag) else \"N/A\"\n",
    "        kup_p_ewma_str = f\"{kup_p_ewma:.3f}\" if pd.notna(kup_p_ewma) else \"N/A\"\n",
    "        reco_var_backtest_text_bt += f\"- AG Model: Kupiec {'OK' if pd.isna(kup_p_ag) or kup_p_ag >= alpha_bt else 'REJECTED'} (p={kup_p_ag_str}), Christoffersen {'OK' if pd.isna(chr_p_ag) or chr_p_ag >= alpha_bt else 'REJECTED'} (p={chr_p_ag_str}).\\n\"\n",
    "        reco_var_backtest_text_bt += f\"- EWMA Model: Kupiec {'OK' if pd.isna(kup_p_ewma) or kup_p_ewma >= alpha_bt else 'REJECTED'} (p={kup_p_ewma_str}).\\n\"\n",
    "\n",
    "        results[\"thesis_reco_var_backtest\"] = reco_var_backtest_text_bt \n",
    "\n",
    "        # --- END Recommendation Block ---\n",
    "\n",
    "\n",
    "        # --- Residual Diagnostics on Rolling Forecast Errors --- \n",
    "        required_cols_resid=['actual_log_return','forecast_log_return_ag','forecast_volatility_ag']\n",
    "        eval_df_resid=evaluation_df.dropna(subset=required_cols_resid)\n",
    "        valid_resid_steps = len(eval_df_resid)\n",
    "\n",
    "\n",
    "        if valid_resid_steps > min_resid_len_diag:\n",
    "            log.info(f\"Diagnosing {valid_resid_steps} standardized 1-step AG residuals...\")\n",
    "            # Calculate standardized residuals: (actual - forecast) / forecast_volatility\n",
    "            forecast_error=eval_df_resid['actual_log_return']-eval_df_resid['forecast_log_return_ag']\n",
    "            forecast_vol=eval_df_resid['forecast_volatility_ag'].replace(0,np.nan).clip(lower=1e-8) # Avoid division by zero/small numbers\n",
    "            standardized_residuals=(forecast_error/forecast_vol).dropna()\n",
    "\n",
    "            if len(standardized_residuals)>min_resid_len_diag:\n",
    "                 print(\"\\n  Rolling 1-Step Residual Diagnostics (Standardized, AG Model):\")\n",
    "                 # Determine lags for tests (e.g., min(20, N/2 - 1))\n",
    "                 lb_lags_diag = min(20, len(standardized_residuals)//2 - 1) if len(standardized_residuals) > 5 else 0\n",
    "                 arch_lags_diag = min(12, len(standardized_residuals)//2 - 1) if len(standardized_residuals) > 5 else 0\n",
    "\n",
    "                 # Ljung-Box test on standardized residuals (check for remaining autocorrelation)\n",
    "                 lb_result=ljung_box_test(standardized_residuals,l=lb_lags_diag if lb_lags_diag>0 else 1)\n",
    "                 # ARCH-LM test on standardized residuals (not squared here, test for ARCH in std resid)\n",
    "                 arch_result=arch_test(standardized_residuals,l=arch_lags_diag if arch_lags_diag>0 else 1)\n",
    "\n",
    "                 results['resid_lb_pvalue']=lb_result.get('lb_pvalue')\n",
    "                 results['resid_lb_wn']=lb_result.get('is_white_noise') # True if no autocorrelation\n",
    "                 results['resid_arch_pvalue']=arch_result.get('arch_pvalue')\n",
    "                 results['resid_arch_het']=arch_result.get('heteroskedastic') # True if ARCH effects remain\n",
    "\n",
    "                 print(f\"    Ljung-Box p={results['resid_lb_pvalue']:.4f} (Lag={lb_result.get('lb_lag_tested',0)}, White Noise={results['resid_lb_wn']})\")\n",
    "                 print(f\"    ARCH LM   p={results['resid_arch_pvalue']:.4f} (Lag={arch_result.get('arch_lag_tested',0)}, Heteroskedastic={results['resid_arch_het']})\")\n",
    "\n",
    "                 # Plot standardized residuals if plotting enabled\n",
    "                 if is_plot_available and plt is not None:\n",
    "                     try:\n",
    "                         fig,ax=plt.subplots(figsize=(12,5))\n",
    "                         ax.plot(standardized_residuals.index,standardized_residuals.values,lw=1.0,marker='.',ms=3,alpha=0.7,label='Standardized Residuals (AG)',color='black')\n",
    "                         ax.set_title(f\"Standardized 1-Step Residuals ({coin_id}, d={d_final})\\n({final_model_desc})\")\n",
    "                         ax.axhline(0,color='red',linestyle='--')\n",
    "                         ax.axhline(1.96,color='grey',linestyle=':', linewidth=0.8) # 95% bounds for approx normal\n",
    "                         ax.axhline(-1.96,color='grey',linestyle=':', linewidth=0.8)\n",
    "                         ax.grid(True); ax.legend(); fig.tight_layout()\n",
    "                         resid_filename=os.path.join(coin_plot_dir,f\"{coin_id}_residuals_roll_std_d{d_final}.png\")\n",
    "                         fig.savefig(resid_filename); plt.close(fig)\n",
    "                         log.info(f\"Rolling residuals plot saved: {resid_filename}\")\n",
    "                     except Exception as e: log.error(f\"Plotting rolling residuals failed: {e}\"); plt.close('all')\n",
    "\n",
    "            else:\n",
    "                 log.warning(f\"Not enough standardized residuals ({len(standardized_residuals)}) for diagnostic tests (require > {min_resid_len_diag}).\")\n",
    "                 results.update({k:np.nan for k in ['resid_lb_pvalue','resid_lb_wn','resid_arch_pvalue','resid_arch_het']})\n",
    "        else:\n",
    "            log.warning(f\"Skipping residual diagnostics: Not enough valid forecast steps ({valid_resid_steps}) with required data.\")\n",
    "            results.update({k:np.nan for k in ['resid_lb_pvalue','resid_lb_wn','resid_arch_pvalue','resid_arch_het']})\n",
    "\n",
    "        # --- Generate Parameter Stability Plot --- \n",
    "        if gen_stab and stability_dates_list and any(len(v)>1 for v in stability_param_data.values()):\n",
    "             log.info(\"Generating parameter stability plot...\")\n",
    "             stab_filename=os.path.join(coin_plot_dir,f\"{coin_id}_param_stab_d{d_final}.png\")\n",
    "             stab_title=f\"{coin_id} Parameter Stability (Backtest, d={d_final})\\n({final_model_desc})\"\n",
    "             # Filter out parameters that were never estimated or always NaN\n",
    "             valid_stab_data={k:v for k,v in stability_param_data.items() if any(pd.notna(x) for x in v)}\n",
    "             if valid_stab_data:\n",
    "                 try:\n",
    "                     plot_ok=plot_parameter_stability(valid_stab_data, stability_dates_list, stab_filename, stab_title)\n",
    "                     results[\"parameter_stability_plot_path\"]=stab_filename if plot_ok else \"Failed\"\n",
    "                 except Exception as e:\n",
    "                     log.error(f\"Generating parameter stability plot failed: {e}\")\n",
    "                     results[\"parameter_stability_plot_path\"]=f\"Error: {e}\"\n",
    "             else:\n",
    "                 log.warning(\"Skipping stability plot: No valid parameter data collected during backtest.\")\n",
    "                 results[\"parameter_stability_plot_path\"]=\"Skipped (no valid data)\"\n",
    "        elif gen_stab:\n",
    "             log.warning(\"Skipping stability plot: Stability tracking disabled or no data collected.\")\n",
    "             results[\"parameter_stability_plot_path\"]=\"Skipped (no data/disabled)\"\n",
    "\n",
    "        # --- Save Rolling Forecast Results --- \n",
    "        try:\n",
    "            eval_filename=os.path.join(coin_plot_dir,f\"{coin_id}_rolling_1step_results_d{d_final}.csv\")\n",
    "            evaluation_df.to_csv(eval_filename,index=True,float_format=\"%.8f\")\n",
    "            log.info(f\"Rolling 1-step results CSV saved: {eval_filename}\")\n",
    "            results[\"rolling_results_csv_path\"]=eval_filename\n",
    "        except Exception as e:\n",
    "            log.error(f\"Saving rolling 1-step results CSV failed: {e}\")\n",
    "            results[\"rolling_results_csv_path\"]=f\"Failed: {e}\"\n",
    "\n",
    "        # --- Generate Plots for Rolling Forecasts --- \n",
    "        if is_plot_available and plt is not None:\n",
    "             # Plot Price Forecasts vs Actuals\n",
    "             if valid_price_steps>0:\n",
    "                 try:\n",
    "                     fig,ax=plt.subplots(figsize=(14,7))\n",
    "                     ax.plot(eval_df_price.index,eval_df_price['actual_price'],label='Actual Price',color='black',lw=1.5)\n",
    "                     ax.plot(eval_df_price.index,eval_df_price['forecast_price_ag'],label='AG Forecast (t+1)',color='red',ls='--',lw=1.0,alpha=0.9)\n",
    "                     ax.plot(eval_df_price.index,eval_df_price['forecast_price_naive'],label='Naive Forecast (y_t)',color='blue',ls=':',lw=1.0,alpha=0.7)\n",
    "                     ax.set_title(f'{coin_id} Rolling 1-Step Price Forecast (d={d_final})\\n({final_model_desc})')\n",
    "                     ax.set_ylabel(\"Price\")\n",
    "                     ax.legend(); ax.grid(True,ls=':')\n",
    "                     plt.setp(ax.xaxis.get_majorticklabels(),rotation=45,ha='right')\n",
    "                     fig.tight_layout()\n",
    "                     price_plot_filename=os.path.join(coin_plot_dir,f\"{coin_id}_roll_1step_price_fc_d{d_final}.png\")\n",
    "                     fig.savefig(price_plot_filename); plt.close(fig)\n",
    "                     log.info(f\"Rolling 1-step price plot saved: {price_plot_filename}\")\n",
    "                 except Exception as e: log.error(f\"Rolling 1-step price plot failed: {e}\"); plt.close('all')\n",
    "             else: log.warning(\"Skipping rolling 1-step price plot: No valid steps.\")\n",
    "\n",
    "             # Plot Volatility Forecasts vs Actual Proxy\n",
    "             volatility_index=evaluation_df.index\n",
    "             if not volatility_index.empty:\n",
    "                 try:\n",
    "                     fig,ax=plt.subplots(figsize=(14,7))\n",
    "                     # Use absolute log return as a proxy for actual volatility\n",
    "                     actual_vol_proxy=np.sqrt(evaluation_df.loc[volatility_index,'actual_variance_proxy'].clip(0)) # sqrt(proxy) = |logret|\n",
    "                     ax.plot(volatility_index, actual_vol_proxy, label='Actual Proxy |LogRet|',color='black',lw=1.2,alpha=0.5)\n",
    "                     # Plot AG volatility forecast if available\n",
    "                     eval_vol_ag_valid = evaluation_df['forecast_volatility_ag'].dropna()\n",
    "                     if not eval_vol_ag_valid.empty:\n",
    "                         ax.plot(eval_vol_ag_valid.index, eval_vol_ag_valid, label='AG Forecast Vol (t+1)',color='red',ls='-',lw=1.5)\n",
    "                     # Plot EWMA volatility forecast if available\n",
    "                     ewma_vol_valid = np.sqrt(evaluation_df['forecast_variance_ewma'].dropna().clip(0))\n",
    "                     if not ewma_vol_valid.empty:\n",
    "                         ax.plot(ewma_vol_valid.index, ewma_vol_valid, label='EWMA Forecast Vol (t+1)',color='blue',ls=':',lw=1.2,alpha=0.8)\n",
    "\n",
    "                     ax.set_title(f'{coin_id} Rolling 1-Step Volatility Forecast (AG vs EWMA, d={d_final})\\n({final_model_desc})')\n",
    "                     ax.set_ylabel(\"Volatility (Daily)\")\n",
    "                     ax.legend(); ax.grid(True,ls=':')\n",
    "                     plt.setp(ax.xaxis.get_majorticklabels(),rotation=45,ha='right')\n",
    "                     fig.tight_layout()\n",
    "                     vol_plot_filename=os.path.join(coin_plot_dir,f\"{coin_id}_roll_1step_vol_fc_ag_vs_ewma_d{d_final}.png\")\n",
    "                     fig.savefig(vol_plot_filename); plt.close(fig)\n",
    "                     log.info(f\"Rolling 1-step volatility plot (AG vs EWMA) saved: {vol_plot_filename}\")\n",
    "                 except Exception as e: log.error(f\"Rolling 1-step volatility plot failed: {e}\"); plt.close('all')\n",
    "             else: log.warning(\"Skipping rolling 1-step volatility plot: No forecast data.\")\n",
    "\n",
    "             # --- Plot VaR Thresholds ---\n",
    "             if is_plot_available and plt is not None and 'actual_log_return' in evaluation_df.columns:\n",
    "                  eval_df_var_plot = evaluation_df.dropna(subset=['actual_log_return', 'var_thresh_ag', 'var_thresh_ewma'])\n",
    "                  if not eval_df_var_plot.empty:\n",
    "                       try:\n",
    "                           fig, ax = plt.subplots(figsize=(14, 7))\n",
    "                           ax.plot(eval_df_var_plot.index, eval_df_var_plot['actual_log_return'], label='Actual Log Return', color='black', alpha=0.6, lw=1.0)\n",
    "                           ax.plot(eval_df_var_plot.index, eval_df_var_plot['var_thresh_ag'], label=f'AG VaR ({alpha:.0%})', color='red', ls='--', lw=1.2)\n",
    "                           ax.plot(eval_df_var_plot.index, eval_df_var_plot['var_thresh_ewma'], label=f'EWMA VaR ({alpha:.0%})', color='blue', ls=':', lw=1.2)\n",
    "                           # Highlight violations for AG model\n",
    "                           violations_idx_ag = eval_df_var_plot[eval_df_var_plot['violation_ag'] == 1].index\n",
    "                           if not violations_idx_ag.empty:\n",
    "                               ax.scatter(violations_idx_ag, eval_df_var_plot.loc[violations_idx_ag, 'actual_log_return'], color='red', marker='o', s=20, label='AG Violation')\n",
    "\n",
    "                           ax.set_title(f'{coin_id} Rolling 1-Step VaR ({alpha:.0%}) Thresholds vs Actual Log Return (d={d_final})')\n",
    "                           ax.set_ylabel(\"Log Return / VaR Threshold\")\n",
    "                           ax.legend(); ax.grid(True, ls=':')\n",
    "                           plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "                           fig.tight_layout()\n",
    "                           var_plot_filename = os.path.join(coin_plot_dir, f\"{coin_id}_roll_1step_var_thresh_d{d_final}.png\")\n",
    "                           fig.savefig(var_plot_filename); plt.close(fig)\n",
    "                           log.info(f\"Rolling 1-step VaR plot saved: {var_plot_filename}\")\n",
    "                       except Exception as e:\n",
    "                           log.error(f\"Rolling 1-step VaR plot failed: {e}\"); plt.close('all')\n",
    "                  else:\n",
    "                      log.warning(\"Skipping rolling 1-step VaR plot: No valid data points.\")\n",
    "\n",
    "\n",
    "\n",
    "        # --- Mode 2: Multi-Horizon Evaluation ('horizon_evaluation') ---\n",
    "    elif forecast_mode == 'horizon_evaluation':\n",
    "\n",
    "        dm_loss_type_price = config.get(\"dm_test_loss_type\", \"Squared Error\")\n",
    "        dm_loss_type_var = config.get(\"dm_test_variance_loss_type\", \"QLIKE\")\n",
    "        alpha = config.get(\"dm_test_alpha\", 0.05)\n",
    "        min_resid_len_diag = 25 # Minimum length for reliable Backtests/Residual Diag\n",
    "\n",
    "\n",
    "        valid_horizons_eval = sorted(list(set(h for h in config.get('evaluation_horizons', []) if isinstance(h, int) and h > 0)))\n",
    "        if not valid_horizons_eval: raise ValueError(\"No valid evaluation_horizons provided for 'horizon_evaluation' mode.\")\n",
    "        horizons = valid_horizons_eval\n",
    "        max_horizon = max(horizons)\n",
    "\n",
    "        log.info(f\"--- [{coin_id}] Starting Multi-Horizon Evaluation ---\")\n",
    "        log.info(f\" VaR/ES/Backtest Alpha: {alpha:.2%}\")\n",
    "\n",
    "        # --- Risk Metrics on Test Set (Empirical) ---\n",
    "        test_returns = test_df['log_return'].dropna()\n",
    "        if not test_returns.empty:\n",
    "            var_95_empirical = value_at_risk(test_returns, alpha)\n",
    "            es_95_empirical = expected_shortfall(test_returns, alpha)\n",
    "            log.info(f\"Empirical Test Set (N={len(test_returns)}): VaR({alpha:.0%})={var_95_empirical:.4f}, ES({alpha:.0%})={es_95_empirical:.4f}\")\n",
    "            results[f'empirical_var{int(alpha*100)}'] = var_95_empirical\n",
    "            results[f'empirical_es{int(alpha*100)}'] = es_95_empirical\n",
    "        else:\n",
    "            log.warning(\"Test set log returns empty, cannot calculate empirical VaR/ES.\")\n",
    "            results[f'empirical_var{int(alpha*100)}'] = np.nan\n",
    "            results[f'empirical_es{int(alpha*100)}'] = np.nan\n",
    "\n",
    "        log.info(f\" Evaluation Horizons: {horizons} days ahead. Model: {final_model_desc}\")\n",
    "        log.info(f\" EWMA Lambda (Benchmark): {ewma_lambda}\")\n",
    "        log.info(f\" Variance DM Test Loss: {dm_var_loss}\")\n",
    "        t_multi_start = time.time()\n",
    "\n",
    "        # Use the initial fit on Train+Val for static forecasts\n",
    "        if initial_arima_fit is None: raise ValueError(\"Initial static ARIMA/GARCH fit failed, cannot perform horizon evaluation.\")\n",
    "        arima_static_fit = initial_arima_fit\n",
    "        garch_static_fit = initial_garch_fit\n",
    "        scale_factor_static = config['default_scale_factor']\n",
    "\n",
    "        # Setup indices for evaluation period\n",
    "        full_data_indexed = processed_data.set_index('date').sort_index()\n",
    "        n_total = len(full_data_indexed)\n",
    "        test_start_date = test_df['date'].iloc[0] if not test_df.empty else None\n",
    "        if test_start_date is None: raise ValueError(\"Test Set is empty, cannot perform horizon evaluation.\")\n",
    "        try:\n",
    "            test_start_loc = full_data_indexed.index.get_loc(test_start_date)\n",
    "            test_start_idx = test_start_loc.start if isinstance(test_start_loc, slice) else test_start_loc\n",
    "        except Exception as e: raise ValueError(f\"Cannot find test start index: {e}\")\n",
    "\n",
    "        # Determine the end point for forecast origins to ensure actuals exist for max_horizon\n",
    "        eval_end_idx = n_total - max_horizon # Last index from which we can forecast max_horizon steps\n",
    "        n_eval_points = eval_end_idx - test_start_idx # Number of forecast origins in the test set\n",
    "\n",
    "        if n_eval_points <= 0:\n",
    "            raise ValueError(f\"Test set too short ({len(test_df)}) for max horizon ({max_horizon}). Need at least {max_horizon} points after test start ({test_start_date.date()}).\")\n",
    "        log.info(f\"Evaluating forecasts from {n_eval_points} points in the test set (indices {test_start_idx} to {eval_end_idx-1}).\")\n",
    "\n",
    "        # Dictionary to store evaluation tuples for each horizon\n",
    "        horizon_eval_data = {h: [] for h in horizons}\n",
    "        fc_generation_errors = 0\n",
    "        fc_inversion_errors = 0\n",
    "        ewma_fc_errors = 0\n",
    "\n",
    "        # --- Loop over forecast origins in the test set ---\n",
    "        for i in range(n_eval_points):\n",
    "            current_eval_origin_idx = test_start_idx + i\n",
    "            current_date = full_data_indexed.index[current_eval_origin_idx]\n",
    "            step_number = i + 1\n",
    "\n",
    "            # Log progress\n",
    "            log_freq=max(1,n_eval_points//10)\n",
    "            if step_number==1 or step_number%log_freq==0 or step_number==n_eval_points:\n",
    "                log.info(f\"Multi-Horizon Eval: Step {step_number}/{n_eval_points} (Forecasting from {current_date.date()})...\")\n",
    "\n",
    "            # Get actual price at origin (t) needed for naive forecast and price inversion\n",
    "            actual_price_t = full_data_indexed.iloc[current_eval_origin_idx]['price']\n",
    "\n",
    "            if pd.isna(actual_price_t):\n",
    "                 log.warning(f\" Step {step_number}: Skipping forecast origin {current_date.date()} due to NaN price.\")\n",
    "                 fc_generation_errors += 1; continue # Skip if origin price is NaN\n",
    "\n",
    "            # Get recent history for undifferencing if needed\n",
    "            recent_log_returns_history_h = None\n",
    "            if d_final > 0:\n",
    "                 hist_end_idx = current_eval_origin_idx # History up to t-1\n",
    "                 hist_start_idx = max(0, hist_end_idx - d_final)\n",
    "                 if hist_end_idx - hist_start_idx == d_final:\n",
    "                     recent_log_returns_history_h = full_data_indexed['log_return'].iloc[hist_start_idx:hist_end_idx].tolist()\n",
    "                 else:\n",
    "                      log.warning(f\" Step {step_number}: Not enough history available at {current_date.date()} for undifferencing d={d_final}.\")\n",
    "\n",
    "            # --- Generate AG multi-step forecast from this origin ---\n",
    "            try:\n",
    "                 fc_mean_series_ag_h, fc_variance_series_ag_h = forecast_arima_garch(\n",
    "                      arima_static_fit, garch_static_fit, steps=max_horizon, sf=scale_factor_static\n",
    "                      )\n",
    "                 log_return_forecasts_diff_ag_h = fc_mean_series_ag_h.values\n",
    "                 variance_forecasts_ag_h = fc_variance_series_ag_h.values.copy()\n",
    "\n",
    "                 # Invert log return forecasts to price level forecasts\n",
    "                 price_forecasts_ag_h = invert_price_forecast(actual_price_t, log_return_forecasts_diff_ag_h, d_final, recent_log_returns_history_h)\n",
    "\n",
    "\n",
    "            except Exception as fc_e:\n",
    "                 log.error(f\" Step {step_number}: Error during AG forecast generation/inversion from {current_date.date()}: {type(fc_e).__name__}: {fc_e}\", exc_info=False) # Log error type\n",
    "                 fc_generation_errors += 1\n",
    "                 continue # Skip to next origin\n",
    "\n",
    "            # --- Generate EWMA multi-step variance forecast from this origin ---\n",
    "            try:\n",
    "                # Use log return history up to time t (inclusive) to calculate EWMA state at t\n",
    "                history_up_to_t_log_ret = full_data_indexed['log_return'].iloc[:current_eval_origin_idx+1].dropna()\n",
    "                ewma_var_fc_h = np.full(max_horizon, np.nan) # Initialize forecast array\n",
    "                ewma_var_state_t = np.nan # State at time t (forecast for all h > 0)\n",
    "\n",
    "                if len(history_up_to_t_log_ret) >= 2:\n",
    "                    ewma_series_approx = (history_up_to_t_log_ret**2).ewm(alpha=1-ewma_lambda, adjust=False).mean()\n",
    "                    if not ewma_series_approx.empty:\n",
    "                        ewma_var_state_t = ewma_series_approx.iloc[-1] # State at time t\n",
    "                        ewma_var_state_t = max(0.0, ewma_var_state_t) if pd.notna(ewma_var_state_t) else np.nan\n",
    "                else:\n",
    "                     log.debug(f\"Step {step_number}: Not enough history ({len(history_up_to_t_log_ret)}) for EWMA forecast from {current_date.date()}.\")\n",
    "\n",
    "                if pd.notna(ewma_var_state_t):\n",
    "                    ewma_var_fc_h.fill(ewma_var_state_t)\n",
    "\n",
    "            except Exception as ewma_e:\n",
    "                 log.error(f\" Step {step_number}: Error during EWMA variance forecast calculation from {current_date.date()}: {ewma_e}\")\n",
    "                 ewma_fc_errors += 1\n",
    "                 ewma_var_fc_h = np.full(max_horizon, np.nan) # Ensure it's NaN on error\n",
    "\n",
    "            # --- Store forecasts and actuals for each required horizon ---\n",
    "            for h in horizons:\n",
    "                 forecast_index_h = h - 1 # 0-based index for forecast arrays\n",
    "                 actual_index_h = current_eval_origin_idx + h # Index of the actual value at t+h\n",
    "\n",
    "                 if actual_index_h < n_total: # Check if actual value exists within data bounds\n",
    "                     actual_price_t_plus_h = full_data_indexed.iloc[actual_index_h]['price']\n",
    "                     actual_log_return_t_plus_h = full_data_indexed.iloc[actual_index_h]['log_return']\n",
    "                     actual_var_proxy_t_plus_h = actual_log_return_t_plus_h**2 if pd.notna(actual_log_return_t_plus_h) else np.nan\n",
    "\n",
    "                     # Get corresponding forecasts\n",
    "                     fc_price_ag_t_plus_h = price_forecasts_ag_h[forecast_index_h]\n",
    "                     naive_forecast_t_plus_h = actual_price_t # Naive price forecast is price at origin t\n",
    "                     fc_var_ag_t_plus_h = variance_forecasts_ag_h[forecast_index_h]\n",
    "                     fc_var_ewma_t_plus_h = ewma_var_fc_h[forecast_index_h]\n",
    "\n",
    "                     # Ensure non-negative variance forecasts before calculating VaR/ES\n",
    "                     fc_var_ag_t_plus_h = max(0, fc_var_ag_t_plus_h) if pd.notna(fc_var_ag_t_plus_h) else np.nan\n",
    "                     fc_var_ewma_t_plus_h = max(0, fc_var_ewma_t_plus_h) if pd.notna(fc_var_ewma_t_plus_h) else np.nan\n",
    "\n",
    "                     # Calculate volatility forecasts\n",
    "                     fc_vol_ag_h = np.sqrt(fc_var_ag_t_plus_h) if pd.notna(fc_var_ag_t_plus_h) else np.nan\n",
    "                     fc_vol_ewma_h = np.sqrt(fc_var_ewma_t_plus_h) if pd.notna(fc_var_ewma_t_plus_h) else np.nan\n",
    "\n",
    "                     # Get the h-step ahead UNDIFFERENCED log return forecast mean\n",
    "                     mean_fc_ag_h_undiff = np.nan\n",
    "                     if d_final == 0:\n",
    "                          mean_fc_ag_h_undiff = log_return_forecasts_diff_ag_h[forecast_index_h]\n",
    "                     else:\n",
    "                          # Placeholder: Use differenced forecast if d>0, needs proper implementation\n",
    "                          mean_fc_ag_h_undiff = log_return_forecasts_diff_ag_h[forecast_index_h]\n",
    "                          # log.debug(f\"H={h}: Using differenced logret fc ({mean_fc_ag_h_undiff:.4f}) for VaR calc (approximate if d>0).\") # Reduced verbosity\n",
    "\n",
    "\n",
    "                     # --- VaR/ES Calculation & Violation Tracking for Horizon h ---\n",
    "                     var_thresh_ag_h, es_ag_h = np.nan, np.nan\n",
    "                     var_thresh_ewma_h, es_ewma_h = np.nan, np.nan\n",
    "                     violation_ag_h = np.nan\n",
    "                     violation_ewma_h = np.nan\n",
    "\n",
    "                     # Extract distribution parameters from static GARCH fit if needed\n",
    "                     dist_params_model = None\n",
    "                     if garch_static_fit is not None:\n",
    "                          params_garch = getattr(garch_static_fit,'params',{})\n",
    "                          if chosen_garch_dist_final == 't' and 'nu' in params_garch:\n",
    "                              dist_params_model = {'nu': params_garch['nu']}\n",
    "                          elif chosen_garch_dist_final == 'skewt' and 'nu' in params_garch and 'lambda' in params_garch:\n",
    "                              dist_params_model = {'nu': params_garch['nu'], 'lambda': params_garch['lambda']}\n",
    "\n",
    "                     # Calculate Model VaR/ES\n",
    "                     if pd.notna(mean_fc_ag_h_undiff) and pd.notna(fc_vol_ag_h):\n",
    "                           var_thresh_ag_h, es_ag_h = calculate_parametric_var_es(\n",
    "                               mean_fc_ag_h_undiff, fc_vol_ag_h, chosen_garch_dist_final, alpha, dist_params_model\n",
    "                           )\n",
    "\n",
    "                     # Calculate Benchmark VaR/ES (EWMA + Normal assumption, 0 mean)\n",
    "                     if pd.notna(fc_vol_ewma_h):\n",
    "                           var_thresh_ewma_h, es_ewma_h = calculate_parametric_var_es(\n",
    "                               0.0, fc_vol_ewma_h, 'normal', alpha\n",
    "                           )\n",
    "\n",
    "                     # Determine Violations if actual return and VaR threshold are valid\n",
    "                     if pd.notna(actual_log_return_t_plus_h):\n",
    "                         if pd.notna(var_thresh_ag_h):\n",
    "                             violation_ag_h = 1 if actual_log_return_t_plus_h < var_thresh_ag_h else 0\n",
    "                         if pd.notna(var_thresh_ewma_h):\n",
    "                             violation_ewma_h = 1 if actual_log_return_t_plus_h < var_thresh_ewma_h else 0\n",
    "\n",
    "\n",
    "                     # Store results for this horizon, including new VaR/ES data\n",
    "                     store_flag = all(pd.notna(v) for v in [\n",
    "                         actual_price_t_plus_h, fc_price_ag_t_plus_h, naive_forecast_t_plus_h,\n",
    "                         actual_var_proxy_t_plus_h, fc_var_ag_t_plus_h, fc_var_ewma_t_plus_h,\n",
    "                         var_thresh_ag_h, var_thresh_ewma_h # Only check VaR thresholds for storing basic metrics\n",
    "                     ]) and actual_var_proxy_t_plus_h >= 0 and fc_var_ag_t_plus_h >= 0 and fc_var_ewma_t_plus_h >= 0\n",
    "\n",
    "                     if store_flag:\n",
    "                          horizon_eval_data[h].append((\n",
    "                              actual_price_t_plus_h, fc_price_ag_t_plus_h, naive_forecast_t_plus_h,\n",
    "                              actual_var_proxy_t_plus_h, fc_var_ag_t_plus_h, fc_var_ewma_t_plus_h,\n",
    "                              current_eval_origin_idx, # Keep origin index\n",
    "                              var_thresh_ag_h, es_ag_h, violation_ag_h, # Model risk metrics\n",
    "                              var_thresh_ewma_h, es_ewma_h, violation_ewma_h, # Benchmark risk metrics\n",
    "                              actual_log_return_t_plus_h # Store actual return needed for backtest\n",
    "                          ))\n",
    "                     else:\n",
    "                          # Log only if essential components are missing\n",
    "                          if pd.isna(actual_price_t_plus_h) or pd.isna(actual_var_proxy_t_plus_h): log.debug(f\"Step {step_number}, H={h}: Actual price/var_proxy NaN.\")\n",
    "                          elif pd.isna(fc_price_ag_t_plus_h) or pd.isna(fc_var_ag_t_plus_h) : log.debug(f\"Step {step_number}, H={h}: AG price/var forecast NaN.\")\n",
    "                          elif pd.isna(fc_var_ewma_t_plus_h): log.debug(f\"Step {step_number}, H={h}: EWMA var forecast NaN.\")\n",
    "                          elif pd.isna(var_thresh_ag_h): log.debug(f\"Step {step_number}, H={h}: AG VaR threshold NaN.\")\n",
    "                          elif pd.isna(var_thresh_ewma_h): log.debug(f\"Step {step_number}, H={h}: EWMA VaR threshold NaN.\")\n",
    "\n",
    "\n",
    "                 else:\n",
    "                     log.warning(f\" Step {step_number}: Actual index {actual_index_h} is out of bounds for horizon h={h}.\")\n",
    "                     break # No point checking further horizons for this origin\n",
    "        # --- End of Multi-Horizon Evaluation Loop ---\n",
    "\n",
    "        # --- Calculate and Report Multi-Horizon Metrics and DM Tests ---\n",
    "        results[\"multi_horizon_eval\"][\"horizons\"] = horizons\n",
    "        results[\"multi_horizon_eval\"][\"n_eval_points\"] = n_eval_points\n",
    "        results[\"multi_horizon_eval\"][\"forecast_generation_errors\"] = fc_generation_errors\n",
    "        results[\"multi_horizon_eval\"][\"forecast_inversion_errors\"] = fc_inversion_errors\n",
    "        results[\"multi_horizon_eval\"][\"ewma_forecast_errors\"] = ewma_fc_errors\n",
    "\n",
    "        print(f\"\\n--- Multi-Horizon Evaluation Results (Test Set, d={d_final}) ---\")\n",
    "        log.info(f\"Forecasts generated from {n_eval_points} origin points.\")\n",
    "        if fc_generation_errors > 0: log.warning(f\"{fc_generation_errors} errors encountered during AG forecast generation.\")\n",
    "        if fc_inversion_errors > 0: log.warning(f\"~{fc_inversion_errors} issues encountered during price forecast inversion (check logs).\")\n",
    "        if ewma_fc_errors > 0: log.warning(f\"~{ewma_fc_errors} issues encountered during EWMA forecast calculation (check logs).\")\n",
    "\n",
    "        horizon_metrics_dict = {}\n",
    "        multi_horizon_results_records = [] # For detailed CSV output\n",
    "        dm_results_price_horizon = {}\n",
    "        dm_results_variance_horizon = {}\n",
    "\n",
    "        for h in horizons:\n",
    "            eval_tuples_h = horizon_eval_data.get(h, [])\n",
    "            n_valid_pairs_h = len(eval_tuples_h)\n",
    "\n",
    "            # Initialize metrics for this horizon\n",
    "            metrics_h = {\n",
    "                'mape_ag': np.nan, 'mape_naive': np.nan,\n",
    "                'rmse_price_ag': np.nan, 'rmse_price_naive': np.nan,\n",
    "                'rmse_var_ag': np.nan, 'qlike_var_ag': np.nan,\n",
    "                'rmse_var_ewma': np.nan, 'qlike_var_ewma': np.nan,\n",
    "                'valid_pairs': n_valid_pairs_h,\n",
    "                # --- Init VaR/ES/Backtest metrics ---\n",
    "                'avg_var_ag': np.nan, 'avg_es_ag': np.nan, 'violations_ag': 0, 'kupiec_p_ag': np.nan, 'christ_p_ag': np.nan,\n",
    "                'avg_var_ewma': np.nan, 'avg_es_ewma': np.nan, 'violations_ewma': 0, 'kupiec_p_ewma': np.nan, 'christ_p_ewma': np.nan,\n",
    "                'n_backtest_obs': 0\n",
    "\n",
    "            }\n",
    "            dm_result_price_h = {'dm_stat': np.nan, 'p_value': np.nan, 'interpretation': 'N/A', 'is_significant': False, 'n_obs': 0, 'error': 'No valid pairs', 'loss_type': dm_loss_type_price}\n",
    "            dm_result_var_h = {'dm_stat': np.nan, 'p_value': np.nan, 'interpretation': 'N/A', 'is_significant': False, 'n_obs': 0, 'error': 'No valid pairs', 'loss_type': dm_loss_type_var}\n",
    "\n",
    "            if n_valid_pairs_h > 0:\n",
    "                 # Unpack data for horizon h \n",
    "                 actuals_p_h = np.array([t[0] for t in eval_tuples_h])\n",
    "                 forecasts_p_ag_h = np.array([t[1] for t in eval_tuples_h])\n",
    "                 forecasts_p_n_h = np.array([t[2] for t in eval_tuples_h])\n",
    "                 actuals_v_h = np.array([t[3] for t in eval_tuples_h])\n",
    "                 forecasts_v_ag_h = np.array([t[4] for t in eval_tuples_h])\n",
    "                 forecasts_v_ewma_h = np.array([t[5] for t in eval_tuples_h])\n",
    "                 # --- Unpack VaR/ES/Violation data ---\n",
    "                 var_thresh_ag_h_all   = np.array([t[7] for t in eval_tuples_h])\n",
    "                 es_ag_h_all           = np.array([t[8] for t in eval_tuples_h])\n",
    "                 violations_ag_h_all   = np.array([t[9] for t in eval_tuples_h])\n",
    "                 var_thresh_ewma_h_all = np.array([t[10] for t in eval_tuples_h])\n",
    "                 es_ewma_h_all         = np.array([t[11] for t in eval_tuples_h])\n",
    "                 violations_ewma_h_all = np.array([t[12] for t in eval_tuples_h])\n",
    "                 actual_returns_h      = np.array([t[13] for t in eval_tuples_h])\n",
    "\n",
    "                 # Calculate Price Metrics\n",
    "                 metrics_h['mape_ag'] = mean_absolute_percentage_error(actuals_p_h, forecasts_p_ag_h)\n",
    "                 metrics_h['mape_naive'] = mean_absolute_percentage_error(actuals_p_h, forecasts_p_n_h)\n",
    "                 metrics_h['rmse_price_ag'] = root_mean_squared_error(actuals_p_h, forecasts_p_ag_h)\n",
    "                 metrics_h['rmse_price_naive'] = root_mean_squared_error(actuals_p_h, forecasts_p_n_h)\n",
    "\n",
    "                 # Calculate Variance Metrics\n",
    "                 metrics_h['rmse_var_ag'] = root_mean_squared_error_variance(actuals_v_h, forecasts_v_ag_h)\n",
    "                 metrics_h['qlike_var_ag'] = qlike_loss(actuals_v_h, forecasts_v_ag_h, epsilon=qlike_eps)\n",
    "                 metrics_h['rmse_var_ewma'] = root_mean_squared_error_variance(actuals_v_h, forecasts_v_ewma_h)\n",
    "                 metrics_h['qlike_var_ewma'] = qlike_loss(actuals_v_h, forecasts_v_ewma_h, epsilon=qlike_eps)\n",
    "\n",
    "                 print(f\"  Horizon h={h:<2}: Valid Pairs={n_valid_pairs_h:<5}\")\n",
    "                 print(f\"    Price  MAPE AG={metrics_h['mape_ag']:>8.2f}% | MAPE Naive={metrics_h['mape_naive']:>8.2f}%\")\n",
    "                 print(f\"    Var    RMSE AG={metrics_h['rmse_var_ag']:.8f} | QLIKE AG={metrics_h['qlike_var_ag']:.6f}\")\n",
    "                 print(f\"    Var    RMSE EWMA={metrics_h['rmse_var_ewma']:.8f} | QLIKE EWMA={metrics_h['qlike_var_ewma']:.6f}\")\n",
    "\n",
    "                 # --- VaR Backtesting for Horizon h ---\n",
    "                 violations_ag_series = violations_ag_h_all[pd.notna(violations_ag_h_all)].astype(int)\n",
    "                 violations_ewma_series = violations_ewma_h_all[pd.notna(violations_ewma_h_all)].astype(int)\n",
    "                 n_backtest_ag = len(violations_ag_series)\n",
    "                 n_backtest_ewma = len(violations_ewma_series)\n",
    "                 metrics_h['n_backtest_obs'] = n_backtest_ag # Use AG count as reference\n",
    "\n",
    "                 kupiec_ag = {'p_value':np.nan, 'LR_stat':np.nan, 'error':None}\n",
    "                 christ_ag = {'p_value':np.nan, 'LR_stat':np.nan, 'error':None}\n",
    "                 kupiec_ewma = {'p_value':np.nan, 'LR_stat':np.nan, 'error':None}\n",
    "                 christ_ewma = {'p_value':np.nan, 'LR_stat':np.nan, 'error':None}\n",
    "\n",
    "                 print(f\"    VaR Backtest (h={h}, alpha={alpha:.2%}):\")\n",
    "                 if n_backtest_ag >= min_resid_len_diag: # Check if enough observations for tests\n",
    "                      n_violations_ag = int(np.sum(violations_ag_series))\n",
    "                      metrics_h[f'violations_ag'] = n_violations_ag\n",
    "                      print(f\"      AG Model: N={n_backtest_ag} | Violations={n_violations_ag} (Expected: {n_backtest_ag*alpha:.1f})\")\n",
    "                      kupiec_ag = kupiec_test(n_violations_ag, n_backtest_ag, alpha)\n",
    "                      christ_ag = christoffersen_test(violations_ag_series, alpha)\n",
    "                      metrics_h[f'kupiec_p_ag'] = kupiec_ag.get('p_value')\n",
    "                      metrics_h[f'christ_p_ag'] = christ_ag.get('p_value')\n",
    "                      print(f\"        Kupiec p={metrics_h[f'kupiec_p_ag']:.4f} | Christoffersen p={metrics_h[f'christ_p_ag']:.4f}\")\n",
    "                 else:\n",
    "                      print(f\"      AG Model: N={n_backtest_ag} | Skipped (Too few obs < {min_resid_len_diag})\")\n",
    "\n",
    "                 if n_backtest_ewma >= min_resid_len_diag:\n",
    "                      n_violations_ewma = int(np.sum(violations_ewma_series))\n",
    "                      metrics_h[f'violations_ewma'] = n_violations_ewma\n",
    "                      print(f\"      EWMA Bench: N={n_backtest_ewma} | Violations={n_violations_ewma} (Expected: {n_backtest_ewma*alpha:.1f})\")\n",
    "                      kupiec_ewma = kupiec_test(n_violations_ewma, n_backtest_ewma, alpha)\n",
    "                      christ_ewma = christoffersen_test(violations_ewma_series, alpha) # This test is technically possible for EWMA, might show dependence\n",
    "                      metrics_h[f'kupiec_p_ewma'] = kupiec_ewma.get('p_value')\n",
    "                      metrics_h[f'christ_p_ewma'] = christ_ewma.get('p_value') # Store Christoffersen for EWMA too\n",
    "                      print(f\"        Kupiec p={metrics_h[f'kupiec_p_ewma']:.4f} | Christoffersen p={metrics_h[f'christ_p_ewma']:.4f}\") # Print Christoffersen for EWMA\n",
    "                 else:\n",
    "                      print(f\"      EWMA Bench: N={n_backtest_ewma} | Skipped (Too few obs < {min_resid_len_diag})\")\n",
    "\n",
    "\n",
    "                 # Calculate average VaR/ES for reporting\n",
    "                 metrics_h[f'avg_var_ag'] = np.nanmean(var_thresh_ag_h_all)\n",
    "                 metrics_h[f'avg_es_ag'] = np.nanmean(es_ag_h_all)\n",
    "                 metrics_h[f'avg_var_ewma'] = np.nanmean(var_thresh_ewma_h_all)\n",
    "                 metrics_h[f'avg_es_ewma'] = np.nanmean(es_ewma_h_all)\n",
    "                 print(f\"      Avg VaR AG={metrics_h[f'avg_var_ag']:.4f}, ES AG={metrics_h[f'avg_es_ag']:.4f}\")\n",
    "                 print(f\"      Avg VaR EWMA={metrics_h[f'avg_var_ewma']:.4f}, ES EWMA={metrics_h[f'avg_es_ewma']:.4f}\")\n",
    "                 # --- VaR Backtesting ---\n",
    "\n",
    "\n",
    "                 # --- Keep existing DM Tests ---\n",
    "                 # Perform DM Test for Price (AG vs Naive)\n",
    "                 if DIEBOLDMARIANO_LIB_AVAILABLE:\n",
    "                      dm_test_p_output_h = diebold_mariano_test(actuals_p_h, forecasts_p_ag_h, forecasts_p_n_h, h=h, loss_type=dm_loss_type_price, alternative='less')\n",
    "                      dm_result_price_h.update(dm_test_p_output_h) # Update dict with test results\n",
    "                      # --- DM Interpretation ---\n",
    "                      if dm_test_p_output_h['error'] is None and pd.notna(dm_test_p_output_h['p_value']):\n",
    "                          p_val, dm_stat = dm_test_p_output_h['p_value'], dm_test_p_output_h['dm_stat']\n",
    "                          if p_val < alpha: # Verwende die oben definierte Variable alpha\n",
    "                              if dm_stat < 0: dm_result_price_h['interpretation'] = f\"AG better (p={p_val:.4f})\"\n",
    "                              else: dm_result_price_h['interpretation'] = f\"Sig. (p={p_val:.4f}), but Stat>=0\"\n",
    "                              dm_result_price_h['is_significant'] = True\n",
    "                          else: dm_result_price_h['interpretation'] = f\"No Diff. (p={p_val:.4f})\"\n",
    "                      elif dm_test_p_output_h['error']:\n",
    "                         dm_result_price_h['interpretation'] = f\"Error: {dm_test_p_output_h['error']}\"\n",
    "                      else: # NaN result\n",
    "                         dm_result_price_h['interpretation'] = \"NaN result\"\n",
    "\n",
    "                      print(f\"    DM Price (h={h}): N={dm_result_price_h['n_obs']:<5} | Loss='{dm_loss_type_price}' | p={dm_result_price_h['p_value']:>8.4f} | {dm_result_price_h['interpretation']}\")\n",
    "                 else:\n",
    "                      print(f\"    DM Price (h={h}): Skipped (library missing)\")\n",
    "                      dm_result_price_h['error'] = \"Library missing\"\n",
    "\n",
    "                 # Perform DM Test for Variance (AG vs EWMA)\n",
    "                 if DIEBOLDMARIANO_LIB_AVAILABLE:\n",
    "                      eps_arg_dm_h = {'qlike_epsilon': qlike_eps} if dm_loss_type_var == \"QLIKE\" else {}\n",
    "                      dm_test_v_output_h = diebold_mariano_test(actuals_v_h, forecasts_v_ag_h, forecasts_v_ewma_h, h=h, loss_type=dm_loss_type_var, alternative='less', **eps_arg_dm_h)\n",
    "                      dm_result_var_h.update(dm_test_v_output_h)\n",
    "                      # --- DM Interpretation ---\n",
    "                      if dm_test_v_output_h['error'] is None and pd.notna(dm_test_v_output_h['p_value']):\n",
    "                          p_val, dm_stat = dm_test_v_output_h['p_value'], dm_test_v_output_h['dm_stat']\n",
    "                          if p_val < alpha:\n",
    "                              if dm_stat < 0: dm_result_var_h['interpretation'] = f\"AG better (p={p_val:.4f})\"\n",
    "                              else: dm_result_var_h['interpretation'] = f\"Sig. (p={p_val:.4f}), but Stat>=0\"\n",
    "                              dm_result_var_h['is_significant'] = True\n",
    "                          else: dm_result_var_h['interpretation'] = f\"No Diff. (p={p_val:.4f})\"\n",
    "                      elif dm_test_v_output_h['error']:\n",
    "                         dm_result_var_h['interpretation'] = f\"Error: {dm_test_v_output_h['error']}\"\n",
    "                      else: # NaN result\n",
    "                         dm_result_var_h['interpretation'] = \"NaN result\"\n",
    "\n",
    "                      print(f\"    DM Var   (h={h}): N={dm_result_var_h['n_obs']:<5} | Loss='{dm_loss_type_var}' | p={dm_result_var_h['p_value']:>8.4f} | {dm_result_var_h['interpretation']}\")\n",
    "                 else:\n",
    "                      print(f\"    DM Var   (h={h}): Skipped (library missing)\")\n",
    "                      dm_result_var_h['error'] = \"Library missing\"\n",
    "\n",
    "\n",
    "                 # Store detailed results for CSV export (Adjust tuple indices)\n",
    "                 origin_indices_h = [t[6] for t in eval_tuples_h] # index 6 = origin_index\n",
    "                 valid_origin_indices_h = [idx for idx in origin_indices_h if 0 <= idx < len(full_data_indexed.index)]\n",
    "                 valid_actual_indices_h = [idx + h for idx in valid_origin_indices_h if 0 <= idx + h < len(full_data_indexed.index)]\n",
    "\n",
    "                 if len(valid_origin_indices_h) == n_valid_pairs_h and len(valid_actual_indices_h) == n_valid_pairs_h:\n",
    "                      origin_dates_h = full_data_indexed.index[valid_origin_indices_h]\n",
    "                      actual_dates_h = full_data_indexed.index[valid_actual_indices_h]\n",
    "                      for idx in range(n_valid_pairs_h):\n",
    "                           # Ensure the correct indices based on the tuple structure defined earlier\n",
    "                           t_data = eval_tuples_h[idx]\n",
    "                           multi_horizon_results_records.append({\n",
    "                               'horizon': h,\n",
    "                               'forecast_origin_date': origin_dates_h[idx].strftime('%Y-%m-%d'),\n",
    "                               'actual_date': actual_dates_h[idx].strftime('%Y-%m-%d'),\n",
    "                               'actual_price': t_data[0],\n",
    "                               'fc_price_ag': t_data[1],\n",
    "                               'fc_price_naive': t_data[2],\n",
    "                               'actual_var_proxy': t_data[3],\n",
    "                               'fc_var_ag': t_data[4],\n",
    "                               'fc_var_ewma': t_data[5],\n",
    "                               'actual_log_return': t_data[13], # Actual Log Return\n",
    "                               'var_thresh_ag': t_data[7],\n",
    "                               'es_ag': t_data[8],\n",
    "                               'violation_ag': t_data[9],\n",
    "                               'var_thresh_ewma': t_data[10],\n",
    "                               'es_ewma': t_data[11],\n",
    "                               'violation_ewma': t_data[12],\n",
    "                           })\n",
    "                 else:\n",
    "                     log.warning(f\"Index length mismatch for H={h}. Skipping detailed CSV rows for this horizon.\")\n",
    "\n",
    "\n",
    "            else: # n_valid_pairs_h == 0\n",
    "                 print(f\"  Horizon h={h:<2}: Valid Pairs=0 | Metrics N/A\")\n",
    "                 print(f\"    DM Tests (h={h}): N/A\")\n",
    "                 print(f\"    VaR Backtest (h={h}): N/A\")\n",
    "\n",
    "\n",
    "            # Store results for this horizon\n",
    "            horizon_metrics_dict[f\"h{h}\"] = metrics_h # metrics_h now contains VaR/ES/Backtest results\n",
    "            dm_results_price_horizon[f\"h{h}_dm_p_test\"] = dm_result_price_h\n",
    "            dm_results_variance_horizon[f\"h{h}_dm_v_test\"] = dm_result_var_h\n",
    "\n",
    "\n",
    "        # Store aggregated horizon results\n",
    "        results[\"multi_horizon_eval\"][\"metrics\"] = horizon_metrics_dict\n",
    "        results[\"multi_horizon_eval\"][\"price_dm_tests\"] = dm_results_price_horizon\n",
    "        results[\"multi_horizon_eval\"][\"variance_dm_tests\"] = dm_results_variance_horizon\n",
    "\n",
    "\n",
    "        # --- Generate Thesis Recommendations based on DM tests and Backtests ---\n",
    "        # Define alpha here, inheriting from the main config or backtest block if preferred\n",
    "        alpha_reco = config.get(\"dm_test_alpha\", 0.05)\n",
    "        dm_loss_price_reco = config.get(\"dm_test_loss_type\", \"Squared Error\")\n",
    "        dm_loss_var_reco = config.get(\"dm_test_variance_loss_type\", \"QLIKE\")\n",
    "\n",
    "        # --- Price DM Recommendation (Horizon) ---\n",
    "        reco_price_text = f\"Diebold-Mariano Test (Price, AG vs Naive, alpha={alpha_reco:.2%}, H1: AG better, Loss={dm_loss_price_reco}):\\n\"\n",
    "        if not DIEBOLDMARIANO_LIB_AVAILABLE: reco_price_text += \"- Tests skipped (library missing).\\n\"\n",
    "        else:\n",
    "            sig_p_horizons = [h for h in horizons if dm_results_price_horizon.get(f\"h{h}_dm_p_test\",{}).get('is_significant',False) and dm_results_price_horizon.get(f\"h{h}_dm_p_test\",{}).get('dm_stat',0)<0]\n",
    "            ns_p_horizons = [h for h in horizons if not dm_results_price_horizon.get(f\"h{h}_dm_p_test\",{}).get('is_significant',False) and dm_results_price_horizon.get(f\"h{h}_dm_p_test\",{}).get('error') is None]\n",
    "            err_p_horizons = [h for h in horizons if dm_results_price_horizon.get(f\"h{h}_dm_p_test\",{}).get('error') is not None]\n",
    "            if sig_p_horizons: reco_price_text += f\"- AG significantly better for horizons: {sig_p_horizons}.\\n\"\n",
    "            if ns_p_horizons: reco_price_text += f\"- No significant difference for horizons {ns_p_horizons} (p >= {alpha_reco:.2f}).\\n\"\n",
    "            if err_p_horizons: reco_price_text += f\"- Test errors occurred for horizons {err_p_horizons}.\\n\"\n",
    "            if not sig_p_horizons and not ns_p_horizons and not err_p_horizons: reco_price_text += \"- No valid tests performed or all failed.\\n\"\n",
    "        # Store using the SHORT key\n",
    "        results[\"thesis_reco_price_dm\"] = reco_price_text\n",
    "\n",
    "        # --- Variance DM Recommendation (Horizon) ---\n",
    "        reco_var_text = f\"Diebold-Mariano Test (Variance, AG vs EWMA, alpha={alpha_reco:.2%}, H1: AG better, Loss={dm_loss_var_reco}):\\n\"\n",
    "        if not DIEBOLDMARIANO_LIB_AVAILABLE: reco_var_text += \"- Tests skipped (library missing).\\n\"\n",
    "        else:\n",
    "            sig_v_horizons = [h for h in horizons if dm_results_variance_horizon.get(f\"h{h}_dm_v_test\",{}).get('is_significant',False) and dm_results_variance_horizon.get(f\"h{h}_dm_v_test\",{}).get('dm_stat',0)<0]\n",
    "            ns_v_horizons = [h for h in horizons if not dm_results_variance_horizon.get(f\"h{h}_dm_v_test\",{}).get('is_significant',False) and dm_results_variance_horizon.get(f\"h{h}_dm_v_test\",{}).get('error') is None]\n",
    "            err_v_horizons = [h for h in horizons if dm_results_variance_horizon.get(f\"h{h}_dm_v_test\",{}).get('error') is not None]\n",
    "            if sig_v_horizons: reco_var_text += f\"- AG significantly better for horizons: {sig_v_horizons}.\\n\"\n",
    "            if ns_v_horizons: reco_var_text += f\"- No significant difference for horizons {ns_v_horizons} (p >= {alpha_reco:.2f}).\\n\"\n",
    "            if err_v_horizons: reco_var_text += f\"- Test errors occurred for horizons {err_v_horizons}.\\n\"\n",
    "            if not sig_v_horizons and not ns_v_horizons and not err_v_horizons: reco_var_text += \"- No valid tests performed or all failed.\\n\"\n",
    "        # Store using the SHORT key\n",
    "        results[\"thesis_reco_var_dm\"] = reco_var_text\n",
    "\n",
    "        # --- VaR Backtest Recommendation (Horizon) ---\n",
    "        reco_var_backtest_text = f\"VaR Backtesting (AG vs EWMA, alpha={alpha_reco:.2%}):\\n\"\n",
    "        kupiec_reject_ag = []\n",
    "        kupiec_reject_ewma = []\n",
    "        christ_reject_ag = []\n",
    "        christ_reject_ewma = []\n",
    "        for h in horizons:\n",
    "            metrics_h_reco = horizon_metrics_dict.get(f\"h{h}\",{})\n",
    "            # Check for existence and value before appending\n",
    "            kup_p_ag_val = metrics_h_reco.get('kupiec_p_ag')\n",
    "            if pd.notna(kup_p_ag_val) and kup_p_ag_val < alpha_reco: kupiec_reject_ag.append(h)\n",
    "\n",
    "            kup_p_ewma_val = metrics_h_reco.get('kupiec_p_ewma')\n",
    "            if pd.notna(kup_p_ewma_val) and kup_p_ewma_val < alpha_reco: kupiec_reject_ewma.append(h)\n",
    "\n",
    "            chr_p_ag_val = metrics_h_reco.get('christ_p_ag')\n",
    "            if pd.notna(chr_p_ag_val) and chr_p_ag_val < alpha_reco: christ_reject_ag.append(h)\n",
    "\n",
    "            # Christoffersen test might be NaN for EWMA, so check is important\n",
    "            chr_p_ewma_val = metrics_h_reco.get('christ_p_ewma')\n",
    "            if pd.notna(chr_p_ewma_val) and chr_p_ewma_val < alpha_reco: christ_reject_ewma.append(h)\n",
    "\n",
    "\n",
    "        reco_var_backtest_text += f\"- Kupiec Test rejects correct AG coverage for horizons: {kupiec_reject_ag if kupiec_reject_ag else 'None'}.\\n\"\n",
    "        reco_var_backtest_text += f\"- Christoffersen Test rejects correct AG coverage/independence for horizons: {christ_reject_ag if christ_reject_ag else 'None'}.\\n\"\n",
    "        reco_var_backtest_text += f\"- Kupiec Test rejects correct EWMA coverage for horizons: {kupiec_reject_ewma if kupiec_reject_ewma else 'None'}.\\n\"\n",
    "        if christ_reject_ewma: # Only mention if Christoffersen rejects for EWMA\n",
    "             reco_var_backtest_text += f\"- Christoffersen Test rejects EWMA independence for horizons: {christ_reject_ewma}.\\n\"\n",
    "        # Store using the SHORT key\n",
    "        results[\"thesis_reco_var_backtest\"] = reco_var_backtest_text\n",
    "\n",
    "        # Print recommendations during the run\n",
    "        print(\"\\n--- Thesis Recommendations Summary (during run_analysis) ---\")\n",
    "        print(\" Price DM Test (AG vs Naive):\")\n",
    "        print(reco_price_text)\n",
    "        print(\" Variance DM Test (AG vs EWMA):\")\n",
    "        print(reco_var_text)\n",
    "        print(\" VaR Backtesting:\")\n",
    "        print(reco_var_backtest_text)\n",
    "\n",
    "        results['multi_horizon_duration_min'] = (time.time() - t_multi_start) / 60\n",
    "        log.info(f\"--- Multi-Horizon Evaluation Finished: {results['multi_horizon_duration_min']:.2f} min ---\")\n",
    "\n",
    "        # Save detailed horizon results to CSV\n",
    "        try:\n",
    "             if multi_horizon_results_records:\n",
    "                 # Create DataFrame from list of dicts\n",
    "                 mh_df = pd.DataFrame(multi_horizon_results_records)\n",
    "                 # Define column order for better readability\n",
    "                 col_order = [\n",
    "                      'horizon', 'forecast_origin_date', 'actual_date',\n",
    "                      'actual_price', 'fc_price_ag', 'fc_price_naive',\n",
    "                      'actual_log_return', # Added actual log return\n",
    "                      'actual_var_proxy', 'fc_var_ag', 'fc_var_ewma',\n",
    "                      'var_thresh_ag', 'es_ag', 'violation_ag',\n",
    "                      'var_thresh_ewma', 'es_ewma', 'violation_ewma'\n",
    "                 ]\n",
    "                 # Ensure all desired columns exist before reordering\n",
    "                 mh_df = mh_df[[col for col in col_order if col in mh_df.columns]]\n",
    "                 mh_csv_path = os.path.join(coin_plot_dir, f\"{coin_id}_multi_horizon_eval_details_d{d_final}.csv\")\n",
    "                 mh_df.to_csv(mh_csv_path, index=False, float_format=\"%.8f\")\n",
    "                 log.info(f\"Multi-horizon details saved: {mh_csv_path}\")\n",
    "                 results[\"multi_horizon_eval_csv_path\"] = mh_csv_path\n",
    "             else:\n",
    "                 log.warning(\"No valid pairs found for multi-horizon details CSV.\")\n",
    "                 results[\"multi_horizon_eval_csv_path\"] = \"N/A\"\n",
    "        except Exception as e:\n",
    "            log.error(f\"Saving multi-horizon results CSV failed: {e}\")\n",
    "            results[\"multi_horizon_eval_csv_path\"] = f\"Error: {e}\"\n",
    "\n",
    "    # --- Mode 3: Future Forecast ('future') ---\n",
    "    elif forecast_mode == 'future':\n",
    "\n",
    "        log.info(f\"--- [{coin_id}] Starting Future Forecast ({config['future_forecast_horizon']} steps) ---\")\n",
    "        results[\"future_forecast\"]={}\n",
    "        # Use the initial fit on Train+Val\n",
    "        if initial_arima_fit is None: raise ValueError(\"Initial static ARIMA/GARCH fit failed, cannot perform future forecast.\")\n",
    "        arima_future = initial_arima_fit\n",
    "        garch_future = initial_garch_fit\n",
    "        scale_factor_future = config['default_scale_factor']\n",
    "\n",
    "        try:\n",
    "            # Get last actual price and date from the fit_base_df (Train+Val)\n",
    "            last_actual_price = fit_base_df['price'].iloc[-1]\n",
    "            last_actual_date = fit_base_df['date'].iloc[-1]\n",
    "            log.info(f\"Forecasting {config['future_forecast_horizon']} steps from {last_actual_date.date()} (Last Price: {last_actual_price:.4f}). Using model: {final_model_desc}\")\n",
    "\n",
    "            # Get recent history for undifferencing if needed\n",
    "            recent_log_returns_history_fut = None\n",
    "            if d_final > 0:\n",
    "                 if len(fit_base_df) >= d_final:\n",
    "                     recent_log_returns_history_fut = fit_base_df['log_return'].iloc[-d_final:].tolist()\n",
    "                 else:\n",
    "                     log.warning(f\"Future Forecast: Not enough history ({len(fit_base_df)} < {d_final}) for undifferencing d={d_final}. Price forecast might be inaccurate.\")\n",
    "\n",
    "            # Generate forecast\n",
    "            horizon_fut = config['future_forecast_horizon']\n",
    "            fc_mean_series_fut, fc_variance_series_fut=forecast_arima_garch(arima_future, garch_future, horizon_fut, scale_factor_future)\n",
    "\n",
    "            # Extract forecasts\n",
    "            fc_log_return_diff_fut=fc_mean_series_fut.values\n",
    "            fc_variance_fut=fc_variance_series_fut.values\n",
    "            # Calculate volatility\n",
    "            fc_volatility_fut = np.sqrt(np.maximum(0, fc_variance_fut)) # Ensure non-negative before sqrt\n",
    "\n",
    "            # Invert to price level\n",
    "            fc_price_fut = invert_price_forecast(last_actual_price, fc_log_return_diff_fut, d_final, recent_log_returns_history_fut)\n",
    "\n",
    "            # Create future dates index\n",
    "            future_dates=pd.date_range(start=last_actual_date+timedelta(days=1),periods=horizon_fut,freq='D')\n",
    "\n",
    "            # Create DataFrame\n",
    "            fc_df_future=pd.DataFrame({'date':future_dates, 'fc_volatility':fc_volatility_fut, 'fc_variance':fc_variance_fut, 'fc_price':fc_price_fut}).set_index('date')\n",
    "\n",
    "            # Store results\n",
    "            results['future_forecast']['data']=fc_df_future.reset_index().to_dict('records')\n",
    "            results['future_forecast']['last_actual_date']=last_actual_date.strftime('%Y-%m-%d')\n",
    "            results['future_forecast']['last_actual_price']=last_actual_price\n",
    "            log.info(\"Future forecast generated.\")\n",
    "\n",
    "            # Save future forecast to CSV\n",
    "            try:\n",
    "                future_csv_filename=os.path.join(coin_plot_dir,f\"{coin_id}_future_fc_d{d_final}.csv\")\n",
    "                fc_df_future.to_csv(future_csv_filename,index=True,float_format=\"%.8f\")\n",
    "                log.info(f\"Future forecast CSV saved: {future_csv_filename}\")\n",
    "                results[\"future_forecast_csv_path\"]=future_csv_filename\n",
    "            except Exception as e:\n",
    "                log.error(f\"Saving future forecast CSV failed: {e}\")\n",
    "                results[\"future_forecast_csv_path\"]=f\"Failed: {e}\"\n",
    "\n",
    "            # Plot future forecast if enabled\n",
    "            if is_plot_available and plt is not None:\n",
    "                  try:\n",
    "                      fig,axes=plt.subplots(2,1,figsize=(12,8),sharex=True)\n",
    "                      # Get recent history for context\n",
    "                      history_points=90\n",
    "                      history_data = fit_base_df.set_index('date').iloc[-history_points:]\n",
    "\n",
    "                      # Price Plot\n",
    "                      axes[0].plot(history_data.index, history_data['price'], label='Historical Price',color='gray',lw=1.2)\n",
    "                      if not fc_df_future.empty and 'fc_price' in fc_df_future.columns:\n",
    "                           axes[0].plot(fc_df_future.index,fc_df_future['fc_price'],label='Forecast Price',color='red',marker='.',ls='-',lw=1.5)\n",
    "                      axes[0].set_title(f'{coin_id} Future Price Forecast (d={d_final})\\n({final_model_desc})')\n",
    "                      axes[0].set_ylabel(\"Price\")\n",
    "                      axes[0].legend(); axes[0].grid(True,ls=':')\n",
    "\n",
    "                      # Volatility Plot\n",
    "                      # Use absolute log return as historical volatility proxy\n",
    "                      hist_vol_proxy = np.abs(history_data['log_return'])\n",
    "                      axes[1].plot(history_data.index, hist_vol_proxy, label='Historical |LogRet|',color='purple',alpha=0.6,lw=1.2)\n",
    "                      if not fc_df_future.empty and 'fc_volatility' in fc_df_future.columns:\n",
    "                           axes[1].plot(fc_df_future.index,fc_df_future['fc_volatility'],label='Forecast Volatility',color='orange',marker='.',ls='-',lw=1.5)\n",
    "                      axes[1].set_title('Future Volatility Forecast')\n",
    "                      axes[1].set_ylabel(\"Daily Volatility\")\n",
    "                      axes[1].legend(); axes[1].grid(True,ls=':')\n",
    "\n",
    "                      plt.setp(axes[1].xaxis.get_majorticklabels(),rotation=45,ha='right')\n",
    "                      fig.tight_layout()\n",
    "                      future_plot_filename=os.path.join(coin_plot_dir,f\"{coin_id}_future_plot_d{d_final}.png\")\n",
    "                      fig.savefig(future_plot_filename); plt.close(fig)\n",
    "                      log.info(f\"Future forecast plot saved: {future_plot_filename}\")\n",
    "                  except Exception as e:\n",
    "                      log.error(f\"Plotting future forecast failed: {e}\"); plt.close('all')\n",
    "\n",
    "        except Exception as future_e:\n",
    "            log.critical(f\"Future forecast generation failed: {future_e}\", exc_info=True)\n",
    "            results[\"status\"]=\"ERROR\"\n",
    "            results[\"error_message\"]=f\"Future Forecast Error: {future_e}\"\n",
    "            results[\"future_forecast\"][\"error\"]=str(future_e)\n",
    "\n",
    "\n",
    "    # --- Unknown Forecast Mode ---\n",
    "    else:\n",
    "        log.error(f\"Unknown forecast_mode specified: {forecast_mode}\")\n",
    "        results[\"status\"]=\"ERROR\"\n",
    "        results[\"error_message\"]=f\"Invalid forecast_mode: {forecast_mode}\"\n",
    "\n",
    "    # --- Finalize Coin Analysis ---\n",
    "    run_end_time=time.time()\n",
    "    results['total_runtime_min']=(run_end_time-start_time_coin)/60\n",
    "    log.info(f\"========== {coin_id.upper()} Analysis Finished in {results['total_runtime_min']:.2f} min (Status: {results.get('status','UNKNOWN')}) ==========\")\n",
    "    current_coin_filter.coin_id=\"N/A\" # Reset logger context\n",
    "    return results\n",
    "\n",
    "# ==============================================================================\n",
    "# --- MAIN EXECUTION SCRIPT ---\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81a7aa",
   "metadata": {},
   "source": [
    "## 6. Empirical Results (Quick Load)\n",
    "\n",
    "If the thesis outputs are already generated, you can load them here to showcase results without\n",
    "re-running the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b945fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "summary_path = Path(\"FINAL_SUMMARY_All_Coins_backtest.csv\")\n",
    "if summary_path.exists():\n",
    "    summary = pd.read_csv(summary_path)\n",
    "    display(summary.head(10))\n",
    "else:\n",
    "    print(\"Summary file not found. Run the full pipeline to generate it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f409b9b",
   "metadata": {},
   "source": [
    "### Sample Figures\n",
    "\n",
    "The thesis code exports multiple plots per coin (ACF/PACF, QQ plots, forecast paths, VaR thresholds, etc.).\n",
    "The following cell loads a few example PNGs if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "plots = list(Path(\".\").glob(\"*roll_1step_price_fc*.png\"))[:3]\n",
    "if plots:\n",
    "    for p in plots:\n",
    "        display(Image.open(p))\n",
    "else:\n",
    "    print(\"No plots found. Run the pipeline or add exported figures to display here.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392c966",
   "metadata": {},
   "source": [
    "## 7. Discussion, Limitations, Alternatives\n",
    "\n",
    "**Limitations**\n",
    "- ARIMA assumes linear mean dynamics and may struggle with regime shifts.\n",
    "- GARCH assumes a specific volatility structure; heavy tails and jumps can reduce accuracy.\n",
    "- Single-asset modeling ignores cross-asset spillovers and correlations.\n",
    "\n",
    "**Alternatives / Extensions**\n",
    "- Regime-switching and Markov-switching volatility models.\n",
    "- Multivariate GARCH (e.g., DCC) for connectedness across assets.\n",
    "- Machine-learning volatility baselines (LSTM/Transformer) for comparison.\n",
    "\n",
    "These extensions are aligned with the thesis motivation but intentionally out-of-scope to preserve\n",
    "interpretability in a portfolio-ready baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b99d4",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook reframes the thesis as a reproducible data-science project. It demonstrates how a\n",
    "transparent ARIMA-GARCH framework can be used to evaluate crypto price and volatility forecasting,\n",
    "and how risk metrics (VaR / ES) and statistical tests can validate model quality. The approach is\n",
    "interpretable, rigorous, and suitable for professional research contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339bf2e2",
   "metadata": {},
   "source": [
    "## 9. Publication Readiness Checklist\n",
    "\n",
    "- [x] Clear structure aligned with thesis chapters\n",
    "- [x] Substantial theory coverage with formulas\n",
    "- [x] Detailed pipeline with reproducible code\n",
    "- [x] Evaluation and risk backtesting included\n",
    "- [x] Results loader for portfolio display\n",
    "- [x] Limitations and extensions documented\n",
    "- [x] Ready for recruiter-facing publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a072c3",
   "metadata": {},
   "source": [
    "---\n",
    "### References (source materials)\n",
    "- Masterthesis___ARIMA_GARCH_Oeffel_final.pdf\n",
    "- ARIMA GARCH FINAL.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}